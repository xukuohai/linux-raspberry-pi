From cbafbfaac42acd5a6dd3f410c2141ea8a0ca6322 Mon Sep 17 00:00:00 2001
From: Ethan Chen <yishache@amazon.com>
Date: Mon, 7 Jun 2021 18:20:46 +0000
Subject: ena: Update to 2.5.0

Sourced from upstream git repo: https://github.com/amzn/amzn-drivers/

Change Log from Upstream:

2.4.1 -> 2.5.0
    **New Features**
    * Unify skb allocation path and use build_skb()
    * Support ethtool priv-flags and LPC state change

    **Bug Fixes**
    * Fix mapping function issues in XDP
    * Fix XDP redirection related failures
    * Fix page_ref_count() related checks to support older kernels correctly.
    * Don't define UBUNTU_VERSION_CODE when not in Ubuntu.
    * Drop unnecessary "#ifdef <function_name>" checks from kcompat.h.
    * Bug fixes and code improvements in legacy poll code

    **Minor Changes**
    * Add debug prints to failed commands
    * Minor performance improvements
    * Replace pci_set_dma_mask/_coherent with dma_set_mask_and_coherent
    * Change ena_increase_stat_atomic() function name
    * Change variable casting in ena_com.c
    * Add explicit include of ethtool.h to linux/ethtool.c
    * Change LLQ fallback print from error to warning
    * Remove unused ENA_DEFAULT_MIN_RX_BUFF_ALLOC_SIZE define
    * Remove unused SUSPEND/RESUME defines
    * Add mac OS defines.
    * Use WRITE/READ_ONCE macros for first_interrupt variable
    * Propagate upstream support for AF XDP busypoll
    * Add Jiffies of last napi call to stats
    * Add ena_ring_tx_doorbell() function
    * Cosmetic changes to LPC
    * Add is_lpc_page indication to help with page mapping
    * Back-propagate xdp helpers from upstream kernel
    * Fix RST format in README file

Signed-off-by: Ethan Chen <yishache@amazon.com>
---
 drivers/amazon/net/ena/ena_admin_defs.h |   5 +-
 drivers/amazon/net/ena/ena_com.c        |  19 +-
 drivers/amazon/net/ena/ena_com.h        |   2 +-
 drivers/amazon/net/ena/ena_eth_com.c    |  30 +-
 drivers/amazon/net/ena/ena_ethtool.c    |  56 ++-
 drivers/amazon/net/ena/ena_netdev.c     | 617 ++++++++++++++----------
 drivers/amazon/net/ena/ena_netdev.h     |  70 +--
 drivers/amazon/net/ena/kcompat.h        | 108 +++--
 8 files changed, 542 insertions(+), 365 deletions(-)
 mode change 100755 => 100644 drivers/amazon/net/ena/ena_admin_defs.h
 mode change 100755 => 100644 drivers/amazon/net/ena/ena_ethtool.c
 mode change 100755 => 100644 drivers/amazon/net/ena/ena_netdev.c
 mode change 100755 => 100644 drivers/amazon/net/ena/ena_netdev.h
 mode change 100755 => 100644 drivers/amazon/net/ena/kcompat.h

diff --git a/drivers/amazon/net/ena/ena_admin_defs.h b/drivers/amazon/net/ena/ena_admin_defs.h
old mode 100755
new mode 100644
index c89c501895e4..090198b02b88
--- a/drivers/amazon/net/ena/ena_admin_defs.h
+++ b/drivers/amazon/net/ena/ena_admin_defs.h
@@ -825,7 +825,8 @@ enum ena_admin_os_type {
 	ENA_ADMIN_OS_FREEBSD                        = 4,
 	ENA_ADMIN_OS_IPXE                           = 5,
 	ENA_ADMIN_OS_ESXI                           = 6,
-	ENA_ADMIN_OS_GROUPS_NUM                     = 6,
+	ENA_ADMIN_OS_MACOS                          = 7,
+	ENA_ADMIN_OS_GROUPS_NUM                     = 7,
 };
 
 struct ena_admin_host_info {
@@ -1064,8 +1065,6 @@ enum ena_admin_aenq_group {
 };
 
 enum ena_admin_aenq_notification_syndrome {
-	ENA_ADMIN_SUSPEND                           = 0,
-	ENA_ADMIN_RESUME                            = 1,
 	ENA_ADMIN_UPDATE_HINTS                      = 2,
 };
 
diff --git a/drivers/amazon/net/ena/ena_com.c b/drivers/amazon/net/ena/ena_com.c
index 7a87dfb2a2a5..5ce5d4980089 100644
--- a/drivers/amazon/net/ena/ena_com.c
+++ b/drivers/amazon/net/ena/ena_com.c
@@ -1362,16 +1362,15 @@ int ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,
 	comp_ctx = ena_com_submit_admin_cmd(admin_queue, cmd, cmd_size,
 					    comp, comp_size);
 	if (IS_ERR(comp_ctx)) {
-		if (comp_ctx == ERR_PTR(-ENODEV))
+		ret = PTR_ERR(comp_ctx);
+		if (ret == -ENODEV)
 			netdev_dbg(admin_queue->ena_dev->net_device,
-				   "Failed to submit command [%ld]\n",
-				   PTR_ERR(comp_ctx));
+				   "Failed to submit command [%d]\n", ret);
 		else
 			netdev_err(admin_queue->ena_dev->net_device,
-				   "Failed to submit command [%ld]\n",
-				   PTR_ERR(comp_ctx));
+				   "Failed to submit command [%d]\n", ret);
 
-		return (int)PTR_ERR(comp_ctx);
+		return ret;
 	}
 
 	ret = ena_com_wait_and_process_admin_cq(comp_ctx, admin_queue);
@@ -2258,7 +2257,7 @@ int ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,
 	return ret;
 }
 
-int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu)
+int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, u32 mtu)
 {
 	struct ena_com_admin_queue *admin_queue;
 	struct ena_admin_set_feat_cmd cmd;
@@ -2277,7 +2276,7 @@ int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu)
 	cmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;
 	cmd.aq_common_descriptor.flags = 0;
 	cmd.feat_common.feature_id = ENA_ADMIN_MTU;
-	cmd.u.mtu.mtu = (u32)mtu;
+	cmd.u.mtu.mtu = mtu;
 
 	ret = ena_com_execute_admin_command(admin_queue,
 					    (struct ena_admin_aq_entry *)&cmd,
@@ -2700,7 +2699,7 @@ int ena_com_indirect_table_set(struct ena_com_dev *ena_dev)
 		return ret;
 	}
 
-	cmd.control_buffer.length = (u32)(1ULL << rss->tbl_log_size) *
+	cmd.control_buffer.length = (1ULL << rss->tbl_log_size) *
 		sizeof(struct ena_admin_rss_ind_table_entry);
 
 	ret = ena_com_execute_admin_command(admin_queue,
@@ -2723,7 +2722,7 @@ int ena_com_indirect_table_get(struct ena_com_dev *ena_dev, u32 *ind_tbl)
 	u32 tbl_size;
 	int i, rc;
 
-	tbl_size = (u32)(1ULL << rss->tbl_log_size) *
+	tbl_size = (1ULL << rss->tbl_log_size) *
 		sizeof(struct ena_admin_rss_ind_table_entry);
 
 	rc = ena_com_get_feature_ex(ena_dev, &get_resp,
diff --git a/drivers/amazon/net/ena/ena_com.h b/drivers/amazon/net/ena/ena_com.h
index 336c535f4ec9..6ac41ca6f956 100644
--- a/drivers/amazon/net/ena/ena_com.h
+++ b/drivers/amazon/net/ena/ena_com.h
@@ -621,7 +621,7 @@ int ena_com_get_eni_stats(struct ena_com_dev *ena_dev,
  *
  * @return: 0 on Success and negative value otherwise.
  */
-int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu);
+int ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, u32 mtu);
 
 /* ena_com_get_offload_settings - Retrieve the device offloads capabilities
  * @ena_dev: ENA communication layer struct
diff --git a/drivers/amazon/net/ena/ena_eth_com.c b/drivers/amazon/net/ena/ena_eth_com.c
index c3be751e7379..3d6f0a466a9e 100644
--- a/drivers/amazon/net/ena/ena_eth_com.c
+++ b/drivers/amazon/net/ena/ena_eth_com.c
@@ -151,11 +151,14 @@ static int ena_com_close_bounce_buffer(struct ena_com_io_sq *io_sq)
 		return 0;
 
 	/* bounce buffer was used, so write it and get a new one */
-	if (pkt_ctrl->idx) {
+	if (likely(pkt_ctrl->idx)) {
 		rc = ena_com_write_bounce_buffer_to_dev(io_sq,
 							pkt_ctrl->curr_bounce_buf);
-		if (unlikely(rc))
+		if (unlikely(rc)) {
+			netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+				   "Failed to write bounce buffer to device\n");
 			return rc;
+		}
 
 		pkt_ctrl->curr_bounce_buf =
 			ena_com_get_next_bounce_buffer(&io_sq->bounce_buf_ctrl);
@@ -185,8 +188,11 @@ static int ena_com_sq_update_llq_tail(struct ena_com_io_sq *io_sq)
 	if (!pkt_ctrl->descs_left_in_line) {
 		rc = ena_com_write_bounce_buffer_to_dev(io_sq,
 							pkt_ctrl->curr_bounce_buf);
-		if (unlikely(rc))
+		if (unlikely(rc)) {
+			netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+				   "Failed to write bounce buffer to device\n");
 			return rc;
+		}
 
 		pkt_ctrl->curr_bounce_buf =
 			ena_com_get_next_bounce_buffer(&io_sq->bounce_buf_ctrl);
@@ -406,8 +412,11 @@ int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
 	}
 
 	if (unlikely(io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV &&
-		     !buffer_to_push))
+		     !buffer_to_push)) {
+		netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+			   "Push header wasn't provided in LLQ mode\n");
 		return -EINVAL;
+	}
 
 	rc = ena_com_write_header_to_bounce(io_sq, buffer_to_push, header_len);
 	if (unlikely(rc))
@@ -423,6 +432,9 @@ int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
 	/* If the caller doesn't want to send packets */
 	if (unlikely(!num_bufs && !header_len)) {
 		rc = ena_com_close_bounce_buffer(io_sq);
+		if (rc)
+			netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+				   "Failed to write buffers to LLQ\n");
 		*nb_hw_desc = io_sq->tail - start_tail;
 		return rc;
 	}
@@ -482,8 +494,11 @@ int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
 		/* The first desc share the same desc as the header */
 		if (likely(i != 0)) {
 			rc = ena_com_sq_update_tail(io_sq);
-			if (unlikely(rc))
+			if (unlikely(rc)) {
+				netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+					   "Failed to update sq tail\n");
 				return rc;
+			}
 
 			desc = get_sq_desc(io_sq);
 			if (unlikely(!desc))
@@ -512,8 +527,11 @@ int ena_com_prepare_tx(struct ena_com_io_sq *io_sq,
 	desc->len_ctrl |= ENA_ETH_IO_TX_DESC_LAST_MASK;
 
 	rc = ena_com_sq_update_tail(io_sq);
-	if (unlikely(rc))
+	if (unlikely(rc)) {
+		netdev_err(ena_com_io_sq_to_ena_dev(io_sq)->net_device,
+			   "Failed to update sq tail of the last descriptor\n");
 		return rc;
+	}
 
 	rc = ena_com_close_bounce_buffer(io_sq);
 
diff --git a/drivers/amazon/net/ena/ena_ethtool.c b/drivers/amazon/net/ena/ena_ethtool.c
old mode 100755
new mode 100644
index 82334c247016..031274399b02
--- a/drivers/amazon/net/ena/ena_ethtool.c
+++ b/drivers/amazon/net/ena/ena_ethtool.c
@@ -3,6 +3,7 @@
  * Copyright 2015-2020 Amazon.com, Inc. or its affiliates. All rights reserved.
  */
 
+#include <linux/ethtool.h>
 #include <linux/pci.h>
 
 #include "ena_netdev.h"
@@ -86,7 +87,7 @@ static const struct ena_stats ena_stats_rx_strings[] = {
 	ENA_STAT_RX_ENTRY(skb_alloc_fail),
 	ENA_STAT_RX_ENTRY(dma_mapping_err),
 	ENA_STAT_RX_ENTRY(bad_desc_num),
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	ENA_STAT_RX_ENTRY(bp_yield),
 	ENA_STAT_RX_ENTRY(bp_missed),
 	ENA_STAT_RX_ENTRY(bp_cleaned),
@@ -122,6 +123,13 @@ static const struct ena_stats ena_stats_ena_com_strings[] = {
 #define ENA_STATS_ARRAY_ENI(adapter)	\
 	(ARRAY_SIZE(ena_stats_eni_strings) * (adapter)->eni_stats_supported)
 
+static const char ena_priv_flags_strings[][ETH_GSTRING_LEN] = {
+#define ENA_PRIV_FLAGS_LPC	BIT(0)
+	"local_page_cache",
+};
+
+#define ENA_PRIV_FLAGS_NR ARRAY_SIZE(ena_priv_flags_strings)
+
 static void ena_safe_update_stat(u64 *src, u64 *dst,
 				 struct u64_stats_sync *syncp)
 {
@@ -242,10 +250,15 @@ int ena_get_sset_count(struct net_device *netdev, int sset)
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
-	if (sset != ETH_SS_STATS)
-		return -EOPNOTSUPP;
+	switch (sset) {
+	case ETH_SS_STATS:
+		return ena_get_sw_stats_count(adapter) +
+		       ena_get_hw_stats_count(adapter);
+	case ETH_SS_PRIV_FLAGS:
+		return ENA_PRIV_FLAGS_NR;
+	}
 
-	return ena_get_sw_stats_count(adapter) + ena_get_hw_stats_count(adapter);
+	return -EOPNOTSUPP;
 }
 
 static void ena_queue_strings(struct ena_adapter *adapter, u8 **data)
@@ -326,10 +339,14 @@ static void ena_get_ethtool_strings(struct net_device *netdev,
 {
 	struct ena_adapter *adapter = netdev_priv(netdev);
 
-	if (sset != ETH_SS_STATS)
-		return;
-
-	ena_get_strings(adapter, data, adapter->eni_stats_supported);
+	switch (sset) {
+	case ETH_SS_STATS:
+		ena_get_strings(adapter, data, adapter->eni_stats_supported);
+		break;
+	case ETH_SS_PRIV_FLAGS:
+		memcpy(data, ena_priv_flags_strings, sizeof(ena_priv_flags_strings));
+		break;
+	}
 }
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 6, 0)
@@ -500,6 +517,8 @@ static void ena_get_drvinfo(struct net_device *dev,
 	strlcpy(info->version, DRV_MODULE_GENERATION, sizeof(info->version));
 	strlcpy(info->bus_info, pci_name(adapter->pdev),
 		sizeof(info->bus_info));
+
+	info->n_priv_flags = ENA_PRIV_FLAGS_NR;
 }
 
 static void ena_get_ringparam(struct net_device *netdev,
@@ -1017,6 +1036,25 @@ static int ena_set_tunable(struct net_device *netdev,
 }
 #endif /* 3.18.0 */
 
+static u32 ena_get_priv_flags(struct net_device *netdev)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+	u32 priv_flags = 0;
+
+	if (adapter->rx_ring->page_cache)
+		priv_flags |= ENA_PRIV_FLAGS_LPC;
+
+	return priv_flags;
+}
+
+static int ena_set_priv_flags(struct net_device *netdev, u32 priv_flags)
+{
+	struct ena_adapter *adapter = netdev_priv(netdev);
+
+	/* LPC is the only supported private flag for now */
+	return ena_set_lpc_state(adapter, !!(priv_flags & ENA_PRIV_FLAGS_LPC));
+}
+
 static const struct ethtool_ops ena_ethtool_ops = {
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 7, 0)
 	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
@@ -1066,6 +1104,8 @@ static const struct ethtool_ops ena_ethtool_ops = {
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
 	.get_ts_info            = ethtool_op_get_ts_info,
 #endif
+	.get_priv_flags		= ena_get_priv_flags,
+	.set_priv_flags		= ena_set_priv_flags,
 };
 
 void ena_set_ethtool_ops(struct net_device *netdev)
diff --git a/drivers/amazon/net/ena/ena_netdev.c b/drivers/amazon/net/ena/ena_netdev.c
old mode 100755
new mode 100644
index 590762a05b7b..4cccbba579dd
--- a/drivers/amazon/net/ena/ena_netdev.c
+++ b/drivers/amazon/net/ena/ena_netdev.c
@@ -45,6 +45,10 @@ MODULE_VERSION(DRV_MODULE_GENERATION);
 
 #define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_IFUP | \
 		NETIF_MSG_TX_DONE | NETIF_MSG_TX_ERR | NETIF_MSG_RX_ERR)
+#ifndef ENA_LINEAR_FRAG_SUPPORTED
+
+#define ENA_SKB_PULL_MIN_LEN 64
+#endif
 static int debug = -1;
 module_param(debug, int, 0444);
 MODULE_PARM_DESC(debug, "Debug level (-1=default,0=none,...,16=all)");
@@ -109,13 +113,19 @@ static int ena_create_io_tx_queues_in_range(struct ena_adapter *adapter,
 					    int first_index, int count);
 #endif /* ENA_XDP_SUPPORT */
 
-/* Increase a stat by cnt while holding syncp seqlock */
-static void ena_increase_stat_atomic(u64 *statp, u64 cnt,
-				     struct u64_stats_sync *syncp)
+/* Increase a stat by cnt while holding syncp seqlock on 32bit machines */
+static void ena_increase_stat(u64 *statp, u64 cnt,
+			      struct u64_stats_sync *syncp)
+{
+	u64_stats_update_begin(syncp);
+	(*statp) += cnt;
+	u64_stats_update_end(syncp);
+}
+
+static void ena_ring_tx_doorbell(struct ena_ring *tx_ring)
 {
-       u64_stats_update_begin(syncp);
-       (*statp) += cnt;
-       u64_stats_update_end(syncp);
+	ena_com_write_sq_doorbell(tx_ring->ena_com_io_sq);
+	ena_increase_stat(&tx_ring->tx_stats.doorbells, 1, &tx_ring->syncp);
 }
 
 #ifdef HAVE_NDO_TX_TIMEOUT_STUCK_QUEUE_PARAMETER
@@ -134,8 +144,7 @@ static void ena_tx_timeout(struct net_device *dev)
 		return;
 
 	adapter->reset_reason = ENA_REGS_RESET_OS_NETDEV_WD;
-	ena_increase_stat_atomic(&adapter->dev_stats.tx_timeout, 1,
-		&adapter->syncp);
+	ena_increase_stat(&adapter->dev_stats.tx_timeout, 1, &adapter->syncp);
 
 	netif_err(adapter, tx_err, dev, "Transmit time out\n");
 }
@@ -189,7 +198,7 @@ static int ena_xmit_common(struct net_device *dev,
 		netif_dbg(adapter, tx_queued, dev,
 			  "llq tx max burst size of queue %d achieved, writing doorbell to send burst\n",
 			  ring->qid);
-		ena_com_write_sq_doorbell(ring->ena_com_io_sq);
+		ena_ring_tx_doorbell(ring);
 	}
 
 	/* prepare the packet's descriptors to dma engine */
@@ -203,8 +212,8 @@ static int ena_xmit_common(struct net_device *dev,
 	if (unlikely(rc)) {
 		netif_err(adapter, tx_queued, dev,
 			  "Failed to prepare tx bufs\n");
-		ena_increase_stat_atomic(&ring->tx_stats.prepare_ctx_err, 1,
-			&ring->syncp);
+		ena_increase_stat(&ring->tx_stats.prepare_ctx_err, 1,
+				  &ring->syncp);
 		if (rc != -ENOMEM) {
 			adapter->reset_reason =
 				ENA_REGS_RESET_DRIVER_INVALID_STATE;
@@ -240,7 +249,6 @@ static int ena_xdp_io_poll(struct napi_struct *napi, int budget)
 	int ret;
 
 	xdp_ring = ena_napi->xdp_ring;
-	xdp_ring->first_interrupt = ena_napi->first_interrupt;
 
 	xdp_budget = budget;
 
@@ -272,6 +280,7 @@ static int ena_xdp_io_poll(struct napi_struct *napi, int budget)
 	xdp_ring->tx_stats.napi_comp += napi_comp_call;
 	xdp_ring->tx_stats.tx_poll++;
 	u64_stats_update_end(&xdp_ring->syncp);
+	xdp_ring->tx_stats.last_napi_jiffies = jiffies;
 
 	return ret;
 }
@@ -284,43 +293,48 @@ static int ena_xdp_tx_map_frame(struct ena_ring *xdp_ring,
 {
 	struct ena_adapter *adapter = xdp_ring->adapter;
 	struct ena_com_buf *ena_buf;
-	dma_addr_t dma = 0;
+	dma_addr_t dma;
+	void *data;
 	u32 size;
 
 	tx_info->xdpf = xdpf;
+	data = tx_info->xdpf->data;
 	size = tx_info->xdpf->len;
-	ena_buf = tx_info->bufs;
 
-	/* llq push buffer */
-	*push_len = min_t(u32, size, xdp_ring->tx_max_header_size);
-	*push_hdr = tx_info->xdpf->data;
+	*push_len = 0;
+
+	if (xdp_ring->tx_mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {
+		/* LLQ push buffer */
+		*push_len = min_t(u32, size, xdp_ring->tx_max_header_size);
+		*push_hdr = data;
+
+		size -= *push_len;
+	} else {
+		*push_hdr = NULL;
+	}
 
-	if (size - *push_len > 0) {
+	if (size > 0) {
 		dma = dma_map_single(xdp_ring->dev,
-				     *push_hdr + *push_len,
-				     size - *push_len,
+				     data + *push_len,
+				     size,
 				     DMA_TO_DEVICE);
 		if (unlikely(dma_mapping_error(xdp_ring->dev, dma)))
 			goto error_report_dma_error;
 
-		tx_info->map_linear_data = 1;
+		tx_info->map_linear_data = 0;
 		tx_info->num_of_bufs = 1;
+		ena_buf = tx_info->bufs;
+		ena_buf->paddr = dma;
+		ena_buf->len = size;
 	}
 
-	ena_buf->paddr = dma;
-	ena_buf->len = size;
-
 	return 0;
 
 error_report_dma_error:
-	ena_increase_stat_atomic(&xdp_ring->tx_stats.dma_mapping_err, 1,
-		&xdp_ring->syncp);
+	ena_increase_stat(&xdp_ring->tx_stats.dma_mapping_err, 1,
+			  &xdp_ring->syncp);
 	netif_warn(adapter, tx_queued, adapter->netdev, "Failed to map xdp buff\n");
 
-	xdp_return_frame_rx_napi(tx_info->xdpf);
-	tx_info->xdpf = NULL;
-	tx_info->num_of_bufs = 0;
-
 	return -EINVAL;
 }
 
@@ -359,14 +373,11 @@ static int ena_xdp_xmit_frame(struct ena_ring *xdp_ring,
 			     xdpf->len);
 	if (rc)
 		goto error_unmap_dma;
-	/* trigger the dma engine. ena_com_write_sq_doorbell()
-	 * has a mb
+	/* trigger the dma engine. ena_ring_tx_doorbell()
+	 * calls a memory barrier inside it.
 	 */
-	if (flags & XDP_XMIT_FLUSH) {
-		ena_com_write_sq_doorbell(xdp_ring->ena_com_io_sq);
-		ena_increase_stat_atomic(&xdp_ring->tx_stats.doorbells, 1,
-			&xdp_ring->syncp);
-	}
+	if (flags & XDP_XMIT_FLUSH)
+		ena_ring_tx_doorbell(xdp_ring);
 
 	return rc;
 
@@ -412,11 +423,8 @@ static int ena_xdp_xmit(struct net_device *dev, int n,
 	}
 
 	/* Ring doorbell to make device aware of the packets */
-	if (flags & XDP_XMIT_FLUSH) {
-		ena_com_write_sq_doorbell(xdp_ring->ena_com_io_sq);
-		ena_increase_stat_atomic(&xdp_ring->tx_stats.doorbells, 1,
-			&xdp_ring->syncp);
-	}
+	if (flags & XDP_XMIT_FLUSH)
+		ena_ring_tx_doorbell(xdp_ring);
 
 	spin_unlock(&xdp_ring->xdp_tx_lock);
 
@@ -428,9 +436,8 @@ static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 {
 	struct bpf_prog *xdp_prog;
 	struct ena_ring *xdp_ring;
-	struct xdp_frame *xdpf;
-	int qid;
 	u32 verdict = XDP_PASS;
+	struct xdp_frame *xdpf;
 	u64 *xdp_stat;
 
 	rcu_read_lock();
@@ -448,9 +455,15 @@ static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 #else
 		xdpf = convert_to_xdp_frame(xdp);
 #endif
+		if (unlikely(!xdpf)) {
+			trace_xdp_exception(rx_ring->netdev, xdp_prog, verdict);
+			xdp_stat = &rx_ring->rx_stats.xdp_aborted;
+			verdict = XDP_ABORTED;
+			break;
+		}
+
 		/* Find xmit queue */
-		qid = rx_ring->qid + rx_ring->adapter->num_io_queues;
-		xdp_ring = &rx_ring->adapter->tx_ring[qid];
+		xdp_ring = rx_ring->xdp_ring;
 
 		/* The XDP queues are shared between XDP_TX and XDP_REDIRECT */
 		spin_lock(&xdp_ring->xdp_tx_lock);
@@ -461,8 +474,13 @@ static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 		xdp_stat = &rx_ring->rx_stats.xdp_tx;
 		break;
 	case XDP_REDIRECT:
-		xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);
-		xdp_stat = &rx_ring->rx_stats.xdp_redirect;
+		if (likely(!xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog))) {
+			xdp_stat = &rx_ring->rx_stats.xdp_redirect;
+			break;
+		}
+		trace_xdp_exception(rx_ring->netdev, xdp_prog, verdict);
+		xdp_stat = &rx_ring->rx_stats.xdp_aborted;
+		verdict = XDP_ABORTED;
 		break;
 	case XDP_ABORTED:
 		trace_xdp_exception(rx_ring->netdev, xdp_prog, verdict);
@@ -479,7 +497,7 @@ static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 		xdp_stat = &rx_ring->rx_stats.xdp_invalid;
 	}
 
-	ena_increase_stat_atomic(xdp_stat, 1, &rx_ring->syncp);
+	ena_increase_stat(xdp_stat, 1, &rx_ring->syncp);
 out:
 	rcu_read_unlock();
 
@@ -526,7 +544,11 @@ static int ena_xdp_register_rxq_info(struct ena_ring *rx_ring)
 {
 	int rc;
 
+#ifdef AF_XDP_BUSY_POLL_SUPPORTED
+	rc = xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev, rx_ring->qid, 0);
+#else
 	rc = xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev, rx_ring->qid);
+#endif
 
 	if (rc) {
 		netif_err(rx_ring->adapter, ifup, rx_ring->netdev,
@@ -570,7 +592,7 @@ static void ena_xdp_exchange_program_rx_in_range(struct ena_adapter *adapter,
 			rx_ring->rx_headroom = XDP_PACKET_HEADROOM;
 		} else {
 			ena_xdp_unregister_rxq_info(rx_ring);
-			rx_ring->rx_headroom = 0;
+			rx_ring->rx_headroom = NET_SKB_PAD;
 		}
 	}
 }
@@ -734,7 +756,6 @@ static void ena_init_io_rings_common(struct ena_adapter *adapter,
 	ring->ena_dev = adapter->ena_dev;
 	ring->per_napi_packets = 0;
 	ring->cpu = 0;
-	ring->first_interrupt = false;
 	ring->no_interrupt_event_cnt = 0;
 	u64_stats_init(&ring->syncp);
 }
@@ -779,7 +800,11 @@ static void ena_init_io_rings(struct ena_adapter *adapter,
 			rxr->smoothed_interval =
 				ena_com_get_nonadaptive_moderation_interval_rx(ena_dev);
 			rxr->empty_rx_queue = 0;
+			rxr->rx_headroom = NET_SKB_PAD;
 			adapter->ena_napi[i].dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+#ifdef ENA_XDP_SUPPORT
+			rxr->xdp_ring = &adapter->tx_ring[i + adapter->num_io_queues];
+#endif
 		}
 	}
 }
@@ -967,7 +992,7 @@ static int ena_setup_rx_resources(struct ena_adapter *adapter,
 	/* Reset rx statistics */
 	memset(&rx_ring->rx_stats, 0x0, sizeof(rx_ring->rx_stats));
 
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	ena_bp_init_lock(rx_ring);
 #endif
 	rx_ring->next_to_clean = 0;
@@ -1098,7 +1123,7 @@ static struct page *ena_return_cache_page(struct ena_ring *rx_ring,
 {
 	/* Remove pages belonging to different node than current_nid from cache */
 	if (unlikely(page_to_nid(ena_page->page) != current_nid)) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.lpc_wrong_numa, 1, &rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.lpc_wrong_numa, 1, &rx_ring->syncp);
 		ena_replace_cache_page(rx_ring, ena_page);
 	}
 
@@ -1117,15 +1142,20 @@ static struct page *ena_return_cache_page(struct ena_ring *rx_ring,
 	return ena_page->page;
 }
 
-static struct page *ena_get_page(struct ena_ring *rx_ring, dma_addr_t *dma, int current_nid)
+static struct page *ena_get_page(struct ena_ring *rx_ring, dma_addr_t *dma,
+				 int current_nid, bool *is_lpc_page)
 {
 	struct ena_page_cache *page_cache = rx_ring->page_cache;
 	u32 head, cache_current_size;
 	struct ena_page *ena_page;
 
 	/* Cache size of zero indicates disabled cache */
-	if (!page_cache)
+	if (!page_cache) {
+		*is_lpc_page = false;
 		return ena_alloc_map_page(rx_ring, dma);
+	}
+
+	*is_lpc_page = true;
 
 	cache_current_size = page_cache->current_size;
 	head = page_cache->head;
@@ -1146,7 +1176,7 @@ static struct page *ena_get_page(struct ena_ring *rx_ring, dma_addr_t *dma, int
 
 		/* Add a new page to the cache */
 		ena_page->page = ena_alloc_map_page(rx_ring, dma);
-		if (!ena_page->page)
+		if (unlikely(!ena_page->page))
 			return NULL;
 
 		ena_page->dma_addr = *dma;
@@ -1158,14 +1188,15 @@ static struct page *ena_get_page(struct ena_ring *rx_ring, dma_addr_t *dma, int
 
 		page_cache->current_size++;
 
-		ena_increase_stat_atomic(&rx_ring->rx_stats.lpc_warm_up, 1, &rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.lpc_warm_up, 1, &rx_ring->syncp);
 
 		return ena_page->page;
 	}
 
 	/* Next page is still in use, so we allocate outside the cache */
 	if (unlikely(page_ref_count(ena_page->page) != 1)) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.lpc_full, 1, &rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.lpc_full, 1, &rx_ring->syncp);
+		*is_lpc_page = false;
 		return ena_alloc_map_page(rx_ring, dma);
 	}
 
@@ -1180,7 +1211,9 @@ static int ena_alloc_rx_page(struct ena_ring *rx_ring,
 	int headroom = rx_ring->rx_headroom;
 	struct ena_com_buf *ena_buf;
 	struct page *page;
+	bool is_lpc_page;
 	dma_addr_t dma;
+	int tailroom;
 
 	/* restore page offset value in case it has been changed by device */
 	rx_info->page_offset = headroom;
@@ -1190,20 +1223,23 @@ static int ena_alloc_rx_page(struct ena_ring *rx_ring,
 		return 0;
 
 	/* We handle DMA here */
-	page = ena_get_page(rx_ring, &dma, current_nid);
+	page = ena_get_page(rx_ring, &dma, current_nid, &is_lpc_page);
 	if (unlikely(!page)) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.page_alloc_fail, 1,
-			&rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.page_alloc_fail, 1,
+				  &rx_ring->syncp);
 		return -ENOMEM;
 	}
 
 	netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
 		  "Allocate page %p, rx_info %p\n", page, rx_info);
 
+	tailroom = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+
 	rx_info->page = page;
+	rx_info->is_lpc_page = is_lpc_page;
 	ena_buf = &rx_info->ena_buf;
 	ena_buf->paddr = dma + headroom;
-	ena_buf->len = ENA_PAGE_SIZE - headroom;
+	ena_buf->len = ENA_PAGE_SIZE - headroom - tailroom;
 
 	return 0;
 }
@@ -1213,10 +1249,8 @@ static void ena_unmap_rx_buff(struct ena_ring *rx_ring,
 {
 	struct ena_com_buf *ena_buf = &rx_info->ena_buf;
 
-	/* If the ref count of the page is 2, then it belong to the page cache,
-	 * and it is up to it to unmap it.
-	 */
-	if (page_ref_count(rx_info->page) == 1)
+	/* LPC pages are unmapped at cache destruction */
+	if (!rx_info->is_lpc_page)
 		dma_unmap_page(rx_ring->dev, ena_buf->paddr - rx_ring->rx_headroom,
 			       ENA_PAGE_SIZE,
 			       DMA_BIDIRECTIONAL);
@@ -1279,8 +1313,8 @@ static int ena_refill_rx_bufs(struct ena_ring *rx_ring, u32 num)
 	}
 
 	if (unlikely(i < num)) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.refil_partial, 1,
-			&rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.refil_partial, 1,
+				  &rx_ring->syncp);
 		netif_warn(rx_ring->adapter, rx_err, rx_ring->netdev,
 			   "Refilled rx qid %d with only %d buffers (from %d)\n",
 			   rx_ring->qid, i, num);
@@ -1496,7 +1530,7 @@ static int handle_invalid_req_id(struct ena_ring *ring, u16 req_id,
 			  "Invalid req_id: %hu\n",
 			  req_id);
 
-	ena_increase_stat_atomic(&ring->tx_stats.bad_req_id, 1, &ring->syncp);
+	ena_increase_stat(&ring->tx_stats.bad_req_id, 1, &ring->syncp);
 
 	/* Trigger device reset */
 	ring->adapter->reset_reason = ENA_REGS_RESET_INV_TX_REQ_ID;
@@ -1607,8 +1641,8 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring, u32 budget)
 		if (netif_tx_queue_stopped(txq) && above_thresh &&
 		    test_bit(ENA_FLAG_DEV_UP, &tx_ring->adapter->flags)) {
 			netif_tx_wake_queue(txq);
-			ena_increase_stat_atomic(&tx_ring->tx_stats.queue_wakeup, 1,
-				&tx_ring->syncp);
+			ena_increase_stat(&tx_ring->tx_stats.queue_wakeup, 1,
+					  &tx_ring->syncp);
 		}
 		__netif_tx_unlock(txq);
 	}
@@ -1616,21 +1650,30 @@ static int ena_clean_tx_irq(struct ena_ring *tx_ring, u32 budget)
 	return tx_pkts;
 }
 
-static struct sk_buff *ena_alloc_skb(struct ena_ring *rx_ring, bool frags)
+static struct sk_buff *ena_alloc_skb(struct ena_ring *rx_ring, void *first_frag)
 {
 	struct sk_buff *skb;
+#ifdef ENA_LINEAR_FRAG_SUPPORTED
 
-	if (frags)
-		skb = napi_get_frags(rx_ring->napi);
-	else
+	if (!first_frag)
 		skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
 						rx_ring->rx_copybreak);
+	else
+		skb = build_skb(first_frag, ENA_PAGE_SIZE);
+#else
+	u32 linear_size = max_t(u32, ENA_SKB_PULL_MIN_LEN, rx_ring->rx_copybreak);
+
+	skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
+					linear_size);
+#endif
 
 	if (unlikely(!skb)) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.skb_alloc_fail, 1,
-			&rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.skb_alloc_fail, 1,
+				  &rx_ring->syncp);
+
 		netif_dbg(rx_ring->adapter, rx_err, rx_ring->netdev,
-			  "Failed to allocate skb. frags: %d\n", frags);
+			  "Failed to allocate skb. first_frag %s\n",
+			  first_frag ? "provided" : "not provided");
 		return NULL;
 	}
 
@@ -1642,13 +1685,15 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 				  u32 descs,
 				  u16 *next_to_clean)
 {
-	struct sk_buff *skb;
 	struct ena_rx_buffer *rx_info;
 	u16 len, req_id, buf = 0;
-#if ENA_BUSY_POLL_SUPPORT
-	bool polling;
+	struct sk_buff *skb;
+	void *page_addr;
+	u32 page_offset;
+	void *data_addr;
+#ifndef ENA_LINEAR_FRAG_SUPPORTED
+	u16 hlen;
 #endif
-	void *va;
 
 	len = ena_bufs[buf].len;
 	req_id = ena_bufs[buf].req_id;
@@ -1666,12 +1711,14 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 		  rx_info, rx_info->page);
 
 	/* save virt address of first buffer */
-	va = page_address(rx_info->page) + rx_info->page_offset;
+	page_addr = page_address(rx_info->page);
+	page_offset = rx_info->page_offset;
+	data_addr = page_addr + page_offset;
 
-	prefetch(va);
+	prefetch(data_addr);
 
 	if (len <= rx_ring->rx_copybreak) {
-		skb = ena_alloc_skb(rx_ring, false);
+		skb = ena_alloc_skb(rx_ring, NULL);
 		if (unlikely(!skb))
 			return NULL;
 
@@ -1684,14 +1731,14 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 					dma_unmap_addr(&rx_info->ena_buf, paddr),
 					len,
 					DMA_FROM_DEVICE);
-		skb_copy_to_linear_data(skb, va, len);
+		skb_copy_to_linear_data(skb, data_addr, len);
 		dma_sync_single_for_device(rx_ring->dev,
 					   dma_unmap_addr(&rx_info->ena_buf, paddr),
 					   len,
 					   DMA_FROM_DEVICE);
 
 		skb_put(skb, len);
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 5, 0)
+#ifdef ENA_BUSY_POLL_SUPPORT
 		skb_mark_napi_id(skb, rx_ring->napi);
 #endif
 		skb->protocol = eth_type_trans(skb, rx_ring->netdev);
@@ -1701,22 +1748,30 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 		return skb;
 	}
 
-#if ENA_BUSY_POLL_SUPPORT
-	polling = ena_bp_busy_polling(rx_ring);
-	/* For busy poll don't allocate frag */
-	skb = ena_alloc_skb(rx_ring, !polling);
-#else
-	skb = ena_alloc_skb(rx_ring, true);
-#endif
+	ena_unmap_rx_buff(rx_ring, rx_info);
+
+	skb = ena_alloc_skb(rx_ring, page_addr);
 	if (unlikely(!skb))
 		return NULL;
 
-	do {
-		ena_unmap_rx_buff(rx_ring, rx_info);
-
-		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_info->page,
-				rx_info->page_offset, len, ENA_PAGE_SIZE);
+#ifdef ENA_LINEAR_FRAG_SUPPORTED
+	/* Populate skb's linear part */
+	skb_reserve(skb, page_offset);
+	skb_put(skb, len);
+#else
+	/* GRO expects us to have the ethernet header in the linear part.
+	 * Copy the first ENA_SKB_PULL_MIN_LEN bytes because it is more
+	 * efficient.
+	 */
+	hlen = min_t(u16, len, ENA_SKB_PULL_MIN_LEN);
+	memcpy(__skb_put(skb, hlen), data_addr, hlen);
+	if (hlen < len)
+		skb_add_rx_frag(skb, 0, rx_info->page,
+				page_offset + hlen, len - hlen, ENA_PAGE_SIZE);
+#endif
+	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
 
+	do {
 		netif_dbg(rx_ring->adapter, rx_status, rx_ring->netdev,
 			  "RX skb updated. len %d. data_len %d\n",
 			  skb->len, skb->data_len);
@@ -1735,24 +1790,17 @@ static struct sk_buff *ena_rx_skb(struct ena_ring *rx_ring,
 		req_id = ena_bufs[buf].req_id;
 
 		rx_info = &rx_ring->rx_buffer_info[req_id];
-	} while (1);
 
-#if ENA_BUSY_POLL_SUPPORT
-	if (polling) {
-		int hlen;
+		ena_unmap_rx_buff(rx_ring, rx_info);
+
+		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_info->page,
+				rx_info->page_offset, len, ENA_PAGE_SIZE);
+
+	} while (1);
 
-		/* copy header into the skb linear data */
-		hlen = rx_ring->rx_copybreak;
-		skb_copy_to_linear_data(skb, va, hlen);
+#ifdef ENA_BUSY_POLL_SUPPORT
+	skb_mark_napi_id(skb, rx_ring->napi);
 
-		/* adjust the first segment and skb len */
-		skb_shinfo(skb)->frags[0].page_offset += hlen;
-		skb_shinfo(skb)->frags[0].size -= hlen;
-		skb->data_len -= hlen;
-		skb->tail += hlen;
-		skb->protocol = eth_type_trans(skb, rx_ring->netdev);
-		skb_mark_napi_id(skb, rx_ring->napi);
-	}
 #endif
 	return skb;
 }
@@ -1783,8 +1831,8 @@ static void ena_rx_checksum(struct ena_ring *rx_ring,
 		     (ena_rx_ctx->l3_csum_err))) {
 		/* ipv4 checksum error */
 		skb->ip_summed = CHECKSUM_NONE;
-		ena_increase_stat_atomic(&rx_ring->rx_stats.bad_csum, 1,
-			&rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.bad_csum, 1,
+				  &rx_ring->syncp);
 		netif_dbg(rx_ring->adapter, rx_err, rx_ring->netdev,
 			  "RX IPv4 header checksum error\n");
 		return;
@@ -1795,8 +1843,8 @@ static void ena_rx_checksum(struct ena_ring *rx_ring,
 		   (ena_rx_ctx->l4_proto == ENA_ETH_IO_L4_PROTO_UDP))) {
 		if (unlikely(ena_rx_ctx->l4_csum_err)) {
 			/* TCP/UDP checksum error */
-			ena_increase_stat_atomic(&rx_ring->rx_stats.bad_csum, 1,
-				&rx_ring->syncp);
+			ena_increase_stat(&rx_ring->rx_stats.bad_csum, 1,
+					  &rx_ring->syncp);
 			netif_dbg(rx_ring->adapter, rx_err, rx_ring->netdev,
 				  "RX L4 checksum error\n");
 			skb->ip_summed = CHECKSUM_NONE;
@@ -1805,11 +1853,11 @@ static void ena_rx_checksum(struct ena_ring *rx_ring,
 
 		if (likely(ena_rx_ctx->l4_csum_checked)) {
 			skb->ip_summed = CHECKSUM_UNNECESSARY;
-			ena_increase_stat_atomic(&rx_ring->rx_stats.csum_good, 1,
-				&rx_ring->syncp);
+			ena_increase_stat(&rx_ring->rx_stats.csum_good, 1,
+					  &rx_ring->syncp);
 		} else {
-			ena_increase_stat_atomic(&rx_ring->rx_stats.csum_unchecked, 1,
-				&rx_ring->syncp);
+			ena_increase_stat(&rx_ring->rx_stats.csum_unchecked, 1,
+					  &rx_ring->syncp);
 			skb->ip_summed = CHECKSUM_NONE;
 		}
 	} else {
@@ -1850,10 +1898,9 @@ static int ena_xdp_handle_buff(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 	int ret;
 
 	rx_info = &rx_ring->rx_buffer_info[rx_ring->ena_bufs[0].req_id];
-	xdp->data = page_address(rx_info->page) + rx_info->page_offset;
-	xdp_set_data_meta_invalid(xdp);
-	xdp->data_hard_start = page_address(rx_info->page);
-	xdp->data_end = xdp->data + rx_ring->ena_bufs[0].len;
+	xdp_prepare_buff(xdp, page_address(rx_info->page),
+			 rx_info->page_offset,
+			 rx_ring->ena_bufs[0].len, false);
 	/* If for some reason we received a bigger packet than
 	 * we expect, then we simply drop it
 	 */
@@ -1905,10 +1952,7 @@ static int ena_clean_rx_irq(struct ena_ring *rx_ring, struct napi_struct *napi,
 		  "%s qid %d\n", __func__, rx_ring->qid);
 	res_budget = budget;
 #ifdef ENA_XDP_SUPPORT
-	xdp.rxq = &rx_ring->xdp_rxq;
-#ifdef XDP_HAS_FRAME_SZ
-	xdp.frame_sz = ENA_PAGE_SIZE;
-#endif
+	xdp_init_buff(&xdp, ENA_PAGE_SIZE, &rx_ring->xdp_rxq);
 #endif /* ENA_XDP_SUPPORT */
 
 	do {
@@ -1989,28 +2033,19 @@ static int ena_clean_rx_irq(struct ena_ring *rx_ring, struct napi_struct *napi,
 
 		skb_record_rx_queue(skb, rx_ring->qid);
 
-		if (rx_ring->ena_bufs[0].len <= rx_ring->rx_copybreak) {
-			total_len += rx_ring->ena_bufs[0].len;
+		if (rx_ring->ena_bufs[0].len <= rx_ring->rx_copybreak)
 			rx_copybreak_pkt++;
-#if ENA_BUSY_POLL_SUPPORT
-			if (ena_bp_busy_polling(rx_ring))
-				netif_receive_skb(skb);
-			else
-				napi_gro_receive(napi, skb);
-#else
+
+		total_len += skb->len;
+
+#ifdef ENA_BUSY_POLL_SUPPORT
+		if (ena_bp_busy_polling(rx_ring))
+			netif_receive_skb(skb);
+		else
 			napi_gro_receive(napi, skb);
-#endif
-		} else {
-			total_len += skb->len;
-#if ENA_BUSY_POLL_SUPPORT
-			if (ena_bp_busy_polling(rx_ring))
-				netif_receive_skb(skb);
-			else
-				napi_gro_frags(napi);
 #else
-			napi_gro_frags(napi);
-#endif
-		}
+		napi_gro_receive(napi, skb);
+#endif /* ENA_BUSY_POLL_SUPPORT */
 
 		res_budget--;
 	} while (likely(res_budget));
@@ -2047,12 +2082,12 @@ static int ena_clean_rx_irq(struct ena_ring *rx_ring, struct napi_struct *napi,
 	adapter = netdev_priv(rx_ring->netdev);
 
 	if (rc == -ENOSPC) {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.bad_desc_num, 1,
-					 &rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.bad_desc_num, 1,
+				  &rx_ring->syncp);
 		adapter->reset_reason = ENA_REGS_RESET_TOO_MANY_RX_DESCS;
 	} else {
-		ena_increase_stat_atomic(&rx_ring->rx_stats.bad_req_id, 1,
-					 &rx_ring->syncp);
+		ena_increase_stat(&rx_ring->rx_stats.bad_req_id, 1,
+				  &rx_ring->syncp);
 		adapter->reset_reason = ENA_REGS_RESET_INV_RX_REQ_ID;
 	}
 
@@ -2113,8 +2148,8 @@ static void ena_unmask_interrupt(struct ena_ring *tx_ring,
 				tx_ring->smoothed_interval,
 				true);
 
-	ena_increase_stat_atomic(&tx_ring->tx_stats.unmask_interrupt, 1,
-		&tx_ring->syncp);
+	ena_increase_stat(&tx_ring->tx_stats.unmask_interrupt, 1,
+			  &tx_ring->syncp);
 
 	/* It is a shared MSI-X.
 	 * Tx and Rx CQ have pointer to it.
@@ -2226,9 +2261,6 @@ static int ena_io_poll(struct napi_struct *napi, int budget)
 	tx_ring = ena_napi->tx_ring;
 	rx_ring = ena_napi->rx_ring;
 
-	tx_ring->first_interrupt = ena_napi->first_interrupt;
-	rx_ring->first_interrupt = ena_napi->first_interrupt;
-
 	tx_budget = tx_ring->ring_size / ENA_TX_POLL_BUDGET_DIVIDER;
 
 	if (!test_bit(ENA_FLAG_DEV_UP, &tx_ring->adapter->flags) ||
@@ -2236,7 +2268,7 @@ static int ena_io_poll(struct napi_struct *napi, int budget)
 		napi_complete_done(napi, 0);
 		return 0;
 	}
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	if (!ena_bp_lock_napi(rx_ring))
 		return budget;
 #endif
@@ -2292,9 +2324,11 @@ static int ena_io_poll(struct napi_struct *napi, int budget)
 	tx_ring->tx_stats.tx_poll++;
 	u64_stats_update_end(&tx_ring->syncp);
 
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	ena_bp_unlock_napi(rx_ring);
 #endif
+	tx_ring->tx_stats.last_napi_jiffies = jiffies;
+
 	return ret;
 }
 
@@ -2319,7 +2353,8 @@ static irqreturn_t ena_intr_msix_io(int irq, void *data)
 {
 	struct ena_napi *ena_napi = data;
 
-	ena_napi->first_interrupt = true;
+	/* Used to check HW health */
+	WRITE_ONCE(ena_napi->first_interrupt, true);
 
 	WRITE_ONCE(ena_napi->interrupts_masked, true);
 	smp_wmb(); /* write interrupts_masked before calling napi */
@@ -2572,6 +2607,9 @@ static void ena_del_napi_in_range(struct ena_adapter *adapter,
 	int i;
 
 	for (i = first_index; i < first_index + count; i++) {
+#ifdef ENA_BUSY_POLL_SUPPORT
+		napi_hash_del(&adapter->ena_napi[i].napi);
+#endif /* ENA_BUSY_POLL_SUPPORT */
 		netif_napi_del(&adapter->ena_napi[i].napi);
 
 #ifdef ENA_XDP_SUPPORT
@@ -2579,6 +2617,11 @@ static void ena_del_napi_in_range(struct ena_adapter *adapter,
 			adapter->ena_napi[i].xdp_ring);
 #endif /* ENA_XDP_SUPPORT */
 	}
+#ifdef ENA_BUSY_POLL_SUPPORT
+
+	/* Wait until all uses of napi struct complete */
+	synchronize_net();
+#endif /* ENA_BUSY_POLL_SUPPORT */
 }
 
 static void ena_init_napi_in_range(struct ena_adapter *adapter,
@@ -2598,6 +2641,10 @@ static void ena_init_napi_in_range(struct ena_adapter *adapter,
 #endif /* ENA_XDP_SUPPORT */
 			       ENA_NAPI_BUDGET);
 
+#ifdef ENA_BUSY_POLL_SUPPORT
+		napi_hash_add(&adapter->ena_napi[i].napi);
+
+#endif /* ENA_BUSY_POLL_SUPPORT */
 		if (!ENA_IS_XDP_INDEX(adapter, i)) {
 			napi->rx_ring = &adapter->rx_ring[i];
 			napi->tx_ring = &adapter->tx_ring[i];
@@ -2610,7 +2657,7 @@ static void ena_init_napi_in_range(struct ena_adapter *adapter,
 	}
 }
 
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 static void ena_napi_disable_in_range(struct ena_adapter *adapter,
 				      int first_index,
 				      int count)
@@ -2621,11 +2668,8 @@ static void ena_napi_disable_in_range(struct ena_adapter *adapter,
 	for (i = first_index; i < first_index + count; i++) {
 		napi_disable(&adapter->ena_napi[i].napi);
 
-		/* XDP doesn't have rx_ring */
-		if (ENA_IS_XDP_INDEX(adapter, i))
-			continue;
 		rx_ring = &adapter->rx_ring[i];
-		timeout = 100;
+		timeout = 1000;
 		while (!ena_bp_disable(rx_ring)) {
 			netif_info(adapter, ifdown, adapter->netdev,
 				   "Rx queue %d locked\n", i);
@@ -2633,9 +2677,9 @@ static void ena_napi_disable_in_range(struct ena_adapter *adapter,
 			timeout--;
 
 			if (!timeout) {
-				netif_err(adapter, ifdown, adapter->netdev,
-					  "Tx queue is stuck\n");
-				continue;
+				WARN(!ena_bp_disable(rx_ring),
+				     "Unable to disable busy poll at ring %d\n", i);
+				break;
 			}
 		}
 	}
@@ -2991,48 +3035,72 @@ static void ena_free_ring_page_cache(struct ena_ring *rx_ring)
 	rx_ring->page_cache = NULL;
 }
 
+static bool ena_is_lpc_supported(struct ena_adapter *adapter,
+				 struct ena_ring *rx_ring,
+				 bool error_print)
+{
+#ifdef ENA_NETDEV_LOGS_WITHOUT_RV
+	void (*print_log)(const struct net_device *dev, const char *format, ...);
+#else
+	int (*print_log)(const struct net_device *dev, const char *format, ...);
+#endif
+	int channels_nr = adapter->num_io_queues + adapter->xdp_num_queues;
+
+	print_log = (error_print) ? netdev_err : netdev_info;
+
+	/* LPC is disabled below min number of channels */
+	if (channels_nr < ENA_LPC_MIN_NUM_OF_CHANNELS) {
+		print_log(adapter->netdev,
+			  "Local page cache is disabled for less than %d channels\n",
+			  ENA_LPC_MIN_NUM_OF_CHANNELS);
+
+		/* Disable LPC for such case. It can enabled again through
+		 * ethtool private-flag.
+		 */
+		adapter->lpc_size = 0;
+
+		return false;
+	}
+#ifdef ENA_XDP_SUPPORT
+
+	/* The driver doesn't support page caches under XDP */
+	if (ena_xdp_present_ring(rx_ring)) {
+		print_log(adapter->netdev,
+			  "Local page cache is disabled when using XDP\n");
+		return false;
+	}
+#endif /* ENA_XDP_SUPPORT */
+
+	return true;
+}
+
 /* Calculate the size of the Local Page Cache. If LPC should be disabled, return
  * a size of 0.
  */
 static u32 ena_calculate_cache_size(struct ena_adapter *adapter,
 				    struct ena_ring *rx_ring)
 {
-	int channels_nr = adapter->num_io_queues + adapter->xdp_num_queues;
-	u32 page_cache_size;
+	u32 page_cache_size = adapter->lpc_size;
 
-	/* lpc_size == 0 means disabled cache */
-	if (lpc_size == 0)
+	/* LPC cache size of 0 means disabled cache */
+	if (page_cache_size == 0)
 		return 0;
 
-	/* LPC is disabled below min number of queues */
-	if (channels_nr < ENA_LPC_MIN_NUM_OF_CHANNELS) {
-		netif_info(adapter, ifup, adapter->netdev,
-			   "Local page cache is disabled for less than %d channels\n",
-			   ENA_LPC_MIN_NUM_OF_CHANNELS);
+	if (!ena_is_lpc_supported(adapter, rx_ring, false))
 		return 0;
-	}
 
-	/* Clap the lpc_size to its maximum value */
-	if (lpc_size > ENA_LPC_MAX_MULTIPLIER) {
-		netif_info(adapter, ifup, adapter->netdev,
-			   "Provided lpc_size %d is too large, reducing to %d (max)\n",
-			   lpc_size, ENA_LPC_MAX_MULTIPLIER);
-		/* Override module param value to avoid printing this message
+	/* Clap the LPC size to its maximum value */
+	if (page_cache_size > ENA_LPC_MAX_MULTIPLIER) {
+		netdev_info(adapter->netdev,
+			    "Provided lpc_size %d is too large, reducing to %d (max)\n",
+			    lpc_size, ENA_LPC_MAX_MULTIPLIER);
+		/* Override LPC size to avoid printing this message
 		 * every up/down operation
 		 */
-		lpc_size = ENA_LPC_MAX_MULTIPLIER;
+		adapter->lpc_size = page_cache_size = lpc_size = ENA_LPC_MAX_MULTIPLIER;
 	}
 
-#ifdef ENA_XDP_SUPPORT
-	/* We currently don't support page caches under XDP */
-	if (ena_xdp_present_ring(rx_ring)) {
-		netif_info(adapter, ifup, adapter->netdev,
-			   "Local page cache is disabled when using XDP\n");
-		return 0;
-	}
-#endif /* ENA_XDP_SUPPORT */
-
-	page_cache_size = lpc_size * ENA_LPC_MULTIPLIER_UNIT;
+	page_cache_size = page_cache_size * ENA_LPC_MULTIPLIER_UNIT;
 	page_cache_size = roundup_pow_of_two(page_cache_size);
 
 	return page_cache_size;
@@ -3124,8 +3192,8 @@ static int ena_up(struct ena_adapter *adapter)
 	if (test_bit(ENA_FLAG_LINK_UP, &adapter->flags))
 		netif_carrier_on(adapter->netdev);
 
-	ena_increase_stat_atomic(&adapter->dev_stats.interface_up, 1,
-		&adapter->syncp);
+	ena_increase_stat(&adapter->dev_stats.interface_up, 1,
+			  &adapter->syncp);
 
 	set_bit(ENA_FLAG_DEV_UP, &adapter->flags);
 
@@ -3164,8 +3232,8 @@ static void ena_down(struct ena_adapter *adapter)
 
 	clear_bit(ENA_FLAG_DEV_UP, &adapter->flags);
 
-	ena_increase_stat_atomic(&adapter->dev_stats.interface_down, 1,
-		&adapter->syncp);
+	ena_increase_stat(&adapter->dev_stats.interface_down, 1,
+			  &adapter->syncp);
 
 	netif_carrier_off(adapter->netdev);
 	netif_tx_disable(adapter->netdev);
@@ -3271,6 +3339,39 @@ static int ena_close(struct net_device *netdev)
 	return 0;
 }
 
+int ena_set_lpc_state(struct ena_adapter *adapter, bool enabled)
+{
+	/* In XDP, lpc_size might be positive even with LPC disabled, use cache
+	 * pointer instead.
+	 */
+	struct ena_page_cache *page_cache = adapter->rx_ring->page_cache;
+
+	/* Exit early if LPC state doesn't change */
+	if (enabled == !!page_cache)
+		return 0;
+
+	if (enabled && !ena_is_lpc_supported(adapter, adapter->rx_ring, true))
+		return -EOPNOTSUPP;
+
+	/* Prevent a case in which disabling LPC on startup, prevents it from
+	 * being enabled afterwards.
+	 */
+	if (!lpc_size)
+		lpc_size = ENA_LPC_DEFAULT_MULTIPLIER;
+
+	adapter->lpc_size = enabled ? lpc_size : 0;
+
+	/* rtnl lock is already obtained in dev_ioctl() layer, so it's safe to
+	 * re-initialize IO resources.
+	 */
+	if (test_bit(ENA_FLAG_DEV_UP, &adapter->flags)) {
+		ena_close(adapter->netdev);
+		ena_up(adapter);
+	}
+
+	return 0;
+}
+
 int ena_update_queue_sizes(struct ena_adapter *adapter,
 			   u32 new_tx_size,
 			   u32 new_rx_size)
@@ -3400,13 +3501,12 @@ static int ena_check_and_linearize_skb(struct ena_ring *tx_ring,
 	    (header_len < tx_ring->tx_max_header_size))
 		return 0;
 
-	ena_increase_stat_atomic(&tx_ring->tx_stats.linearize, 1,
-		&tx_ring->syncp);
+	ena_increase_stat(&tx_ring->tx_stats.linearize, 1, &tx_ring->syncp);
 
 	rc = skb_linearize(skb);
 	if (unlikely(rc)) {
-		ena_increase_stat_atomic(&tx_ring->tx_stats.linearize_failed, 1,
-			&tx_ring->syncp);
+		ena_increase_stat(&tx_ring->tx_stats.linearize_failed, 1,
+				  &tx_ring->syncp);
 	}
 
 	return rc;
@@ -3446,8 +3546,8 @@ static int ena_tx_map_skb(struct ena_ring *tx_ring,
 					       tx_ring->push_buf_intermediate_buf);
 		*header_len = push_len;
 		if (unlikely(skb->data != *push_hdr)) {
-			ena_increase_stat_atomic(&tx_ring->tx_stats.llq_buffer_copy, 1,
-				&tx_ring->syncp);
+			ena_increase_stat(&tx_ring->tx_stats.llq_buffer_copy, 1,
+					  &tx_ring->syncp);
 
 			delta = push_len - skb_head_len;
 		}
@@ -3504,8 +3604,8 @@ static int ena_tx_map_skb(struct ena_ring *tx_ring,
 	return 0;
 
 error_report_dma_error:
-	ena_increase_stat_atomic(&tx_ring->tx_stats.dma_mapping_err, 1,
-		&tx_ring->syncp);
+	ena_increase_stat(&tx_ring->tx_stats.dma_mapping_err, 1,
+			  &tx_ring->syncp);
 	netif_warn(adapter, tx_queued, adapter->netdev, "Failed to map skb\n");
 
 	tx_info->skb = NULL;
@@ -3580,8 +3680,8 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb, struct net_device *dev)
 			  __func__, qid);
 
 		netif_tx_stop_queue(txq);
-		ena_increase_stat_atomic(&tx_ring->tx_stats.queue_stop, 1,
-			&tx_ring->syncp);
+		ena_increase_stat(&tx_ring->tx_stats.queue_stop, 1,
+				  &tx_ring->syncp);
 
 		/* There is a rare condition where this function decide to
 		 * stop the queue but meanwhile clean_tx_irq updates
@@ -3596,27 +3696,22 @@ static netdev_tx_t ena_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		if (ena_com_sq_have_enough_space(tx_ring->ena_com_io_sq,
 						 ENA_TX_WAKEUP_THRESH)) {
 			netif_tx_wake_queue(txq);
-			ena_increase_stat_atomic(&tx_ring->tx_stats.queue_wakeup, 1,
-				&tx_ring->syncp);
+			ena_increase_stat(&tx_ring->tx_stats.queue_wakeup, 1,
+					  &tx_ring->syncp);
 		}
 	}
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 18, 0)
 #ifdef HAVE_NETDEV_XMIT_MORE
-	if (netif_xmit_stopped(txq) || !netdev_xmit_more()) {
+	if (netif_xmit_stopped(txq) || !netdev_xmit_more())
 #else
-	if (netif_xmit_stopped(txq) || !skb->xmit_more) {
+	if (netif_xmit_stopped(txq) || !skb->xmit_more)
 #endif /* HAVE_NETDEV_XMIT_MORE */
 #endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 18, 0) */
-		/* trigger the dma engine. ena_com_write_sq_doorbell()
-		 * has a mb
+		/* trigger the dma engine. ena_ring_tx_doorbell()
+		 * calls a memory barrier inside it.
 		 */
-		ena_com_write_sq_doorbell(tx_ring->ena_com_io_sq);
-		ena_increase_stat_atomic(&tx_ring->tx_stats.doorbells, 1,
-			&tx_ring->syncp);
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 18, 0)
-	}
-#endif
+		ena_ring_tx_doorbell(tx_ring);
 
 	return NETDEV_TX_OK;
 
@@ -3929,7 +4024,7 @@ static struct net_device_stats *ena_get_stats(struct net_device *netdev)
 	return stats;
 }
 #endif
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 
 #define ENA_BP_NAPI_BUDGET 8
 static int ena_busy_poll(struct napi_struct *napi)
@@ -3974,7 +4069,7 @@ static const struct net_device_ops ena_netdev_ops = {
 	.ndo_set_rx_mode	= ena_set_rx_mode,
 #endif
 	.ndo_validate_addr	= eth_validate_addr,
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	.ndo_busy_poll		= ena_busy_poll,
 #endif
 #ifdef ENA_XDP_SUPPORT
@@ -4032,7 +4127,7 @@ static int ena_set_queues_placement_policy(struct pci_dev *pdev,
 
 	llq_feature_mask = 1 << ENA_ADMIN_LLQ;
 	if (!(ena_dev->supported_features & llq_feature_mask)) {
-		dev_err(&pdev->dev,
+		dev_warn(&pdev->dev,
 			"LLQ is not supported Fallback to host mode policy.\n");
 		ena_dev->tx_mem_queue_type = ENA_ADMIN_PLACEMENT_POLICY_HOST;
 		return 0;
@@ -4115,18 +4210,26 @@ static int ena_device_init(struct ena_com_dev *ena_dev, struct pci_dev *pdev,
 		goto err_mmio_read_less;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0)
+	rc = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(dma_width));
+	if (rc) {
+		dev_err(dev, "dma_set_mask_and_coherent failed %d\n", rc);
+		goto err_mmio_read_less;
+	}
+#else
 	rc = pci_set_dma_mask(pdev, DMA_BIT_MASK(dma_width));
 	if (rc) {
-		dev_err(dev, "pci_set_dma_mask failed 0x%x\n", rc);
+		dev_err(dev, "pci_set_dma_mask failed %d\n", rc);
 		goto err_mmio_read_less;
 	}
 
 	rc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(dma_width));
 	if (rc) {
-		dev_err(dev, "err_pci_set_consistent_dma_mask failed 0x%x\n",
+		dev_err(dev, "err_pci_set_consistent_dma_mask failed %d\n",
 			rc);
 		goto err_mmio_read_less;
 	}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0) */
 
 	/* ENA admin level init */
 	rc = ena_com_admin_init(ena_dev, &aenq_handlers);
@@ -4360,7 +4463,9 @@ static void ena_fw_reset_device(struct work_struct *work)
 static int check_for_rx_interrupt_queue(struct ena_adapter *adapter,
 					struct ena_ring *rx_ring)
 {
-	if (likely(rx_ring->first_interrupt))
+	struct ena_napi *ena_napi = container_of(rx_ring->napi, struct ena_napi, napi);
+
+	if (likely(READ_ONCE(ena_napi->first_interrupt)))
 		return 0;
 
 	if (ena_com_cq_empty(rx_ring->ena_com_io_cq))
@@ -4384,6 +4489,10 @@ static int check_for_rx_interrupt_queue(struct ena_adapter *adapter,
 static int check_missing_comp_in_tx_queue(struct ena_adapter *adapter,
 					  struct ena_ring *tx_ring)
 {
+	struct ena_napi *ena_napi = container_of(tx_ring->napi, struct ena_napi, napi);
+	unsigned int time_since_last_napi;
+	unsigned int missing_tx_comp_to;
+	bool is_tx_comp_time_expired;
 	struct ena_tx_buffer *tx_buf;
 	unsigned long last_jiffies;
 	u32 missed_tx = 0;
@@ -4397,8 +4506,10 @@ static int check_missing_comp_in_tx_queue(struct ena_adapter *adapter,
 			/* no pending Tx at this location */
 			continue;
 
-		if (unlikely(!tx_ring->first_interrupt && time_is_before_jiffies(last_jiffies +
-			     2 * adapter->missing_tx_completion_to))) {
+		is_tx_comp_time_expired = time_is_before_jiffies(last_jiffies +
+			 2 * adapter->missing_tx_completion_to);
+
+		if (unlikely(!READ_ONCE(ena_napi->first_interrupt) && is_tx_comp_time_expired)) {
 			/* If after graceful period interrupt is still not
 			 * received, we schedule a reset
 			 */
@@ -4411,12 +4522,17 @@ static int check_missing_comp_in_tx_queue(struct ena_adapter *adapter,
 			return -EIO;
 		}
 
-		if (unlikely(time_is_before_jiffies(last_jiffies +
-				adapter->missing_tx_completion_to))) {
-			if (!tx_buf->print_once)
+		is_tx_comp_time_expired = time_is_before_jiffies(last_jiffies +
+			adapter->missing_tx_completion_to);
+
+		if (unlikely(is_tx_comp_time_expired)) {
+			if (!tx_buf->print_once) {
+				time_since_last_napi = jiffies_to_usecs(jiffies - tx_ring->tx_stats.last_napi_jiffies);
+				missing_tx_comp_to = jiffies_to_msecs(adapter->missing_tx_completion_to);
 				netif_notice(adapter, tx_err, adapter->netdev,
-					     "Found a Tx that wasn't completed on time, qid %d, index %d.\n",
-					     tx_ring->qid, i);
+					     "Found a Tx that wasn't completed on time, qid %d, index %d. %u usecs have passed since last napi execution. Missing Tx timeout value %u msecs\n",
+					     tx_ring->qid, i, time_since_last_napi, missing_tx_comp_to);
+			}
 
 			tx_buf->print_once = 1;
 			missed_tx++;
@@ -4434,8 +4550,8 @@ static int check_missing_comp_in_tx_queue(struct ena_adapter *adapter,
 		rc = -EIO;
 	}
 
-	ena_increase_stat_atomic(&tx_ring->tx_stats.missed_tx , missed_tx,
-		&tx_ring->syncp);
+	ena_increase_stat(&tx_ring->tx_stats.missed_tx, missed_tx,
+			  &tx_ring->syncp);
 
 	return rc;
 }
@@ -4518,8 +4634,8 @@ static void check_for_empty_rx_ring(struct ena_adapter *adapter)
 			rx_ring->empty_rx_queue++;
 
 			if (rx_ring->empty_rx_queue >= EMPTY_RX_REFILL) {
-				ena_increase_stat_atomic(&rx_ring->rx_stats.empty_rx_ring, 1,
-					&rx_ring->syncp);
+				ena_increase_stat(&rx_ring->rx_stats.empty_rx_ring, 1,
+						  &rx_ring->syncp);
 
 				netif_err(adapter, drv, adapter->netdev,
 					  "Trigger refill for ring %d\n", i);
@@ -4549,8 +4665,8 @@ static void check_for_missing_keep_alive(struct ena_adapter *adapter)
 	if (unlikely(time_is_before_jiffies(keep_alive_expired))) {
 		netif_err(adapter, drv, adapter->netdev,
 			  "Keep alive watchdog timeout.\n");
-		ena_increase_stat_atomic(&adapter->dev_stats.wd_expired, 1,
-			&adapter->syncp);
+		ena_increase_stat(&adapter->dev_stats.wd_expired, 1,
+				  &adapter->syncp);
 		adapter->reset_reason = ENA_REGS_RESET_KEEP_ALIVE_TO;
 		set_bit(ENA_FLAG_TRIGGER_RESET, &adapter->flags);
 	}
@@ -4561,8 +4677,8 @@ static void check_for_admin_com_state(struct ena_adapter *adapter)
 	if (unlikely(!ena_com_get_admin_running_state(adapter->ena_dev))) {
 		netif_err(adapter, drv, adapter->netdev,
 			  "ENA admin queue is not in running state!\n");
-		ena_increase_stat_atomic(&adapter->dev_stats.admin_q_pause, 1,
-			&adapter->syncp);
+		ena_increase_stat(&adapter->dev_stats.admin_q_pause, 1,
+				  &adapter->syncp);
 		adapter->reset_reason = ENA_REGS_RESET_ADMIN_TO;
 		set_bit(ENA_FLAG_TRIGGER_RESET, &adapter->flags);
 	}
@@ -4936,6 +5052,13 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		return rc;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0)
+	rc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(ENA_MAX_PHYS_ADDR_SIZE_BITS));
+	if (rc) {
+		dev_err(&pdev->dev, "dma_set_mask_and_coherent failed %d\n", rc);
+		goto err_disable_device;
+	}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0) */
 	rc = pci_set_dma_mask(pdev, DMA_BIT_MASK(ENA_MAX_PHYS_ADDR_SIZE_BITS));
 	if (rc) {
 		dev_err(&pdev->dev, "pci_set_dma_mask failed %d\n", rc);
@@ -4948,6 +5071,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 			rc);
 		goto err_disable_device;
 	}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(3, 13, 0) */
 
 	pci_set_master(pdev);
 
@@ -5040,6 +5164,7 @@ static int ena_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	adapter->num_io_queues = clamp_val(num_io_queues, ENA_MIN_NUM_IO_QUEUES,
 					   max_num_io_queues);
+	adapter->lpc_size = lpc_size;
 	adapter->max_num_io_queues = max_num_io_queues;
 	adapter->last_monitored_tx_qid = 0;
 
@@ -5269,8 +5394,7 @@ static int ena_suspend(struct pci_dev *pdev,  pm_message_t state)
 #endif /* ENA_GENERIC_PM_OPS */
 	struct ena_adapter *adapter = pci_get_drvdata(pdev);
 
-	ena_increase_stat_atomic(&adapter->dev_stats.suspend, 1,
-		&adapter->syncp);
+	ena_increase_stat(&adapter->dev_stats.suspend, 1, &adapter->syncp);
 
 	rtnl_lock();
 	if (unlikely(test_bit(ENA_FLAG_TRIGGER_RESET, &adapter->flags))) {
@@ -5301,8 +5425,7 @@ static int ena_resume(struct pci_dev *pdev)
 #endif /* ENA_GENERIC_PM_OPS */
 	int rc;
 
-	ena_increase_stat_atomic(&adapter->dev_stats.resume, 1,
-		&adapter->syncp);
+	ena_increase_stat(&adapter->dev_stats.resume, 1, &adapter->syncp);
 
 	rtnl_lock();
 #if LINUX_VERSION_CODE < KERNEL_VERSION(5,5,0)
diff --git a/drivers/amazon/net/ena/ena_netdev.h b/drivers/amazon/net/ena/ena_netdev.h
old mode 100755
new mode 100644
index 144bfb5378c0..1061a0a3d149
--- a/drivers/amazon/net/ena/ena_netdev.h
+++ b/drivers/amazon/net/ena/ena_netdev.h
@@ -25,8 +25,8 @@
 #include "ena_eth_com.h"
 
 #define DRV_MODULE_GEN_MAJOR	2
-#define DRV_MODULE_GEN_MINOR	4
-#define DRV_MODULE_GEN_SUBMINOR 1
+#define DRV_MODULE_GEN_MINOR	5
+#define DRV_MODULE_GEN_SUBMINOR 0
 
 #define DRV_MODULE_NAME		"ena"
 #ifndef DRV_MODULE_GENERATION
@@ -67,12 +67,6 @@
 #define ENA_TX_WAKEUP_THRESH		(MAX_SKB_FRAGS + 2)
 #define ENA_DEFAULT_RX_COPYBREAK	(256 - NET_IP_ALIGN)
 
-/* limit the buffer size to 600 bytes to handle MTU changes from very
- * small to very large, in which case the number of buffers per packet
- * could exceed ENA_PKT_MAX_BUFS
- */
-#define ENA_DEFAULT_MIN_RX_BUFF_ALLOC_SIZE 600
-
 #define ENA_MIN_MTU		128
 
 #define ENA_NAME_MAX_LEN	20
@@ -156,14 +150,14 @@ struct ena_irq {
 };
 
 struct ena_napi {
-	struct napi_struct napi ____cacheline_aligned;
+	bool first_interrupt ____cacheline_aligned;
+	bool interrupts_masked;
+	struct napi_struct napi;
 	struct ena_ring *tx_ring;
 	struct ena_ring *rx_ring;
 #ifdef ENA_XDP_SUPPORT
 	struct ena_ring *xdp_ring;
 #endif /* ENA_XDP_SUPPORT */
-	bool first_interrupt;
-	bool interrupts_masked;
 	u32 qid;
 	struct dim dim;
 };
@@ -218,6 +212,7 @@ struct ena_rx_buffer {
 	struct sk_buff *skb;
 	struct page *page;
 	u32 page_offset;
+	bool is_lpc_page;
 	struct ena_com_buf ena_buf;
 } ____cacheline_aligned;
 
@@ -237,6 +232,7 @@ struct ena_stats_tx {
 	u64 llq_buffer_copy;
 	u64 missed_tx;
 	u64 unmask_interrupt;
+	u64 last_napi_jiffies;
 };
 
 struct ena_stats_rx {
@@ -250,7 +246,7 @@ struct ena_stats_rx {
 	u64 skb_alloc_fail;
 	u64 dma_mapping_err;
 	u64 bad_desc_num;
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	u64 bp_yield;
 	u64 bp_missed;
 	u64 bp_cleaned;
@@ -319,6 +315,10 @@ struct ena_ring {
 	struct bpf_prog *xdp_bpf_prog;
 	struct xdp_rxq_info xdp_rxq;
 	spinlock_t xdp_tx_lock;	/* synchronize XDP TX/Redirect traffic */
+	/* Used for rx queues only to point to the xdp tx ring, to
+	 * which traffic should be redirected from this rx ring.
+	 */
+	struct ena_ring *xdp_ring;
 #endif
 
 	u16 next_to_use;
@@ -332,7 +332,6 @@ struct ena_ring {
 	/* The maximum header length the device can handle */
 	u8 tx_max_header_size;
 
-	bool first_interrupt;
 	bool disable_meta_caching;
 	u16 no_interrupt_event_cnt;
 
@@ -355,12 +354,12 @@ struct ena_ring {
 
 	u8 *push_buf_intermediate_buf;
 	int empty_rx_queue;
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 	atomic_t bp_state;
 #endif
 } ____cacheline_aligned;
 
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 enum ena_busy_poll_state_t {
 	ENA_BP_STATE_IDLE = 0,
 	ENA_BP_STATE_NAPI,
@@ -405,6 +404,9 @@ struct ena_adapter {
 	u32 num_io_queues;
 	u32 max_num_io_queues;
 
+	/* Local page cache size */
+	u32 lpc_size;
+
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 8, 0)
 	struct msix_entry *msix_entries;
 #endif
@@ -477,6 +479,8 @@ void ena_dump_stats_to_buf(struct ena_adapter *adapter, u8 *buf);
 
 int ena_update_hw_stats(struct ena_adapter *adapter);
 
+int ena_set_lpc_state(struct ena_adapter *adapter, bool enabled);
+
 int ena_update_queue_sizes(struct ena_adapter *adapter,
 			   u32 new_tx_size,
 			   u32 new_rx_size);
@@ -484,8 +488,7 @@ int ena_update_queue_sizes(struct ena_adapter *adapter,
 int ena_update_queue_count(struct ena_adapter *adapter, u32 new_channel_count);
 
 int ena_get_sset_count(struct net_device *netdev, int sset);
-
-#if ENA_BUSY_POLL_SUPPORT
+#ifdef ENA_BUSY_POLL_SUPPORT
 static inline void ena_bp_init_lock(struct ena_ring *rx_ring)
 {
 	/* reset state to idle */
@@ -553,41 +556,8 @@ static inline bool ena_bp_disable(struct ena_ring *rx_ring)
 
 	return rc == ENA_BP_STATE_IDLE;
 }
-#else
-static inline void ena_bp_init_lock(struct ena_ring *rx_ring)
-{
-}
-
-static inline bool ena_bp_lock_napi(struct ena_ring *rx_ring)
-{
-       return true;
-}
-
-static inline void ena_bp_unlock_napi(struct ena_ring *rx_ring)
-{
-}
-
-static inline bool ena_bp_lock_poll(struct ena_ring *rx_ring)
-{
-       return false;
-}
-
-static inline void ena_bp_unlock_poll(struct ena_ring *rx_ring)
-{
-}
-
-static inline bool ena_bp_busy_polling(struct ena_ring *rx_ring)
-{
-       return false;
-}
-
-static inline bool ena_bp_disable(struct ena_ring *rx_ring)
-{
-	return true;
-}
 #endif /* ENA_BUSY_POLL_SUPPORT */
 
-
 #ifdef ENA_XDP_SUPPORT
 enum ena_xdp_errors_t {
 	ENA_XDP_ALLOWED = 0,
diff --git a/drivers/amazon/net/ena/kcompat.h b/drivers/amazon/net/ena/kcompat.h
old mode 100755
new mode 100644
index 35ab08cada98..824128cd8dc6
--- a/drivers/amazon/net/ena/kcompat.h
+++ b/drivers/amazon/net/ena/kcompat.h
@@ -38,7 +38,9 @@ Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 #ifndef LINUX_VERSION_CODE
 #include <linux/version.h>
-#else
+#endif
+
+#ifndef KERNEL_VERSION
 #define KERNEL_VERSION(a,b,c) (((a) << 16) + ((b) << 8) + (c))
 #endif
 
@@ -99,10 +101,11 @@ Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 #define __GFP_COLD 0
 #endif
 
-#define ENA_BUSY_POLL_SUPPORT defined(CONFIG_NET_RX_BUSY_POLL) && \
+#if defined(CONFIG_NET_RX_BUSY_POLL) && \
 	LINUX_VERSION_CODE < KERNEL_VERSION(4,5,0) && \
 	LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
-
+#define ENA_BUSY_POLL_SUPPORT
+#endif
 /******************************************************************************/
 /************************** Ubuntu macros *************************************/
 /******************************************************************************/
@@ -125,10 +128,7 @@ Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
  *  3.16.0-23-generic
  * ABI is 23
  */
-#ifndef UTS_UBUNTU_RELEASE_ABI
-#define UTS_UBUNTU_RELEASE_ABI 0
-#define UBUNTU_VERSION_CODE 0
-#else
+#ifdef UTS_UBUNTU_RELEASE_ABI
 
 #if UTS_UBUNTU_RELEASE_ABI > 255
 #undef UTS_UBUNTU_RELEASE_ABI
@@ -238,7 +238,9 @@ Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
 
 /*****************************************************************************/
 #if ( LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37) )
-#ifndef netif_set_real_num_tx_queues
+/* The function netif_set_real_num_tx_queues() doesn't return value for
+ * kernels < 2.6.37
+ */
 static inline int _kc_netif_set_real_num_tx_queues(struct net_device *dev,
                                                    unsigned int txq)
 {
@@ -247,16 +249,7 @@ static inline int _kc_netif_set_real_num_tx_queues(struct net_device *dev,
 }
 #define netif_set_real_num_tx_queues(dev, txq) \
         _kc_netif_set_real_num_tx_queues(dev, txq)
-#endif
-#ifndef netif_set_real_num_rx_queues
-static inline int __kc_netif_set_real_num_rx_queues(struct net_device __always_unused *dev,
-                                                    unsigned int __always_unused rxq)
-{
-        return 0;
-}
-#define netif_set_real_num_rx_queues(dev, rxq) \
-        __kc_netif_set_real_num_rx_queues((dev), (rxq))
-#endif
+
 #endif /* < 2.6.37 */
 
 #if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,3,0) )
@@ -278,18 +271,6 @@ typedef u32 netdev_features_t;
 
 /******************************************************************************/
 #if ( LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0) )
-#ifndef skb_add_rx_frag
-#define skb_add_rx_frag _kc_skb_add_rx_frag
-static inline void _kc_skb_add_rx_frag(struct sk_buff *skb, int i,
-				       struct page *page, int off, int size,
-				       unsigned int truesize)
-{
-	skb_fill_page_desc(skb, i, page, off, size);
-	skb->len += size;
-	skb->data_len += size;
-	skb->truesize += truesize;
-}
-#endif
 #ifdef NET_ADDR_RANDOM
 #define eth_hw_addr_random(N) do { \
 	eth_random_addr(N->dev_addr); \
@@ -383,7 +364,7 @@ static inline u32 ethtool_rxfh_indir_default(u32 index, u32 n_rx_rings)
 #endif
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,13,0)
-#if UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,24)
+#if defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,24)
 #define HAVE_NDO_SELECT_QUEUE_ACCEL_FALLBACK_V1
 #else
 #define HAVE_NDO_SELECT_QUEUE_ACCEL
@@ -416,7 +397,7 @@ static inline void reinit_completion(struct completion *x)
        RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(7,0))) \
      && !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0))&& \
      !defined(UEK3_RELEASE))) || \
-     (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,13,0,30))
+     (defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,13,0,30))
 static inline int pci_enable_msix_range(struct pci_dev *dev,
 					struct msix_entry *entries,
 					int minvec,
@@ -456,7 +437,7 @@ static inline void *devm_kcalloc(struct device *dev,
 #if (( LINUX_VERSION_CODE < KERNEL_VERSION(3,13,8) ) && \
      !RHEL_RELEASE_CODE && \
      !(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0))) || \
-     (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,13,0,30))
+     (defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,13,0,30))
 enum pkt_hash_types {
 	PKT_HASH_TYPE_NONE,	/* Undefined type */
 	PKT_HASH_TYPE_L2,	/* Input: src_MAC, dest_MAC */
@@ -476,7 +457,7 @@ static inline void skb_set_hash(struct sk_buff *skb, __u32 hash,
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)
 #if !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(7,0) && \
 			        RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(6,6)) \
-    && !(UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,105))
+    && !(defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,105))
 static inline int pci_msix_vec_count(struct pci_dev *dev)
 {
 	int pos;
@@ -504,7 +485,7 @@ static inline void ether_addr_copy(u8 *dst, const u8 *src)
 #endif
 
 #if ( LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0) || \
-	(UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,30))) || \
+	(defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,30))) || \
 	(SLE_VERSION_CODE && SLE_VERSION_CODE >= SLE_VERSION(12,0,0)) || \
 	(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(7,0) \
 	                     && RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(7,1))
@@ -581,7 +562,7 @@ static inline void __napi_schedule_irqoff(struct napi_struct *n)
 	|| (RHEL_RELEASE_CODE && ((RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,7)) && \
 	                          (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))) \
 	                      || RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) \
-	|| (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,19,0,51))
+	|| (defined(UBUNTU_VERSION_CODE) && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,19,0,51))
 #else
 static inline void napi_complete_done(struct napi_struct *n, int work_done)
 {
@@ -590,8 +571,8 @@ static inline void napi_complete_done(struct napi_struct *n, int work_done)
 #endif
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(4,1,0) \
-	|| (UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE >= UBUNTU_VERSION(3,13,0,126)) && \
-	(UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,14,0,0)) \
+	|| (defined(UBUNTU_VERSION_CODE) && \
+	(UBUNTU_VERSION(3,13,0,126) <= UBUNTU_VERSION_CODE && UBUNTU_VERSION_CODE < UBUNTU_VERSION(3,14,0,0))) \
 	|| (RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))
 
 #else
@@ -718,7 +699,7 @@ do {									\
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0) && \
     !(RHEL_RELEASE_CODE && ((RHEL_RELEASE_CODE != RHEL_RELEASE_VERSION(7, 1)) && \
                             (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6, 6)))) && \
-                            !(UBUNTU_VERSION_CODE) && \
+                            !defined(UBUNTU_VERSION_CODE) && \
                             !defined(UEK3_RELEASE)
 
 #define DO_ONCE(func, ...)						     \
@@ -759,12 +740,19 @@ static inline void netdev_rss_key_fill(void *buffer, size_t len)
 #define ENA_GENERIC_PM_OPS
 #endif
 
-#if ((LINUX_VERSION_CODE < KERNEL_VERSION(4, 5 ,0)) && \
+#if ((LINUX_VERSION_CODE < KERNEL_VERSION(4, 6 ,0)) && \
      !(RHEL_RELEASE_CODE && RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)))
+/* Linux versions 4.4.216 - 4.5 (non inclusive) back propagated page_ref_count
+ * function from kernel 4.6. To make things more difficult, Ubuntu didn't add
+ * these changes to its 4.4.* kernels
+ */
+#if !(KERNEL_VERSION(4, 4 ,216) <= LINUX_VERSION_CODE && LINUX_VERSION_CODE < KERNEL_VERSION(4, 5 ,0)) ||\
+      defined(UBUNTU_VERSION_CODE)
 static inline int page_ref_count(struct page *page)
 {
 	return atomic_read(&page->_count);
 }
+#endif /* !(KERNEL_VERSION(4, 4 ,216) <= LINUX_VERSION_CODE && LINUX_VERSION_CODE < KERNEL_VERSION(4, 5 ,0)) */
 
 static inline void page_ref_inc(struct page *page)
 {
@@ -803,4 +791,44 @@ static inline int numa_mem_id(void)
 #endif /* numa_mem_id */
 #endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 34) */
 
+#ifndef fallthrough
+#define fallthrough do {} while (0)  /* fallthrough */
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 11, 0)
+#define AF_XDP_BUSY_POLL_SUPPORTED
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+#define ENA_LINEAR_FRAG_SUPPORTED
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 17, 0)
+#define ENA_NETDEV_LOGS_WITHOUT_RV
+#endif
+
+#if defined(ENA_XDP_SUPPORT) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 12, 0)
+static __always_inline void
+xdp_init_buff(struct xdp_buff *xdp, u32 frame_sz, struct xdp_rxq_info *rxq)
+{
+	xdp->rxq = rxq;
+#ifdef XDP_HAS_FRAME_SZ
+	xdp->frame_sz = frame_sz;
+#endif
+}
+
+static __always_inline void
+xdp_prepare_buff(struct xdp_buff *xdp, unsigned char *hard_start,
+		 int headroom, int data_len, const bool meta_valid)
+{
+	unsigned char *data = hard_start + headroom;
+
+	xdp->data_hard_start = hard_start;
+	xdp->data = data;
+	xdp->data_end = data + data_len;
+	xdp->data_meta = meta_valid ? data : data + 1;
+}
+
+#endif /* defined(ENA_XDP_SUPPORT) && LINUX_VERSION_CODE <= KERNEL_VERSION(5, 12, 0) */
+
 #endif /* _KCOMPAT_H_ */
-- 
2.32.0

