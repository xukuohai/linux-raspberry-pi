
Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   95.60 syscall.c:109
    2.93 syscall.c:119
    1.47 daifflags.h:28
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (197 samples, percent: local period)
-----------------------------------------------------------------------------------------------------------------
                           39ccc: 188
                           39cd0: 6
                           39ce4: 3
                   h->nr_samples: 197
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008029c90 <el0_svc_common.constprop.0>:
         : 6                el0_svc_common.constprop.0():
         : 33               if (ret != -ENOSYS)
         : 34               return ret;
         : 35               }
         : 36               #endif
         :
         : 38               return sys_ni_syscall();
    0.00 :   ffff800008029c90:       nop
    0.00 :   ffff800008029c94:       nop
         : 41               el0_svc_common():
         : 81               }
         :
         : 83               int syscall_trace_enter(struct pt_regs *regs);
         : 84               void syscall_trace_exit(struct pt_regs *regs);
         :
         : 86               static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
    0.00 :   ffff800008029c98:       paciasp
    0.00 :   ffff800008029c9c:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff800008029ca0:       mov     x29, sp
    0.00 :   ffff800008029ca4:       str     x19, [sp, #16]
    0.00 :   ffff800008029ca8:       mov     x19, x0
         : 92               get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800008029cac:       mrs     x0, sp_el0
         : 26               read_ti_thread_flags():
         : 127              * This may be used in noinstr code, and needs to be __always_inline to prevent
         : 128              * inadvertent instrumentation.
         : 129              */
         : 130              static __always_inline unsigned long read_ti_thread_flags(struct thread_info *ti)
         : 131              {
         : 132              return READ_ONCE(ti->flags);
    0.00 :   ffff800008029cb0:       ldr     x0, [x0]
         : 134              el0_svc_common():
         : 87               const syscall_fn_t syscall_table[])
         : 88               {
         : 89               unsigned long flags = read_thread_flags();
         :
         : 91               regs->orig_x0 = regs->regs[0];
         : 92               regs->syscallno = scno;
    0.00 :   ffff800008029cb4:       str     w1, [x19, #280]
         : 86               regs->orig_x0 = regs->regs[0];
    0.00 :   ffff800008029cb8:       ldr     x2, [x19]
    0.00 :   ffff800008029cbc:       str     x2, [x19, #272]
         : 89               arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff800008029cc0:       b       ffff800008029d28 <el0_svc_common.constprop.0+0x98>
         : 45               arch_static_branch():
         : 21               asm_volatile_goto(
    0.00 :   ffff800008029cc4:       nop
         : 23               local_daif_restore():
         : 117              * So we don't need additional synchronization here.
         : 118              */
         : 119              gic_write_pmr(pmr);
         : 120              }
         :
         : 122              write_sysreg(flags, daif);
    0.00 :   ffff800008029cc8:       msr     daif, xzr
         : 124              el0_svc_common():
         : 109              * (Similarly for HVC and SMC elsewhere.)
         : 110              */
         :
         : 112              local_daif_restore(DAIF_PROCCTX);
         :
         : 114              if (flags & _TIF_MTE_ASYNC_FAULT) {
   95.60 :   ffff800008029ccc:       tbnz    w0, #5, ffff800008029d78 <el0_svc_common.constprop.0+0xe8> // syscall.c:109
         : 119              */
         : 120              syscall_set_return_value(current, regs, -ERESTARTNOINTR, 0);
         : 121              return;
         : 122              }
         :
         : 124              if (has_syscall_work(flags)) {
    2.93 :   ffff800008029cd0:       tst     x0, #0x1f00 // syscall.c:119
    0.00 :   ffff800008029cd4:       b.ne    ffff800008029dd0 <el0_svc_common.constprop.0+0x140>  // b.any
         : 142              scno = syscall_trace_enter(regs);
         : 143              if (scno == NO_SYSCALL)
         : 144              goto trace_exit;
         : 145              }
         :
         : 147              invoke_syscall(regs, scno, sc_nr, syscall_table);
    0.00 :   ffff800008029cd8:       mov     x0, x19
    0.00 :   ffff800008029cdc:       mov     w2, #0x1c3                      // #451
    0.00 :   ffff800008029ce0:       bl      ffff800008029b70 <invoke_syscall>
         : 151              local_daif_mask():
         : 28               asm volatile(
    1.47 :   ffff800008029ce4:       msr     daifset, #0xf // daifflags.h:28
         : 30               arch_static_branch_jump():
         : 38               asm_volatile_goto(
    0.00 :   ffff800008029ce8:       b       ffff800008029d50 <el0_svc_common.constprop.0+0xc0>
         : 40               arch_static_branch():
         : 21               asm_volatile_goto(
    0.00 :   ffff800008029cec:       nop
         : 23               get_current():
    0.00 :   ffff800008029cf0:       mrs     x0, sp_el0
         : 20               read_ti_thread_flags():
    0.00 :   ffff800008029cf4:       ldr     x0, [x0]
         : 128              el0_svc_common():
         : 152              * exit regardless, as the old entry assembly did.
         : 153              */
         : 154              if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
         : 155              local_daif_mask();
         : 156              flags = read_thread_flags();
         : 157              if (!has_syscall_work(flags) && !(flags & _TIF_SINGLESTEP))
    0.00 :   ffff800008029cf8:       and     x0, x0, #0x3fff00
    0.00 :   ffff800008029cfc:       and     x0, x0, #0xffffffffffe01fff
    0.00 :   ffff800008029d00:       cbz     x0, ffff800008029d18 <el0_svc_common.constprop.0+0x88>
         : 161              arch_static_branch_jump():
         : 38               asm_volatile_goto(
    0.00 :   ffff800008029d04:       b       ffff800008029da4 <el0_svc_common.constprop.0+0x114>
         : 40               arch_static_branch():
         : 21               asm_volatile_goto(
    0.00 :   ffff800008029d08:       nop
         : 23               local_daif_restore():
         : 117              write_sysreg(flags, daif);
    0.00 :   ffff800008029d0c:       msr     daif, xzr
         : 119              el0_svc_common():
         : 158              return;
         : 159              local_daif_restore(DAIF_PROCCTX);
         : 160              }
         :
         : 162              trace_exit:
         : 163              syscall_trace_exit(regs);
    0.00 :   ffff800008029d10:       mov     x0, x19
    0.00 :   ffff800008029d14:       bl      ffff80000801d5e0 <syscall_trace_exit>
         : 159              }
    0.00 :   ffff800008029d18:       ldr     x19, [sp, #16]
    0.00 :   ffff800008029d1c:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800008029d20:       autiasp
    0.00 :   ffff800008029d24:       ret
         : 164              generic_test_bit():
         : 128              /*
         : 129              * Unlike the bitops with the '__' prefix above, this one *is* atomic,
         : 130              * so `volatile` must always stay here with no cast-aways. See
         : 131              * `Documentation/atomic_bitops.txt` for the details.
         : 132              */
         : 133              return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    0.00 :   ffff800008029d28:       adrp    x2, ffff8000097ed000 <reset_devices>
    0.00 :   ffff800008029d2c:       ldr     x2, [x2, #1392]
         : 136              local_daif_restore():
         : 77               if (system_uses_irq_prio_masking()) {
    0.00 :   ffff800008029d30:       tst     w2, #0x400000
    0.00 :   ffff800008029d34:       b.eq    ffff800008029cc8 <el0_svc_common.constprop.0+0x38>  // b.none
         : 80               gic_write_pmr():
         : 127              return read_sysreg_s(SYS_ICC_PMR_EL1);
         : 128              }
         :
         : 130              static __always_inline void gic_write_pmr(u32 val)
         : 131              {
         : 132              write_sysreg_s(val, SYS_ICC_PMR_EL1);
    0.00 :   ffff800008029d38:       mov     x2, #0xe0                       // #224
    0.00 :   ffff800008029d3c:       msr     icc_pmr_el1, x2
         : 135              arch_static_branch():
    0.00 :   ffff800008029d40:       nop
         : 22               local_daif_restore():
         : 117              write_sysreg(flags, daif);
    0.00 :   ffff800008029d44:       msr     daif, xzr
         : 119              el0_svc_common():
         : 109              if (flags & _TIF_MTE_ASYNC_FAULT) {
    0.00 :   ffff800008029d48:       tbz     w0, #5, ffff800008029cd0 <el0_svc_common.constprop.0+0x40>
    0.00 :   ffff800008029d4c:       b       ffff800008029d78 <el0_svc_common.constprop.0+0xe8>
         : 112              generic_test_bit():
    0.00 :   ffff800008029d50:       adrp    x0, ffff8000097ed000 <reset_devices>
    0.00 :   ffff800008029d54:       ldr     x0, [x0, #1392]
         : 130              local_daif_mask():
         : 35               if (system_uses_irq_prio_masking())
    0.00 :   ffff800008029d58:       tst     w0, #0x400000
    0.00 :   ffff800008029d5c:       b.eq    ffff800008029cf0 <el0_svc_common.constprop.0+0x60>  // b.none
         : 38               gic_write_pmr():
    0.00 :   ffff800008029d60:       mov     x0, #0xf0                       // #240
    0.00 :   ffff800008029d64:       msr     icc_pmr_el1, x0
         : 128              }
    0.00 :   ffff800008029d68:       b       ffff800008029cf0 <el0_svc_common.constprop.0+0x60>
         : 130              local_daif_restore():
         : 79               pmr_sync();
    0.00 :   ffff800008029d6c:       dsb     sy
         : 117              write_sysreg(flags, daif);
    0.00 :   ffff800008029d70:       msr     daif, xzr
         : 119              el0_svc_common():
    0.00 :   ffff800008029d74:       tbz     w0, #5, ffff800008029cd0 <el0_svc_common.constprop.0+0x40>
         : 110              get_current():
    0.00 :   ffff800008029d78:       mrs     x0, sp_el0
         : 20               generic_test_bit():
    0.00 :   ffff800008029d7c:       ldr     x2, [x0]
         : 129              syscall_set_return_value():
         : 58               int error, long val)
         : 59               {
         : 60               if (error)
         : 61               val = error;
         :
         : 63               if (is_compat_thread(task_thread_info(task)))
    0.00 :   ffff800008029d80:       mov     w1, #0xfffffdff                 // #-513
    0.00 :   ffff800008029d84:       mov     x0, #0xfffffffffffffdff         // #-513
    0.00 :   ffff800008029d88:       tst     w2, #0x400000
    0.00 :   ffff800008029d8c:       csel    x0, x0, x1, eq  // eq = none
         : 61               val = lower_32_bits(val);
         :
         : 63               regs->regs[0] = val;
    0.00 :   ffff800008029d90:       str     x0, [x19]
         : 65               el0_svc_common():
         : 159              }
    0.00 :   ffff800008029d94:       ldr     x19, [sp, #16]
    0.00 :   ffff800008029d98:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800008029d9c:       autiasp
    0.00 :   ffff800008029da0:       ret
         : 164              generic_test_bit():
    0.00 :   ffff800008029da4:       adrp    x0, ffff8000097ed000 <reset_devices>
    0.00 :   ffff800008029da8:       ldr     x0, [x0, #1392]
         : 130              local_daif_restore():
         : 77               if (system_uses_irq_prio_masking()) {
    0.00 :   ffff800008029dac:       tst     w0, #0x400000
    0.00 :   ffff800008029db0:       b.eq    ffff800008029d0c <el0_svc_common.constprop.0+0x7c>  // b.none
    0.00 :   ffff800008029db4:       nop
         : 81               gic_write_pmr():
         : 127              write_sysreg_s(val, SYS_ICC_PMR_EL1);
    0.00 :   ffff800008029db8:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800008029dbc:       msr     icc_pmr_el1, x0
         : 130              arch_static_branch():
    0.00 :   ffff800008029dc0:       nop
    0.00 :   ffff800008029dc4:       b       ffff800008029d0c <el0_svc_common.constprop.0+0x7c>
         : 23               local_daif_restore():
         : 79               pmr_sync();
    0.00 :   ffff800008029dc8:       dsb     sy
    0.00 :   ffff800008029dcc:       b       ffff800008029d0c <el0_svc_common.constprop.0+0x7c>
         : 82               el0_svc_common():
         : 135              if (scno == NO_SYSCALL)
    0.00 :   ffff800008029dd0:       cmn     w1, #0x1
    0.00 :   ffff800008029dd4:       b.eq    ffff800008029e04 <el0_svc_common.constprop.0+0x174>  // b.none
         : 137              scno = syscall_trace_enter(regs);
    0.00 :   ffff800008029dd8:       mov     x0, x19
    0.00 :   ffff800008029ddc:       str     x3, [sp, #40]
    0.00 :   ffff800008029de0:       bl      ffff80000801d400 <syscall_trace_enter>
    0.00 :   ffff800008029de4:       mov     w1, w0
         : 138              if (scno == NO_SYSCALL)
    0.00 :   ffff800008029de8:       cmn     w0, #0x1
    0.00 :   ffff800008029dec:       ldr     x3, [sp, #40]
    0.00 :   ffff800008029df0:       b.eq    ffff800008029d10 <el0_svc_common.constprop.0+0x80>  // b.none
         : 142              invoke_syscall(regs, scno, sc_nr, syscall_table);
    0.00 :   ffff800008029df4:       mov     x0, x19
    0.00 :   ffff800008029df8:       mov     w2, #0x1c3                      // #451
    0.00 :   ffff800008029dfc:       bl      ffff800008029b70 <invoke_syscall>
         : 149              if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
    0.00 :   ffff800008029e00:       b       ffff800008029d10 <el0_svc_common.constprop.0+0x80>
         : 151              get_current():
    0.00 :   ffff800008029e04:       mrs     x0, sp_el0
         : 20               generic_test_bit():
    0.00 :   ffff800008029e08:       ldr     x2, [x0]
         : 129              syscall_set_return_value():
         : 58               if (is_compat_thread(task_thread_info(task)))
    0.00 :   ffff800008029e0c:       mov     w1, #0xffffffda                 // #-38
    0.00 :   ffff800008029e10:       mov     x0, #0xffffffffffffffda         // #-38
    0.00 :   ffff800008029e14:       tst     w2, #0x400000
    0.00 :   ffff800008029e18:       csel    x0, x0, x1, eq  // eq = none
         : 61               regs->regs[0] = val;
    0.00 :   ffff800008029e1c:       str     x0, [x19]
         : 62               }
    0.00 :   ffff800008029e20:       b       ffff800008029dd8 <el0_svc_common.constprop.0+0x148>

Sorted summary for file /usr/lib/aarch64-linux-gnu/libc.so.6
----------------------------------------------

   92.20 libc.so.6[e1368]
    4.33 libc.so.6[e1364]
    2.60 libc.so.6[e1370]
    0.87 libc.so.6[e1374]
 Percent |	Source code & Disassembly of libc.so.6 for cycles (116 samples, percent: local period)
------------------------------------------------------------------------------------------------------
                           e1364: 5
                           e1368: 107
                           e1370: 3
                           e1374: 1
                   h->nr_samples: 116
         :
         :
         :
         : 3     Disassembly of section .text:
         :
         : 5     00000000000e1340 <syscall@@GLIBC_2.17>:
    0.00 :   e1340:  nop
    0.00 :   e1344:  mov     w8, w0
    0.00 :   e1348:  mov     x0, x1
    0.00 :   e134c:  mov     x1, x2
    0.00 :   e1350:  mov     x2, x3
    0.00 :   e1354:  mov     x3, x4
    0.00 :   e1358:  mov     x4, x5
    0.00 :   e135c:  mov     x5, x6
    0.00 :   e1360:  mov     x6, x7
    4.33 :   e1364:  svc     #0x0 // libc.so.6[e1364]
   92.20 :   e1368:  cmn     x0, #0xfff // libc.so.6[e1368]
    0.00 :   e136c:  b.cs    e1374 <syscall@@GLIBC_2.17+0x34>  // b.hs, b.nlast
    2.60 :   e1370:  ret // libc.so.6[e1370]
    0.87 :   e1374:  b       27590 <__libc_start_main@@GLIBC_2.34+0x15c> // libc.so.6[e1374]
    0.00 :   e1378:  b       27590 <__libc_start_main@@GLIBC_2.34+0x15c>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   44.15 ftrace.c:1489
   11.62 preempt.h:74
   11.62 ftrace.c:7551
    6.97 trace_recursion.h:180
    6.97 ftrace.c:7553
    4.72 ftrace.c:7508
    2.32 trace_recursion.h:165
    2.32 current.h:19
    2.32 ftrace.c:7519
    2.32 preempt.h:49
    2.32 ftrace.c:1486
    2.32 ftrace.c:1487
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (43 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          16bdf4: 2
                          16be00: 1
                          16be14: 1
                          16be20: 1
                          16be68: 2
                          16be7c: 1
                          16be88: 1
                          16be9c: 2
                          16bea4: 1
                          16bea8: 1
                          16bec4: 1
                          16bec8: 19
                          16bf20: 1
                          16bf2c: 1
                          16bf38: 1
                          16bf4c: 1
                          16bf54: 1
                          16bf64: 5
                   h->nr_samples: 43
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000815bdf0 <arch_ftrace_ops_list_func>:
         : 6                arch_ftrace_ops_list_func():
         : 7551             * arch_ftrace_ops_list_func.
         : 7552             */
         : 7553             #if ARCH_SUPPORTS_FTRACE_OPS
         : 7554             void arch_ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip,
         : 7555             struct ftrace_ops *op, struct ftrace_regs *fregs)
         : 7556             {
    0.00 :   ffff80000815bdf0:       paciasp
    4.65 :   ffff80000815bdf4:       stp     x29, x30, [sp, #-144]! // ftrace.c:7551
    0.00 :   ffff80000815bdf8:       mrs     x2, sp_el0
    0.00 :   ffff80000815bdfc:       mov     x29, sp
    2.32 :   ffff80000815be00:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80000815be04:       mov     x20, x1
         : 7563             trace_test_and_set_recursion():
         : 147              int start)
         : 148              {
         : 149              unsigned int val = READ_ONCE(current->trace_recursion);
         : 150              int bit;
         :
         : 152              bit = trace_get_context_bit() + start;
    0.00 :   ffff80000815be08:       mov     w5, #0x8                        // #8
         : 154              arch_ftrace_ops_list_func():
    0.00 :   ffff80000815be0c:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80000815be10:       mov     x21, x3
    2.32 :   ffff80000815be14:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80000815be18:       mov     x23, x0
    0.00 :   ffff80000815be1c:       ldr     x4, [x2, #1168]
    2.32 :   ffff80000815be20:       str     x4, [sp, #136]
    0.00 :   ffff80000815be24:       mov     x4, #0x0                        // #0
         : 7558             trace_test_and_set_recursion():
         : 148              if (unlikely(val & (1 << bit))) {
    0.00 :   ffff80000815be28:       mov     w2, #0x1                        // #1
         : 150              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80000815be2c:       mrs     x4, sp_el0
         : 26               trace_test_and_set_recursion():
         : 144              unsigned int val = READ_ONCE(current->trace_recursion);
    0.00 :   ffff80000815be30:       ldr     x7, [x4, #2520]
         : 146              preempt_count():
         : 13               #define PREEMPT_NEED_RESCHED    BIT(32)
         : 14               #define PREEMPT_ENABLED (PREEMPT_NEED_RESCHED)
         :
         : 16               static inline int preempt_count(void)
         : 17               {
         : 18               return READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff80000815be34:       ldr     w6, [x4, #8]
         : 20               interrupt_context_level():
         : 94               static __always_inline unsigned char interrupt_context_level(void)
         : 95               {
         : 96               unsigned long pc = preempt_count();
         : 97               unsigned char level = 0;
         :
         : 99               level += !!(pc & (NMI_MASK));
    0.00 :   ffff80000815be38:       tst     w6, #0xf00000
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK));
         : 97               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff80000815be3c:       and     w1, w6, #0xffff00
         : 94               level += !!(pc & (NMI_MASK));
    0.00 :   ffff80000815be40:       cset    w4, ne  // ne = any
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff80000815be44:       and     w1, w1, #0xffff01ff
         : 95               level += !!(pc & (NMI_MASK | HARDIRQ_MASK));
    0.00 :   ffff80000815be48:       tst     w6, #0xff0000
    0.00 :   ffff80000815be4c:       cinc    w4, w4, ne      // ne = any
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff80000815be50:       cmp     w1, #0x0
         : 98               trace_get_context_bit():
         : 121              return TRACE_CTX_NORMAL - bit;
    0.00 :   ffff80000815be54:       cinc    w4, w4, ne      // ne = any
         : 123              trace_test_and_set_recursion():
         : 147              bit = trace_get_context_bit() + start;
    0.00 :   ffff80000815be58:       sub     w5, w5, w4
         : 148              if (unlikely(val & (1 << bit))) {
    0.00 :   ffff80000815be5c:       lsl     w2, w2, w5
    0.00 :   ffff80000815be60:       tst     w2, w7
    0.00 :   ffff80000815be64:       b.ne    ffff80000815bf84 <arch_ftrace_ops_list_func+0x194>  // b.any
         : 152              trace_clear_recursion():
         : 180              */
         : 181              static __always_inline void trace_clear_recursion(int bit)
         : 182              {
         : 183              preempt_enable_notrace();
         : 184              barrier();
         : 185              trace_recursion_clear(bit);
    4.65 :   ffff80000815be68:       mvn     w22, w2 // trace_recursion.h:180
    0.00 :   ffff80000815be6c:       str     x25, [sp, #64]
    0.00 :   ffff80000815be70:       sxtw    x22, w22
         : 189              trace_test_and_set_recursion():
         : 165              current->trace_recursion = val;
    0.00 :   ffff80000815be74:       orr     w2, w2, w7
         : 167              get_current():
    0.00 :   ffff80000815be78:       mrs     x4, sp_el0
         : 20               trace_test_and_set_recursion():
    2.32 :   ffff80000815be7c:       str     x2, [x4, #2520] // trace_recursion.h:165
         : 166              __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff80000815be80:       ldr     w1, [x4, #8]
         : 48               pc += val;
    0.00 :   ffff80000815be84:       add     w1, w1, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    2.32 :   ffff80000815be88:       str     w1, [x4, #8] // preempt.h:49
         : 51               __ftrace_ops_list_func():
         : 7506             do_for_each_ftrace_op(op, ftrace_ops_list) {
    0.00 :   ffff80000815be8c:       adrp    x0, ffff800009638000 <folio_wait_table+0x14c0>
    0.00 :   ffff80000815be90:       add     x25, x0, #0xc28
         : 7527             } while_for_each_ftrace_op(op);
    0.00 :   ffff80000815be94:       add     x24, x25, #0x8
         : 7506             do_for_each_ftrace_op(op, ftrace_ops_list) {
    0.00 :   ffff80000815be98:       ldr     x19, [x0, #3112]
         : 7508             if (op->flags & FTRACE_OPS_FL_STUB)
    4.72 :   ffff80000815be9c:       ldr     x0, [x19, #16] // ftrace.c:7508
    0.00 :   ffff80000815bea0:       tbnz    w0, #5, ffff80000815bef8 <arch_ftrace_ops_list_func+0x108>
         : 7519             if ((!(op->flags & FTRACE_OPS_FL_RCU) || rcu_is_watching()) &&
    2.32 :   ffff80000815bea4:       tbnz    w0, #14, ffff80000815bf74 <arch_ftrace_ops_list_func+0x184> // ftrace.c:7519
         : 7521             ftrace_ops_test():
         : 1486             rcu_assign_pointer(hash.filter_hash, ops->func_hash->filter_hash);
    2.32 :   ffff80000815bea8:       ldr     x0, [x19, #88] // ftrace.c:1486
    0.00 :   ffff80000815beac:       add     x1, sp, #0x60
    0.00 :   ffff80000815beb0:       ldr     x0, [x0, #8]
    0.00 :   ffff80000815beb4:       stlr    x0, [x1]
         : 1487             rcu_assign_pointer(hash.notrace_hash, ops->func_hash->notrace_hash);
    0.00 :   ffff80000815beb8:       ldr     x0, [x19, #88]
    0.00 :   ffff80000815bebc:       add     x1, sp, #0x58
    0.00 :   ffff80000815bec0:       ldr     x0, [x0]
    2.32 :   ffff80000815bec4:       stlr    x0, [x1] // ftrace.c:1487
         : 1489             if (hash_contains_ip(ip, &hash))
   44.15 :   ffff80000815bec8:       ldp     x1, x2, [sp, #88] // ftrace.c:1489
    0.00 :   ffff80000815becc:       mov     x0, x23
    0.00 :   ffff80000815bed0:       bl      ffff80000815b530 <hash_contains_ip.isra.0>
    0.00 :   ffff80000815bed4:       tst     w0, #0xff
    0.00 :   ffff80000815bed8:       b.eq    ffff80000815bef8 <arch_ftrace_ops_list_func+0x108>  // b.none
         : 1495             __ftrace_ops_list_func():
         : 7521             if (FTRACE_WARN_ON(!op->func)) {
    0.00 :   ffff80000815bedc:       ldr     x4, [x19]
    0.00 :   ffff80000815bee0:       cbz     x4, ffff80000815bfa0 <arch_ftrace_ops_list_func+0x1b0>
         : 7525             op->func(ip, parent_ip, op, fregs);
    0.00 :   ffff80000815bee4:       mov     x3, x21
    0.00 :   ffff80000815bee8:       mov     x2, x19
    0.00 :   ffff80000815beec:       mov     x1, x20
    0.00 :   ffff80000815bef0:       mov     x0, x23
    0.00 :   ffff80000815bef4:       blr     x4
         : 7527             } while_for_each_ftrace_op(op);
    0.00 :   ffff80000815bef8:       ldr     x19, [x19, #8]
    0.00 :   ffff80000815befc:       cmp     x19, #0x0
    0.00 :   ffff80000815bf00:       ccmp    x19, x24, #0x4, ne      // ne = any
    0.00 :   ffff80000815bf04:       b.ne    ffff80000815be9c <arch_ftrace_ops_list_func+0xac>  // b.any
         : 7532             get_current():
    0.00 :   ffff80000815bf08:       mrs     x1, sp_el0
         : 20               __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff80000815bf0c:       ldr     x0, [x1, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff80000815bf10:       sub     x0, x0, #0x1
    0.00 :   ffff80000815bf14:       str     w0, [x1, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff80000815bf18:       cbnz    x0, ffff80000815bf64 <arch_ftrace_ops_list_func+0x174>
         : 81               trace_clear_recursion():
         : 178              preempt_enable_notrace();
    0.00 :   ffff80000815bf1c:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
         : 180              get_current():
    2.32 :   ffff80000815bf20:       mrs     x1, sp_el0 // current.h:19
         : 20               trace_clear_recursion():
         : 180              trace_recursion_clear(bit);
    0.00 :   ffff80000815bf24:       ldr     x0, [x1, #2520]
    0.00 :   ffff80000815bf28:       and     x0, x0, x22
    2.32 :   ffff80000815bf2c:       str     x0, [x1, #2520] // trace_recursion.h:180
         : 184              arch_ftrace_ops_list_func():
         : 7553             __ftrace_ops_list_func(ip, parent_ip, NULL, fregs);
         : 7554             }
    0.00 :   ffff80000815bf30:       ldr     x25, [sp, #64]
    0.00 :   ffff80000815bf34:       mrs     x0, sp_el0
    2.32 :   ffff80000815bf38:       ldr     x2, [sp, #136] // ftrace.c:7553
    0.00 :   ffff80000815bf3c:       ldr     x1, [x0, #1168]
    0.00 :   ffff80000815bf40:       subs    x2, x2, x1
    0.00 :   ffff80000815bf44:       mov     x1, #0x0                        // #0
    0.00 :   ffff80000815bf48:       b.ne    ffff80000815bf98 <arch_ftrace_ops_list_func+0x1a8>  // b.any
    2.32 :   ffff80000815bf4c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80000815bf50:       ldp     x21, x22, [sp, #32]
    2.32 :   ffff80000815bf54:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80000815bf58:       ldp     x29, x30, [sp], #144
    0.00 :   ffff80000815bf5c:       autiasp
    0.00 :   ffff80000815bf60:       ret
         : 7568             __preempt_count_dec_and_test():
   11.62 :   ffff80000815bf64:       ldr     x0, [x1, #8] // preempt.h:74
    0.00 :   ffff80000815bf68:       cbnz    x0, ffff80000815bf20 <arch_ftrace_ops_list_func+0x130>
         : 76               trace_clear_recursion():
         : 178              preempt_enable_notrace();
    0.00 :   ffff80000815bf6c:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff80000815bf70:       b       ffff80000815bf20 <arch_ftrace_ops_list_func+0x130>
         : 181              __ftrace_ops_list_func():
         : 7519             if ((!(op->flags & FTRACE_OPS_FL_RCU) || rcu_is_watching()) &&
    0.00 :   ffff80000815bf74:       bl      ffff8000080e5770 <rcu_is_watching>
    0.00 :   ffff80000815bf78:       tst     w0, #0xff
    0.00 :   ffff80000815bf7c:       b.ne    ffff80000815bea8 <arch_ftrace_ops_list_func+0xb8>  // b.any
    0.00 :   ffff80000815bf80:       b       ffff80000815bef8 <arch_ftrace_ops_list_func+0x108>
         : 7524             trace_test_and_set_recursion():
         : 158              if (val & (1 << bit)) {
    0.00 :   ffff80000815bf84:       tbnz    w7, #9, ffff80000815bf34 <arch_ftrace_ops_list_func+0x144>
    0.00 :   ffff80000815bf88:       mov     x22, #0xfffffffffffffdff        // #-513
    0.00 :   ffff80000815bf8c:       mov     w2, #0x200                      // #512
    0.00 :   ffff80000815bf90:       str     x25, [sp, #64]
    0.00 :   ffff80000815bf94:       b       ffff80000815be74 <arch_ftrace_ops_list_func+0x84>
    0.00 :   ffff80000815bf98:       str     x25, [sp, #64]
         : 165              arch_ftrace_ops_list_func():
         : 7553             }
    0.00 :   ffff80000815bf9c:       bl      ffff800008ae5de0 <__stack_chk_fail>
         : 7555             __ftrace_ops_list_func():
         : 7521             if (FTRACE_WARN_ON(!op->func)) {
    0.00 :   ffff80000815bfa0:       brk     #0x800
         : 7523             ftrace_kill():
         : 8040             */
         : 8041             void ftrace_kill(void)
         : 8042             {
         : 8043             ftrace_disabled = 1;
         : 8044             ftrace_enabled = 0;
         : 8045             ftrace_trace_function = ftrace_stub;
    0.00 :   ffff80000815bfa4:       adrp    x3, ffff80000802e000 <arch_ftrace_update_code+0x10>
    0.00 :   ffff80000815bfa8:       add     x3, x3, #0x144
         : 8038             ftrace_disabled = 1;
    0.00 :   ffff80000815bfac:       mov     w4, #0x1                        // #1
         : 8040             __ftrace_ops_list_func():
         : 7522             pr_warn("op=%p %pS\n", op, op);
    0.00 :   ffff80000815bfb0:       mov     x2, x19
    0.00 :   ffff80000815bfb4:       mov     x1, x19
    0.00 :   ffff80000815bfb8:       adrp    x0, ffff800008d80000 <kallsyms_token_index+0x17f60>
    0.00 :   ffff80000815bfbc:       add     x0, x0, #0x678
         : 7527             ftrace_kill():
         : 8040             ftrace_trace_function = ftrace_stub;
    0.00 :   ffff80000815bfc0:       str     x3, [x25, #192]
         : 8039             ftrace_enabled = 0;
    0.00 :   ffff80000815bfc4:       stp     w4, wzr, [x25, #200]
         : 8041             __ftrace_ops_list_func():
         : 7522             pr_warn("op=%p %pS\n", op, op);
    0.00 :   ffff80000815bfc8:       bl      ffff800008ad5220 <_printk>
         : 7523             goto out;
    0.00 :   ffff80000815bfcc:       b       ffff80000815bf08 <arch_ftrace_ops_list_func+0x118>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   86.49 preempt.h:62
    9.01 preempt.h:49
    4.50 trampoline.c:1065
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (22 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          1f1bfc: 2
                          1f1c08: 1
                          1f1c20: 19
                   h->nr_samples: 22
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081e1bd0 <__bpf_prog_exit>:
         : 6                __bpf_prog_exit():
         : 1061             }
         : 1062             }
         :
         : 1064             void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start, struct bpf_tramp_run_ctx *run_ctx)
         : 1065             __releases(RCU)
         : 1066             {
    0.00 :   ffff8000081e1bd0:       paciasp
    0.00 :   ffff8000081e1bd4:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000081e1bd8:       mov     x29, sp
    0.00 :   ffff8000081e1bdc:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1be0:       mov     x20, x0
         : 1072             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000081e1be4:       mrs     x19, sp_el0
         : 26               __bpf_prog_exit():
         : 1062             bpf_reset_run_ctx(run_ctx->saved_run_ctx);
    0.00 :   ffff8000081e1be8:       ldr     x2, [x2, #8]
         : 1064             bpf_reset_run_ctx():
         : 1541             }
         :
         : 1543             static inline void bpf_reset_run_ctx(struct bpf_run_ctx *old_ctx)
         : 1544             {
         : 1545             #ifdef CONFIG_BPF_SYSCALL
         : 1546             current->bpf_ctx = old_ctx;
    0.00 :   ffff8000081e1bec:       str     x2, [x19, #2664]
         : 1548             __bpf_prog_exit():
         :
         : 1065             update_prog_stats(prog, start);
    0.00 :   ffff8000081e1bf0:       bl      ffff8000081e1974 <update_prog_stats>
         : 1067             __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000081e1bf4:       ldr     w0, [x19, #8]
         : 48               pc += val;
    0.00 :   ffff8000081e1bf8:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    9.01 :   ffff8000081e1bfc:       str     w0, [x19, #8] // preempt.h:49
         : 51               __percpu_add_case_32():
         :
         : 128              PERCPU_RW_OPS(8)
         : 129              PERCPU_RW_OPS(16)
         : 130              PERCPU_RW_OPS(32)
         : 131              PERCPU_RW_OPS(64)
         : 132              PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000081e1c00:       mov     w2, #0xffffffff                 // #-1
         : 134              __kern_my_cpu_offset():
         : 40               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000081e1c04:       mrs     x1, tpidr_el1
         : 42               __bpf_prog_exit():
         : 1065             this_cpu_dec(*(prog->active));
    4.50 :   ffff8000081e1c08:       ldr     x0, [x20, #40] // trampoline.c:1065
         : 1067             __percpu_add_case_32():
         : 127              PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000081e1c0c:       add     x0, x0, x1
    0.00 :   ffff8000081e1c10:       ldxr    w4, [x0]
    0.00 :   ffff8000081e1c14:       add     w4, w4, w2
    0.00 :   ffff8000081e1c18:       stxr    w3, w4, [x0]
    0.00 :   ffff8000081e1c1c:       cbnz    w3, ffff8000081e1c10 <__bpf_prog_exit+0x40>
         : 133              __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
   86.49 :   ffff8000081e1c20:       ldr     x0, [x19, #8] // preempt.h:62
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000081e1c24:       sub     x0, x0, #0x1
    0.00 :   ffff8000081e1c28:       str     w0, [x19, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081e1c2c:       cbnz    x0, ffff8000081e1c4c <__bpf_prog_exit+0x7c>
         : 81               __bpf_prog_exit():
    0.00 :   ffff8000081e1c30:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
         : 1066             migrate_enable();
    0.00 :   ffff8000081e1c34:       bl      ffff800008095e00 <migrate_enable>
         : 1068             rcu_read_unlock():
         : 738              static inline void rcu_read_unlock(void)
         : 739              {
         : 740              RCU_LOCKDEP_WARN(!rcu_is_watching(),
         : 741              "rcu_read_unlock() used illegally while idle");
         : 742              __release(RCU);
         : 743              __rcu_read_unlock();
    0.00 :   ffff8000081e1c38:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 745              __bpf_prog_exit():
         : 1068             rcu_read_unlock();
         : 1069             }
    0.00 :   ffff8000081e1c3c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1c40:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000081e1c44:       autiasp
    0.00 :   ffff8000081e1c48:       ret
         : 1074             __preempt_count_dec_and_test():
    0.00 :   ffff8000081e1c4c:       ldr     x0, [x19, #8]
    0.00 :   ffff8000081e1c50:       cbnz    x0, ffff8000081e1c34 <__bpf_prog_exit+0x64>
         : 76               __bpf_prog_exit():
         : 1065             this_cpu_dec(*(prog->active));
    0.00 :   ffff8000081e1c54:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000081e1c58:       b       ffff8000081e1c34 <__bpf_prog_exit+0x64>

Sorted summary for file bpf_prog_21856463590f61f1_bench_trigger_fentry
----------------------------------------------

   70.70 bpf_prog_21856463590f61f1_bench_trigger_fentry[5c]
   11.75 bpf_prog_21856463590f61f1_bench_trigger_fentry[0]
    5.85 bpf_prog_21856463590f61f1_bench_trigger_fentry[4c]
    5.85 bpf_prog_21856463590f61f1_bench_trigger_fentry[58]
    5.85 bpf_prog_21856463590f61f1_bench_trigger_fentry[24]
 Percent |	Source code & Disassembly of bpf_prog_21856463590f61f1_bench_trigger_fentry for cycles (17 samples, percent: local period)
------------------------------------------------------------------------------------------------------------------------------------------
                               0: 2
                              24: 1
                              4c: 1
                              58: 1
                              5c: 12
                   h->nr_samples: 17
         : 0  int bench_trigger_fentry(void *ctx)
   11.75 :    0:bti	c // bpf_prog_21856463590f61f1_bench_trigger_fentry[0]
    0.00 :    4:add	x9, x30, #0x0
    0.00 :    8:nop
    0.00 :    c:paciasp
    0.00 :   10:stp	x29, x30, [sp, #-16]!
    0.00 :   14:mov	x29, sp
    0.00 :   18:stp	x19, x20, [sp, #-16]!
    0.00 :   1c:stp	x21, x22, [sp, #-16]!
    0.00 :   20:stp	x25, x26, [sp, #-16]!
    5.85 :   24:stp	x27, x28, [sp, #-16]! // bpf_prog_21856463590f61f1_bench_trigger_fentry[24]
    0.00 :   28:mov	x25, sp
    0.00 :   2c:mov	x26, #0x0                   	// #0
    0.00 :   30:bti	j
    0.00 :   34:sub	x27, x25, #0x0
    0.00 :   38:sub	sp, sp, #0x0
    0.00 :   3c:mov	x0, #0x1                   	// #1
         : 0  	__sync_add_and_fetch(&hits, 1);
    0.00 :   40:mov	x1, #0xffff000000000000    	// #-281474976710656
    0.00 :   44:movk	x1, #0x8000, lsl #32
    0.00 :   48:movk	x1, #0x865, lsl #16
    5.85 :   4c:ldxr	x11, [x1] // bpf_prog_21856463590f61f1_bench_trigger_fentry[4c]
    0.00 :   50:add	x11, x11, x0
    0.00 :   54:stxr	w12, x11, [x1]
    5.85 :   58:cbnz	w12, 0x000000000000004c // bpf_prog_21856463590f61f1_bench_trigger_fentry[58]
         : 0  	return 0;
   70.70 :   5c:mov	w7, #0x0                   	// #0 // bpf_prog_21856463590f61f1_bench_trigger_fentry[5c]
    0.00 :   60:mov	sp, sp
    0.00 :   64:ldp	x27, x28, [sp], #16
    0.00 :   68:ldp	x25, x26, [sp], #16
    0.00 :   6c:ldp	x21, x22, [sp], #16
    0.00 :   70:ldp	x19, x20, [sp], #16
    0.00 :   74:ldp	x29, x30, [sp], #16
    0.00 :   78:add	x0, x7, #0x0
    0.00 :   7c:autiasp
    0.00 :   80:ret
    0.00 :   84:nop
    0.00 :   88:ldr	x10, 0x0000000000000090
    0.00 :   8c:br	x10

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   20.72 syscall.c:44
   19.82 syscall.c:71
   19.82 syscall.c:49
   13.21 syscall.c:38
   13.21 barrier.h:93
    6.61 syscall.c:0
    6.61 generic-non-atomic.h:128
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (15 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                           39b70: 1
                           39b7c: 2
                           39b8c: 1
                           39b94: 3
                           39bac: 2
                           39bb8: 2
                           39bc4: 1
                           39bdc: 1
                           39be8: 1
                           39bf0: 1
                   h->nr_samples: 15
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008029b70 <invoke_syscall>:
         : 6                invoke_syscall():
    6.61 :   ffff800008029b70:       nop // syscall.c:0
    0.00 :   ffff800008029b74:       nop
         : 44               }
         :
         : 46               static void invoke_syscall(struct pt_regs *regs, unsigned int scno,
         : 47               unsigned int sc_nr,
         : 48               const syscall_fn_t syscall_table[])
         : 49               {
    0.00 :   ffff800008029b78:       paciasp
   14.11 :   ffff800008029b7c:       stp     x29, x30, [sp, #-48]! // syscall.c:44
    0.00 :   ffff800008029b80:       mov     x29, sp
    0.00 :   ffff800008029b84:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800008029b88:       mov     x19, x0
    6.61 :   ffff800008029b8c:       str     x21, [sp, #32]
         : 56               arch_static_branch():
         : 21               #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         : 23               static __always_inline bool arch_static_branch(struct static_key *key,
         : 24               bool branch)
         : 25               {
         : 26               asm_volatile_goto(
    0.00 :   ffff800008029b90:       nop
         : 28               invoke_syscall():
         : 49               long ret;
         :
         : 51               add_random_kstack_offset();
         :
         : 53               if (scno < sc_nr) {
   19.82 :   ffff800008029b94:       cmp     w1, w2 // syscall.c:49
    0.00 :   ffff800008029b98:       b.cs    ffff800008029c64 <invoke_syscall+0xf4>  // b.hs, b.nlast
         : 51               syscall_fn_t syscall_fn;
         : 52               syscall_fn = syscall_table[array_index_nospec(scno, sc_nr)];
    0.00 :   ffff800008029b9c:       mov     w2, w2
    0.00 :   ffff800008029ba0:       mov     w0, w1
         : 55               array_index_mask_nospec():
         : 86               static inline unsigned long array_index_mask_nospec(unsigned long idx,
         : 87               unsigned long sz)
         : 88               {
         : 89               unsigned long mask;
         :
         : 91               asm volatile(
    0.00 :   ffff800008029ba4:       cmp     x0, x2
    0.00 :   ffff800008029ba8:       ngc     x0, xzr
         : 93               "       sbc     %0, xzr, xzr\n"
         : 94               : "=r" (mask)
         : 95               : "r" (idx), "Ir" (sz)
         : 96               : "cc");
         :
         : 98               csdb();
   13.21 :   ffff800008029bac:       csdb // barrier.h:93
         : 100              invoke_syscall():
    0.00 :   ffff800008029bb0:       and     w1, w1, w0
         : 52               __invoke_syscall():
         : 38               return syscall_fn(regs);
    0.00 :   ffff800008029bb4:       mov     x0, x19
   13.21 :   ffff800008029bb8:       ldr     x1, [x3, x1, lsl #3] // syscall.c:38
    0.00 :   ffff800008029bbc:       blr     x1
         : 42               get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800008029bc0:       mrs     x1, sp_el0
         : 26               generic_test_bit():
         : 128              /*
         : 129              * Unlike the bitops with the '__' prefix above, this one *is* atomic,
         : 130              * so `volatile` must always stay here with no cast-aways. See
         : 131              * `Documentation/atomic_bitops.txt` for the details.
         : 132              */
         : 133              return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    6.61 :   ffff800008029bc4:       ldr     x2, [x1] // generic-non-atomic.h:128
    0.00 :   ffff800008029bc8:       and     x1, x0, #0xffffffff
    0.00 :   ffff800008029bcc:       tst     w2, #0x400000
    0.00 :   ffff800008029bd0:       csel    x0, x1, x0, ne  // ne = any
         : 138              syscall_set_return_value():
         : 61               val = error;
         :
         : 63               if (is_compat_thread(task_thread_info(task)))
         : 64               val = lower_32_bits(val);
         :
         : 66               regs->regs[0] = val;
    0.00 :   ffff800008029bd4:       str     x0, [x19]
         : 68               arch_static_branch():
    0.00 :   ffff800008029bd8:       nop
         : 22               invoke_syscall():
         : 71               * 16-byte (i.e. 4-bit) aligned SP at function boundaries.
         : 72               *
         : 73               * The resulting 5 bits of entropy is seen in SP[8:4].
         : 74               */
         : 75               choose_random_kstack_offset(get_random_int() & 0x1FF);
         : 76               }
    6.61 :   ffff800008029bdc:       mov     sp, x29 // syscall.c:71
    0.00 :   ffff800008029be0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008029be4:       ldr     x21, [sp, #32]
    6.61 :   ffff800008029be8:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800008029bec:       autiasp
    6.61 :   ffff800008029bf0:       ret
         : 83               __kern_my_cpu_offset():
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
         : 46               "mrs %0, tpidr_el2",
         : 47               ARM64_HAS_VIRT_HOST_EXTN)
         : 48               : "=r" (off) :
         : 49               "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff800008029bf4:       mov     x20, sp
         : 51               invoke_syscall():
         : 70               choose_random_kstack_offset(get_random_int() & 0x1FF);
    0.00 :   ffff800008029bf8:       adrp    x19, ffff800009384000 <this_cpu_vector>
         : 72               __kern_my_cpu_offset():
         : 40               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008029bfc:       mrs     x0, tpidr_el1
         : 42               invoke_syscall():
    0.00 :   ffff800008029c00:       add     x19, x19, #0x80
    0.00 :   ffff800008029c04:       ldr     w21, [x19, x0]
         : 72               get_random_int():
         : 45               void get_random_bytes(void *buf, size_t len);
         : 46               u32 get_random_u32(void);
         : 47               u64 get_random_u64(void);
         : 48               static inline unsigned int get_random_int(void)
         : 49               {
         : 50               return get_random_u32();
    0.00 :   ffff800008029c08:       bl      ffff8000086bb0b4 <get_random_u32>
         : 52               invoke_syscall():
    0.00 :   ffff800008029c0c:       and     w0, w0, #0x1ff
         : 71               __kern_my_cpu_offset():
    0.00 :   ffff800008029c10:       mrs     x1, tpidr_el1
         : 41               invoke_syscall():
    0.00 :   ffff800008029c14:       eor     w0, w0, w21
    0.00 :   ffff800008029c18:       str     w0, [x19, x1]
         : 71               }
    0.00 :   ffff800008029c1c:       mov     sp, x29
    0.00 :   ffff800008029c20:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008029c24:       ldr     x21, [sp, #32]
    0.00 :   ffff800008029c28:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800008029c2c:       autiasp
    0.00 :   ffff800008029c30:       ret
         : 78               __kern_my_cpu_offset():
         : 44               "Q" (*(const unsigned long *)current_stack_pointer));
    0.00 :   ffff800008029c34:       mov     x4, sp
         : 46               invoke_syscall():
         : 47               add_random_kstack_offset();
    0.00 :   ffff800008029c38:       adrp    x0, ffff800009384000 <this_cpu_vector>
         : 49               __kern_my_cpu_offset():
         : 40               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008029c3c:       mrs     x5, tpidr_el1
         : 42               invoke_syscall():
    0.00 :   ffff800008029c40:       add     x0, x0, #0x80
    0.00 :   ffff800008029c44:       ldr     w0, [x0, x5]
    0.00 :   ffff800008029c48:       and     x0, x0, #0x3ff
    0.00 :   ffff800008029c4c:       add     x0, x0, #0xf
    0.00 :   ffff800008029c50:       and     x0, x0, #0x7f0
    0.00 :   ffff800008029c54:       sub     sp, x4, x0
    0.00 :   ffff800008029c58:       mov     x0, sp
         : 49               if (scno < sc_nr) {
    0.00 :   ffff800008029c5c:       cmp     w1, w2
    0.00 :   ffff800008029c60:       b.cc    ffff800008029b9c <invoke_syscall+0x2c>  // b.lo, b.ul, b.last
         : 52               get_current():
    0.00 :   ffff800008029c64:       mrs     x0, sp_el0
         : 20               generic_test_bit():
    0.00 :   ffff800008029c68:       ldr     x0, [x0]
         : 129              do_ni_syscall():
         : 26               if (is_compat_task()) {
    0.00 :   ffff800008029c6c:       tst     w0, #0x400000
    0.00 :   ffff800008029c70:       b.eq    ffff800008029c84 <invoke_syscall+0x114>  // b.none
         : 27               ret = compat_arm_syscall(regs, scno);
    0.00 :   ffff800008029c74:       mov     x0, x19
    0.00 :   ffff800008029c78:       bl      ffff80000802da20 <compat_arm_syscall>
         : 28               if (ret != -ENOSYS)
    0.00 :   ffff800008029c7c:       cmn     x0, #0x26
    0.00 :   ffff800008029c80:       b.ne    ffff800008029bc0 <invoke_syscall+0x50>  // b.any
         : 33               return sys_ni_syscall();
    0.00 :   ffff800008029c84:       bl      ffff800008082240 <sys_ni_syscall>
    0.00 :   ffff800008029c88:       b       ffff800008029bc0 <invoke_syscall+0x50>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   20.00 radix-tree.c:758
   20.00 radix-tree.c:768
   20.00 radix-tree.c:748
   13.33 radix-tree.c:87
    6.67 radix-tree.c:67
    6.67 radix-tree.c:86
    6.67 radix-tree.c:777
    6.67 radix-tree.c:774
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (15 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          5623d0: 2
                          5623e0: 1
                          562420: 1
                          562428: 1
                          562440: 2
                          562450: 3
                          56245c: 1
                          562464: 1
                          56247c: 3
                   h->nr_samples: 15
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000085523d0 <__radix_tree_lookup>:
         : 6                __radix_tree_lookup():
         : 748              *       pointing to a node, in which case *@nodep will be NULL.
         : 749              */
         : 750              void *__radix_tree_lookup(const struct radix_tree_root *root,
         : 751              unsigned long index, struct radix_tree_node **nodep,
         : 752              void __rcu ***slotp)
         : 753              {
   13.33 :   ffff8000085523d0:       bti     c // radix-tree.c:748
    0.00 :   ffff8000085523d4:       mov     x9, x0
         : 755              unsigned long maxindex;
         : 756              void __rcu **slot;
         :
         : 758              restart:
         : 759              parent = NULL;
         : 760              slot = (void __rcu **)&root->xa_head;
    0.00 :   ffff8000085523d8:       add     x10, x0, #0x8
         : 762              shift_maxindex():
         : 211              return (RADIX_TREE_MAP_SIZE << shift) - 1;
    0.00 :   ffff8000085523dc:       mov     x11, #0x40                      // #64
         : 213              __radix_tree_lookup():
         : 748              {
    6.67 :   ffff8000085523e0:       paciasp
         : 750              radix_tree_load_root():
         : 389              struct radix_tree_node *node = rcu_dereference_raw(root->xa_head);
    0.00 :   ffff8000085523e4:       ldr     x0, [x9, #8]
         : 391              radix_tree_is_internal_node():
         : 57               #define RADIX_TREE_ENTRY_MASK           3UL
         : 58               #define RADIX_TREE_INTERNAL_NODE        2UL
         :
         : 60               static inline bool radix_tree_is_internal_node(void *ptr)
         : 61               {
         : 62               return ((unsigned long)ptr & RADIX_TREE_ENTRY_MASK) ==
    0.00 :   ffff8000085523e8:       and     x6, x0, #0x3
         : 64               radix_tree_load_root():
         : 393              if (likely(radix_tree_is_internal_node(node))) {
    0.00 :   ffff8000085523ec:       cmp     x6, #0x2
    0.00 :   ffff8000085523f0:       b.ne    ffff80000855246c <__radix_tree_lookup+0x9c>  // b.any
         : 396              entry_to_node():
         : 67               return (void *)((unsigned long)ptr & ~RADIX_TREE_INTERNAL_NODE);
    0.00 :   ffff8000085523f4:       and     x4, x0, #0xfffffffffffffffd
         : 69               node_maxindex():
         : 216              return shift_maxindex(node->shift);
    0.00 :   ffff8000085523f8:       ldrb    w4, [x4]
         : 218              shift_maxindex():
         : 211              return (RADIX_TREE_MAP_SIZE << shift) - 1;
    0.00 :   ffff8000085523fc:       lsl     x4, x11, x4
    0.00 :   ffff800008552400:       sub     x4, x4, #0x1
         : 214              __radix_tree_lookup():
         : 757              radix_tree_load_root(root, &node, &maxindex);
         : 758              if (index > maxindex)
    0.00 :   ffff800008552404:       cmp     x1, x4
    0.00 :   ffff800008552408:       b.hi    ffff80000855247c <__radix_tree_lookup+0xac>  // b.pmore
         : 755              slot = (void __rcu **)&root->xa_head;
    0.00 :   ffff80000855240c:       mov     x4, x10
         : 754              parent = NULL;
    0.00 :   ffff800008552410:       mov     x5, #0x0                        // #0
    0.00 :   ffff800008552414:       nop
         : 760              return NULL;
         :
         : 762              while (radix_tree_is_internal_node(node)) {
    0.00 :   ffff800008552418:       cmp     x6, #0x2
    0.00 :   ffff80000855241c:       b.ne    ffff800008552454 <__radix_tree_lookup+0x84>  // b.any
         : 765              entry_to_node():
         : 67               return (void *)((unsigned long)ptr & ~RADIX_TREE_INTERNAL_NODE);
    6.67 :   ffff800008552420:       and     x5, x0, #0xfffffffffffffffd // radix-tree.c:67
         : 69               __radix_tree_lookup():
         : 765              unsigned offset;
         :
         : 767              parent = entry_to_node(node);
         : 768              offset = radix_tree_descend(parent, &node, index);
         : 769              slot = parent->slots + offset;
    0.00 :   ffff800008552424:       add     x8, x5, #0x28
         : 771              radix_tree_descend():
         : 86               unsigned int offset = (index >> parent->shift) & RADIX_TREE_MAP_MASK;
    6.67 :   ffff800008552428:       ldrb    w7, [x5] // radix-tree.c:86
    0.00 :   ffff80000855242c:       lsr     x4, x1, x7
         : 87               void __rcu **entry = rcu_dereference_raw(parent->slots[offset]);
    0.00 :   ffff800008552430:       and     x4, x4, #0x3f
    0.00 :   ffff800008552434:       add     x6, x4, #0x4
         : 90               __radix_tree_lookup():
         : 765              slot = parent->slots + offset;
    0.00 :   ffff800008552438:       add     x4, x8, x4, lsl #3
         : 767              radix_tree_descend():
         : 87               void __rcu **entry = rcu_dereference_raw(parent->slots[offset]);
    0.00 :   ffff80000855243c:       add     x6, x5, x6, lsl #3
   13.33 :   ffff800008552440:       ldr     x0, [x6, #8] // radix-tree.c:87
         : 90               __radix_tree_lookup():
         : 766              if (node == RADIX_TREE_RETRY)
    0.00 :   ffff800008552444:       and     x6, x0, #0x3
    0.00 :   ffff800008552448:       cmp     x0, #0x402
    0.00 :   ffff80000855244c:       b.eq    ffff8000085523e4 <__radix_tree_lookup+0x14>  // b.none
         : 768              goto restart;
         : 769              if (parent->shift == 0)
   20.00 :   ffff800008552450:       cbnz    w7, ffff800008552418 <__radix_tree_lookup+0x48> // radix-tree.c:768
         : 772              break;
         : 773              }
         :
         : 775              if (nodep)
    0.00 :   ffff800008552454:       cbz     x2, ffff80000855245c <__radix_tree_lookup+0x8c>
         : 773              *nodep = parent;
    0.00 :   ffff800008552458:       str     x5, [x2]
         : 774              if (slotp)
    6.67 :   ffff80000855245c:       cbz     x3, ffff800008552464 <__radix_tree_lookup+0x94> // radix-tree.c:774
         : 775              *slotp = slot;
    0.00 :   ffff800008552460:       str     x4, [x3]
         : 777              return node;
         : 778              }
    6.67 :   ffff800008552464:       autiasp // radix-tree.c:777
    0.00 :   ffff800008552468:       ret
         : 757              if (index > maxindex)
    0.00 :   ffff80000855246c:       cbnz    x1, ffff80000855247c <__radix_tree_lookup+0xac>
         : 755              slot = (void __rcu **)&root->xa_head;
    0.00 :   ffff800008552470:       mov     x4, x10
         : 754              parent = NULL;
    0.00 :   ffff800008552474:       mov     x5, #0x0                        // #0
    0.00 :   ffff800008552478:       b       ffff800008552454 <__radix_tree_lookup+0x84>
         : 758              return NULL;
   20.00 :   ffff80000855247c:       mov     x0, #0x0                        // #0 // radix-tree.c:758
    0.00 :   ffff800008552480:       b       ffff800008552464 <__radix_tree_lookup+0x94>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   73.33 trampoline.c:118
   13.33 refcount.h:199
    6.67 trampoline.c:137
    6.67 trampoline.c:107
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (15 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          1f1800: 2
                          1f1824: 1
                          1f182c: 9
                          1f1830: 2
                          1f18b8: 1
                   h->nr_samples: 15
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081e1800 <call_bpf_prog.isra.0>:
         : 6                call_bpf_prog.isra.0():
         :
         : 200              if (oldp)
         : 201              *oldp = old;
         :
         : 203              if (unlikely(!old))
         : 204              refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
   13.33 :   ffff8000081e1800:       nop // refcount.h:199
    0.00 :   ffff8000081e1804:       nop
         : 207              call_bpf_prog():
         :
         : 108              mutex_unlock(&tr->mutex);
         : 109              return ret;
         : 110              }
         : 111              #else
         : 112              static unsigned int call_bpf_prog(struct bpf_tramp_link *l,
    0.00 :   ffff8000081e1808:       paciasp
    0.00 :   ffff8000081e180c:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000081e1810:       mov     x29, sp
    0.00 :   ffff8000081e1814:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1818:       mov     x19, x0
    0.00 :   ffff8000081e181c:       mov     x20, x2
    0.00 :   ffff8000081e1820:       stp     x21, x22, [sp, #32]
    6.67 :   ffff8000081e1824:       stp     x23, x24, [sp, #48] // trampoline.c:107
    0.00 :   ffff8000081e1828:       mov     x24, x3
         : 118              struct bpf_tramp_run_ctx *run_ctx) = __bpf_prog_exit;
         : 119              struct bpf_prog *p = l->link.prog;
         : 120              unsigned int ret;
         : 121              u64 start_time;
         :
         : 123              if (p->aux->sleepable) {
   60.00 :   ffff8000081e182c:       ldr     x0, [x0, #56] // trampoline.c:118
   13.33 :   ffff8000081e1830:       ldrb    w0, [x0, #140]
    0.00 :   ffff8000081e1834:       cbnz    w0, ffff8000081e1858 <call_bpf_prog.isra.0+0x58>
         : 121              enter = __bpf_prog_enter_sleepable;
         : 122              exit = __bpf_prog_exit_sleepable;
         : 123              } else if (p->expected_attach_type == BPF_LSM_CGROUP) {
    0.00 :   ffff8000081e1838:       ldr     w0, [x19, #8]
    0.00 :   ffff8000081e183c:       cmp     w0, #0x2b
    0.00 :   ffff8000081e1840:       b.eq    ffff8000081e18c4 <call_bpf_prog.isra.0+0xc4>  // b.none
         : 112              void (*exit)(struct bpf_prog *prog, u64 start,
    0.00 :   ffff8000081e1844:       adrp    x22, ffff8000081e1000 <print_bpf_insn+0x580>
         : 110              u64 (*enter)(struct bpf_prog *prog,
    0.00 :   ffff8000081e1848:       adrp    x2, ffff8000081e1000 <print_bpf_insn+0x580>
         : 112              void (*exit)(struct bpf_prog *prog, u64 start,
    0.00 :   ffff8000081e184c:       add     x22, x22, #0xbd0
         : 110              u64 (*enter)(struct bpf_prog *prog,
    0.00 :   ffff8000081e1850:       add     x2, x2, #0xd20
    0.00 :   ffff8000081e1854:       b       ffff8000081e1868 <call_bpf_prog.isra.0+0x68>
         : 120              exit = __bpf_prog_exit_sleepable;
    0.00 :   ffff8000081e1858:       adrp    x22, ffff8000081e1000 <print_bpf_insn+0x580>
         : 119              enter = __bpf_prog_enter_sleepable;
    0.00 :   ffff8000081e185c:       adrp    x2, ffff8000081e1000 <print_bpf_insn+0x580>
         : 120              exit = __bpf_prog_exit_sleepable;
    0.00 :   ffff8000081e1860:       add     x22, x22, #0xc60
         : 119              enter = __bpf_prog_enter_sleepable;
    0.00 :   ffff8000081e1864:       add     x2, x2, #0xe10
         : 126              enter = __bpf_prog_enter_lsm_cgroup;
         : 127              exit = __bpf_prog_exit_lsm_cgroup;
         : 128              }
         :
         : 130              ctx->bpf_cookie = l->cookie;
    0.00 :   ffff8000081e1868:       str     x1, [x20]
         :
         : 129              start_time = enter(p, ctx);
    0.00 :   ffff8000081e186c:       mov     x0, x19
    0.00 :   ffff8000081e1870:       mov     x1, x20
         : 130              if (!start_time)
         : 131              return 0;
    0.00 :   ffff8000081e1874:       mov     w23, #0x0                       // #0
         : 128              start_time = enter(p, ctx);
    0.00 :   ffff8000081e1878:       blr     x2
    0.00 :   ffff8000081e187c:       mov     x21, x0
         : 129              if (!start_time)
    0.00 :   ffff8000081e1880:       cbz     x0, ffff8000081e18a8 <call_bpf_prog.isra.0+0xa8>
         :
         : 133              ret = p->bpf_func(args, p->insnsi);
    0.00 :   ffff8000081e1884:       ldr     x2, [x19, #48]
    0.00 :   ffff8000081e1888:       add     x1, x19, #0x48
    0.00 :   ffff8000081e188c:       mov     x0, x24
    0.00 :   ffff8000081e1890:       blr     x2
    0.00 :   ffff8000081e1894:       mov     w23, w0
         :
         : 135              exit(p, start_time, ctx);
    0.00 :   ffff8000081e1898:       mov     x2, x20
    0.00 :   ffff8000081e189c:       mov     x1, x21
    0.00 :   ffff8000081e18a0:       mov     x0, x19
    0.00 :   ffff8000081e18a4:       blr     x22
         :
         : 138              return ret;
         : 139              }
    0.00 :   ffff8000081e18a8:       mov     w0, w23
    0.00 :   ffff8000081e18ac:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e18b0:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000081e18b4:       ldp     x23, x24, [sp, #48]
    6.67 :   ffff8000081e18b8:       ldp     x29, x30, [sp], #64 // trampoline.c:137
    0.00 :   ffff8000081e18bc:       autiasp
    0.00 :   ffff8000081e18c0:       ret
         : 123              exit = __bpf_prog_exit_lsm_cgroup;
    0.00 :   ffff8000081e18c4:       adrp    x22, ffff8000081e1000 <print_bpf_insn+0x580>
         : 122              enter = __bpf_prog_enter_lsm_cgroup;
    0.00 :   ffff8000081e18c8:       adrp    x2, ffff8000081e1000 <print_bpf_insn+0x580>
         : 123              exit = __bpf_prog_exit_lsm_cgroup;
    0.00 :   ffff8000081e18cc:       add     x22, x22, #0x200
         : 122              enter = __bpf_prog_enter_lsm_cgroup;
    0.00 :   ffff8000081e18d0:       add     x2, x2, #0x1c0
    0.00 :   ffff8000081e18d4:       b       ffff8000081e1868 <call_bpf_prog.isra.0+0x68>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   64.29 preempt.h:62
   14.29 percpu.h:130
    7.14 trampoline.c:1028
    7.14 bpf.h:1532
    7.14 jump_label.h:21
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (14 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          1f1d44: 1
                          1f1d4c: 1
                          1f1d6c: 2
                          1f1d7c: 9
                          1f1d98: 1
                   h->nr_samples: 14
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081e1d20 <__bpf_prog_enter>:
         : 6                __bpf_prog_enter():
         : 1024             * [2..MAX_U64] - execute bpf prog and record execution time.
         : 1025             *     This is start time.
         : 1026             */
         : 1027             u64 notrace __bpf_prog_enter(struct bpf_prog *prog, struct bpf_tramp_run_ctx *run_ctx)
         : 1028             __acquires(RCU)
         : 1029             {
    0.00 :   ffff8000081e1d20:       paciasp
    0.00 :   ffff8000081e1d24:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000081e1d28:       mov     x29, sp
    0.00 :   ffff8000081e1d2c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1d30:       mov     x19, x1
    0.00 :   ffff8000081e1d34:       mov     x20, x0
         : 1036             rcu_read_lock():
         : 704              * read-side critical sections may be preempted and they may also block, but
         : 705              * only when acquiring spinlocks that are subject to priority inheritance.
         : 706              */
         : 707              static __always_inline void rcu_read_lock(void)
         : 708              {
         : 709              __rcu_read_lock();
    0.00 :   ffff8000081e1d38:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 711              __bpf_prog_enter():
         : 1026             rcu_read_lock();
         : 1027             migrate_disable();
    0.00 :   ffff8000081e1d3c:       bl      ffff800008091dc0 <migrate_disable>
         : 1029             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000081e1d40:       mrs     x2, sp_el0
         : 26               bpf_set_run_ctx():
         : 1532             static inline struct bpf_run_ctx *bpf_set_run_ctx(struct bpf_run_ctx *new_ctx)
         : 1533             {
         : 1534             struct bpf_run_ctx *old_ctx = NULL;
         :
         : 1536             #ifdef CONFIG_BPF_SYSCALL
         : 1537             old_ctx = current->bpf_ctx;
    7.14 :   ffff8000081e1d44:       ldr     x0, [x2, #2664] // bpf.h:1532
         : 1533             current->bpf_ctx = new_ctx;
    0.00 :   ffff8000081e1d48:       str     x19, [x2, #2664]
         : 1535             __bpf_prog_enter():
         :
         : 1029             run_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);
    7.14 :   ffff8000081e1d4c:       str     x0, [x19, #8] // trampoline.c:1028
         : 1031             __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000081e1d50:       ldr     w0, [x2, #8]
         : 48               pc += val;
    0.00 :   ffff8000081e1d54:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff8000081e1d58:       str     w0, [x2, #8]
         : 51               __percpu_add_return_case_32():
         : 130              PERCPU_RW_OPS(32)
         : 131              PERCPU_RW_OPS(64)
         : 132              PERCPU_OP(add, add, stadd)
         : 133              PERCPU_OP(andnot, bic, stclr)
         : 134              PERCPU_OP(or, orr, stset)
         : 135              PERCPU_RET_OP(add, add, ldadd)
    0.00 :   ffff8000081e1d5c:       mov     w0, #0x1                        // #1
         : 137              __kern_my_cpu_offset():
         : 40               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000081e1d60:       mrs     x3, tpidr_el1
         : 42               __bpf_prog_enter():
         :
         : 1031             if (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {
    0.00 :   ffff8000081e1d64:       ldr     x1, [x20, #40]
         : 1033             __percpu_add_return_case_32():
         : 130              PERCPU_RET_OP(add, add, ldadd)
    0.00 :   ffff8000081e1d68:       add     x1, x1, x3
   14.29 :   ffff8000081e1d6c:       ldxr    w19, [x1] // percpu.h:130
    0.00 :   ffff8000081e1d70:       add     w19, w19, w0
    0.00 :   ffff8000081e1d74:       stxr    w4, w19, [x1]
    0.00 :   ffff8000081e1d78:       cbnz    w4, ffff8000081e1d6c <__bpf_prog_enter+0x4c>
         : 136              __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
   64.29 :   ffff8000081e1d7c:       ldr     x1, [x2, #8] // preempt.h:62
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000081e1d80:       sub     x1, x1, #0x1
    0.00 :   ffff8000081e1d84:       str     w1, [x2, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081e1d88:       cbnz    x1, ffff8000081e1db0 <__bpf_prog_enter+0x90>
         : 81               __bpf_prog_enter():
    0.00 :   ffff8000081e1d8c:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000081e1d90:       cmp     w19, #0x1
    0.00 :   ffff8000081e1d94:       b.ne    ffff8000081e1ddc <__bpf_prog_enter+0xbc>  // b.any
         : 1032             arch_static_branch():
         : 21               #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         : 23               static __always_inline bool arch_static_branch(struct static_key *key,
         : 24               bool branch)
         : 25               {
         : 26               asm_volatile_goto(
    7.14 :   ffff8000081e1d98:       nop // jump_label.h:21
         : 28               bpf_prog_start_time():
         : 988              u64 start = NO_START_TIME;
    0.00 :   ffff8000081e1d9c:       mov     x0, #0x1                        // #1
         : 990              __bpf_prog_enter():
         : 1035             inc_misses_counter(prog);
         : 1036             return 0;
         : 1037             }
         : 1038             return bpf_prog_start_time();
         : 1039             }
    0.00 :   ffff8000081e1da0:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1da4:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000081e1da8:       autiasp
    0.00 :   ffff8000081e1dac:       ret
         : 1044             __preempt_count_dec_and_test():
    0.00 :   ffff8000081e1db0:       ldr     x0, [x2, #8]
    0.00 :   ffff8000081e1db4:       cbnz    x0, ffff8000081e1d90 <__bpf_prog_enter+0x70>
         : 76               __bpf_prog_enter():
         : 1030             if (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {
    0.00 :   ffff8000081e1db8:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000081e1dbc:       b       ffff8000081e1d90 <__bpf_prog_enter+0x70>
         : 1033             bpf_prog_start_time():
         : 991              start = sched_clock();
    0.00 :   ffff8000081e1dc0:       bl      ffff800008117490 <sched_clock>
         : 992              if (unlikely(!start))
    0.00 :   ffff8000081e1dc4:       cmp     x0, #0x0
    0.00 :   ffff8000081e1dc8:       csinc   x0, x0, xzr, ne // ne = any
         : 995              __bpf_prog_enter():
         : 1035             }
    0.00 :   ffff8000081e1dcc:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1dd0:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000081e1dd4:       autiasp
    0.00 :   ffff8000081e1dd8:       ret
         : 1040             __kern_my_cpu_offset():
         : 40               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000081e1ddc:       mrs     x1, tpidr_el1
         : 42               inc_misses_counter():
         : 1003             stats = this_cpu_ptr(prog->stats);
    0.00 :   ffff8000081e1de0:       ldr     x0, [x20, #32]
    0.00 :   ffff8000081e1de4:       add     x0, x0, x1
         : 1006             arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff8000081e1de8:       b       ffff8000081e1e00 <__bpf_prog_enter+0xe0>
         : 45               __lse_atomic64_add():
         : 127              }
         :
         : 129              ATOMIC64_OP(andnot, stclr)
         : 130              ATOMIC64_OP(or, stset)
         : 131              ATOMIC64_OP(xor, steor)
         : 132              ATOMIC64_OP(add, stadd)
    0.00 :   ffff8000081e1dec:       add     x0, x0, #0x10
    0.00 :   ffff8000081e1df0:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000081e1df4:       stadd   x1, [x0]
         : 136              __bpf_prog_enter():
         : 1032             return 0;
    0.00 :   ffff8000081e1df8:       mov     x0, #0x0                        // #0
         : 1034             __lse_atomic64_add():
    0.00 :   ffff8000081e1dfc:       b       ffff8000081e1da0 <__bpf_prog_enter+0x80>
         : 128              __ll_sc_atomic64_add():
         : 210              ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         : 211              ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         : 212              ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         : 213              ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         : 215              ATOMIC64_OPS(add, add, I)
    0.00 :   ffff8000081e1e00:       add     x0, x0, #0x10
    0.00 :   ffff8000081e1e04:       b       ffff8000081e2960 <__bpf_tramp_exit+0x18c>
         : 218              __bpf_prog_enter():
    0.00 :   ffff8000081e1e08:       mov     x0, #0x0                        // #0
         : 1033             __ll_sc_atomic64_add():
    0.00 :   ffff8000081e1e0c:       b       ffff8000081e1da0 <__bpf_prog_enter+0x80>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   58.33 trace.h:813
    8.33 ftrace.c:1117
    8.33 ftrace.c:1115
    8.33 ftrace.c:1443
    8.33 ftrace.c:1100
    8.33 ftrace.c:1118
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (12 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          16b534: 1
                          16b53c: 3
                          16b54c: 4
                          16b5bc: 1
                          16b5c4: 1
                          16b5cc: 1
                          16b5d4: 1
                   h->nr_samples: 12
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000815b530 <hash_contains_ip.isra.0>:
         : 6                hash_contains_ip():
         : 1443             ftrace_hash_rec_enable_modify(ops, enable);
         :
         : 1445             return 0;
         : 1446             }
         :
         : 1448             static bool hash_contains_ip(unsigned long ip,
    0.00 :   ffff80000815b530:       mov     x3, x0
    8.33 :   ffff80000815b534:       paciasp // ftrace.c:1443
         : 1451             ftrace_hash_empty():
         : 813              struct ftrace_func_entry *
         : 814              ftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip);
         :
         : 816              static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
         : 817              {
         : 818              return !hash || !(hash->count || (hash->flags & FTRACE_HASH_FL_MOD));
    0.00 :   ffff80000815b538:       cbz     x2, ffff80000815b54c <hash_contains_ip.isra.0+0x1c>
   25.00 :   ffff80000815b53c:       ldr     x0, [x2, #16] // trace.h:813
    0.00 :   ffff80000815b540:       cbnz    x0, ffff80000815b5bc <hash_contains_ip.isra.0+0x8c>
    0.00 :   ffff80000815b544:       ldr     x0, [x2, #24]
    0.00 :   ffff80000815b548:       tbnz    w0, #0, ffff80000815b5bc <hash_contains_ip.isra.0+0x8c>
   33.33 :   ffff80000815b54c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80000815b550:       cbz     x1, ffff80000815b5b4 <hash_contains_ip.isra.0+0x84>
    0.00 :   ffff80000815b554:       ldr     x2, [x1, #16]
    0.00 :   ffff80000815b558:       cbz     x2, ffff80000815b5f8 <hash_contains_ip.isra.0+0xc8>
         : 828              ftrace_hash_key():
         : 1100             if (hash->size_bits > 0)
    0.00 :   ffff80000815b55c:       ldr     x2, [x1]
    0.00 :   ffff80000815b560:       cbz     x2, ffff80000815b588 <hash_contains_ip.isra.0+0x58>
         : 1103             hash_64_generic():
         : 78               #endif
         : 79               static __always_inline u32 hash_64_generic(u64 val, unsigned int bits)
         : 80               {
         : 81               #if BITS_PER_LONG == 64
         : 82               /* 64x64-bit multiply is efficient on all 64-bit processors */
         : 83               return val * GOLDEN_RATIO_64 >> (64 - bits);
    0.00 :   ffff80000815b564:       mov     x0, #0x83eb                     // #33771
    0.00 :   ffff80000815b568:       mov     w4, #0x40                       // #64
    0.00 :   ffff80000815b56c:       movk    x0, #0x80b5, lsl #16
    0.00 :   ffff80000815b570:       sub     w2, w4, w2
    0.00 :   ffff80000815b574:       movk    x0, #0x8646, lsl #32
    0.00 :   ffff80000815b578:       movk    x0, #0x61c8, lsl #48
    0.00 :   ffff80000815b57c:       mul     x0, x3, x0
    0.00 :   ffff80000815b580:       lsr     x0, x0, x2
         : 92               __ftrace_lookup_ip():
         : 1115             hhd = &hash->buckets[key];
    0.00 :   ffff80000815b584:       ubfiz   x2, x0, #3, #32
    0.00 :   ffff80000815b588:       ldr     x1, [x1, #8]
         : 1117             hlist_for_each_entry_rcu_notrace(entry, hhd, hlist) {
    0.00 :   ffff80000815b58c:       mov     w0, #0x1                        // #1
    0.00 :   ffff80000815b590:       ldr     x1, [x1, x2]
    0.00 :   ffff80000815b594:       cbnz    x1, ffff80000815b5a4 <hash_contains_ip.isra.0+0x74>
    0.00 :   ffff80000815b598:       b       ffff80000815b5b4 <hash_contains_ip.isra.0+0x84>
    0.00 :   ffff80000815b59c:       ldr     x1, [x1]
    0.00 :   ffff80000815b5a0:       cbz     x1, ffff80000815b640 <hash_contains_ip.isra.0+0x110>
         : 1118             if (entry->ip == ip)
    0.00 :   ffff80000815b5a4:       ldr     x0, [x1, #16]
    0.00 :   ffff80000815b5a8:       cmp     x3, x0
    0.00 :   ffff80000815b5ac:       b.ne    ffff80000815b59c <hash_contains_ip.isra.0+0x6c>  // b.any
    0.00 :   ffff80000815b5b0:       mov     w0, #0x0                        // #0
         : 1123             hash_contains_ip():
         : 1456             */
         : 1457             return (ftrace_hash_empty(hash->filter_hash) ||
         : 1458             __ftrace_lookup_ip(hash->filter_hash, ip)) &&
         : 1459             (ftrace_hash_empty(hash->notrace_hash) ||
         : 1460             !__ftrace_lookup_ip(hash->notrace_hash, ip));
         : 1461             }
    0.00 :   ffff80000815b5b4:       autiasp
    0.00 :   ffff80000815b5b8:       ret
         : 1464             ftrace_hash_key():
         : 1100             if (hash->size_bits > 0)
    8.33 :   ffff80000815b5bc:       ldr     x4, [x2] // ftrace.c:1100
    0.00 :   ffff80000815b5c0:       cbnz    x4, ffff80000815b608 <hash_contains_ip.isra.0+0xd8>
         : 1103             __ftrace_lookup_ip():
         : 1115             hhd = &hash->buckets[key];
    8.33 :   ffff80000815b5c4:       ldr     x2, [x2, #8] // ftrace.c:1115
         : 1117             hlist_for_each_entry_rcu_notrace(entry, hhd, hlist) {
    0.00 :   ffff80000815b5c8:       mov     w0, #0x0                        // #0
    8.33 :   ffff80000815b5cc:       ldr     x2, [x2, x4] // ftrace.c:1117
    0.00 :   ffff80000815b5d0:       cbz     x2, ffff80000815b5b4 <hash_contains_ip.isra.0+0x84>
         : 1118             if (entry->ip == ip)
    8.33 :   ffff80000815b5d4:       ldr     x0, [x2, #16] // ftrace.c:1118
    0.00 :   ffff80000815b5d8:       cmp     x3, x0
    0.00 :   ffff80000815b5dc:       b.eq    ffff80000815b54c <hash_contains_ip.isra.0+0x1c>  // b.none
         : 1117             hlist_for_each_entry_rcu_notrace(entry, hhd, hlist) {
    0.00 :   ffff80000815b5e0:       ldr     x2, [x2]
    0.00 :   ffff80000815b5e4:       cbz     x2, ffff80000815b5b0 <hash_contains_ip.isra.0+0x80>
         : 1118             if (entry->ip == ip)
    0.00 :   ffff80000815b5e8:       ldr     x0, [x2, #16]
    0.00 :   ffff80000815b5ec:       cmp     x3, x0
    0.00 :   ffff80000815b5f0:       b.ne    ffff80000815b5e0 <hash_contains_ip.isra.0+0xb0>  // b.any
    0.00 :   ffff80000815b5f4:       b       ffff80000815b54c <hash_contains_ip.isra.0+0x1c>
         : 1123             ftrace_hash_empty():
    0.00 :   ffff80000815b5f8:       ldr     x2, [x1, #24]
    0.00 :   ffff80000815b5fc:       tbnz    w2, #0, ffff80000815b55c <hash_contains_ip.isra.0+0x2c>
         : 815              hash_contains_ip():
         : 1456             }
    0.00 :   ffff80000815b600:       autiasp
    0.00 :   ffff80000815b604:       ret
         : 1459             hash_64_generic():
    0.00 :   ffff80000815b608:       mov     x0, #0x83eb                     // #33771
    0.00 :   ffff80000815b60c:       mov     w5, #0x40                       // #64
    0.00 :   ffff80000815b610:       movk    x0, #0x80b5, lsl #16
    0.00 :   ffff80000815b614:       sub     w4, w5, w4
    0.00 :   ffff80000815b618:       movk    x0, #0x8646, lsl #32
    0.00 :   ffff80000815b61c:       movk    x0, #0x61c8, lsl #48
         : 84               __ftrace_lookup_ip():
         : 1115             hhd = &hash->buckets[key];
    0.00 :   ffff80000815b620:       ldr     x2, [x2, #8]
         : 1117             hash_64_generic():
    0.00 :   ffff80000815b624:       mul     x0, x3, x0
    0.00 :   ffff80000815b628:       lsr     x0, x0, x4
         : 80               __ftrace_lookup_ip():
    0.00 :   ffff80000815b62c:       ubfiz   x4, x0, #3, #32
         : 1117             hlist_for_each_entry_rcu_notrace(entry, hhd, hlist) {
    0.00 :   ffff80000815b630:       mov     w0, #0x0                        // #0
    0.00 :   ffff80000815b634:       ldr     x2, [x2, x4]
    0.00 :   ffff80000815b638:       cbnz    x2, ffff80000815b5d4 <hash_contains_ip.isra.0+0xa4>
    0.00 :   ffff80000815b63c:       b       ffff80000815b5b4 <hash_contains_ip.isra.0+0x84>
    0.00 :   ffff80000815b640:       mov     w0, #0x1                        // #1
         : 1123             hash_contains_ip():
         : 1456             }
    0.00 :   ffff80000815b644:       autiasp
    0.00 :   ffff80000815b648:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   25.00 trampoline.c:177
   25.00 trampoline.c:182
   16.67 trampoline.c:183
   16.67 trampoline.c:197
   16.67 trampoline.c:180
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (12 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          1f1a54: 2
                          1f1a64: 1
                          1f1ab8: 2
                          1f1ae4: 3
                          1f1af4: 2
                          1f1bcc: 2
                   h->nr_samples: 12
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081e19f0 <bpf_fprobe_entry>:
         : 6                bpf_fprobe_entry():
         : 1057             flags = u64_stats_update_begin_irqsave(&stats->syncp);
         : 1058             u64_stats_inc(&stats->cnt);
         : 1059             u64_stats_add(&stats->nsecs, sched_clock() - start);
         : 1060             u64_stats_update_end_irqrestore(&stats->syncp, flags);
         : 1061             }
         : 1062             }
    0.00 :   ffff8000081e19f0:       bti     c
    0.00 :   ffff8000081e19f4:       nop
    0.00 :   ffff8000081e19f8:       nop
         : 165              {
    0.00 :   ffff8000081e19fc:       paciasp
    0.00 :   ffff8000081e1a00:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff8000081e1a04:       mov     w4, #0x0                        // #0
    0.00 :   ffff8000081e1a08:       mov     x29, sp
    0.00 :   ffff8000081e1a0c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1a10:       mov     x19, x3
    0.00 :   ffff8000081e1a14:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000081e1a18:       mov     x22, x0
    0.00 :   ffff8000081e1a1c:       mov     x21, x2
    0.00 :   ffff8000081e1a20:       stp     x23, x24, [sp, #48]
    0.00 :   ffff8000081e1a24:       str     x25, [sp, #64]
         : 167              struct bpf_fprobe_context *fprobe_ctx = fp->ops.private;
    0.00 :   ffff8000081e1a28:       ldr     x24, [x0, #24]
         : 168              struct bpf_tramp_links *links = fprobe_ctx->links;
    0.00 :   ffff8000081e1a2c:       ldr     x23, [x24]
         : 174              memset(&call_ctx->ctx, 0, sizeof(call_ctx->ctx));
    0.00 :   ffff8000081e1a30:       stp     xzr, xzr, [x3]
         : 175              call_ctx->ip = ip;
    0.00 :   ffff8000081e1a34:       str     x1, [x3, #16]
         : 176              for (i = 0; i < fprobe_ctx->nr_args; i++)
    0.00 :   ffff8000081e1a38:       ldr     w0, [x24, #8]
    0.00 :   ffff8000081e1a3c:       cmp     w0, #0x0
    0.00 :   ffff8000081e1a40:       b.gt    ffff8000081e1a64 <bpf_fprobe_entry+0x74>
    0.00 :   ffff8000081e1a44:       b       ffff8000081e1a90 <bpf_fprobe_entry+0xa0>
         : 177              call_ctx->args[i] = ftrace_regs_get_argument(regs, i);
    0.00 :   ffff8000081e1a48:       ldr     x0, [x21, x1, lsl #3]
    0.00 :   ffff8000081e1a4c:       add     x1, x19, x1, lsl #3
         : 176              for (i = 0; i < fprobe_ctx->nr_args; i++)
    0.00 :   ffff8000081e1a50:       add     w4, w4, #0x1
         : 177              call_ctx->args[i] = ftrace_regs_get_argument(regs, i);
   16.67 :   ffff8000081e1a54:       str     x0, [x1, #24] // trampoline.c:177
         : 176              for (i = 0; i < fprobe_ctx->nr_args; i++)
    0.00 :   ffff8000081e1a58:       ldr     w0, [x24, #8]
    0.00 :   ffff8000081e1a5c:       cmp     w0, w4
    0.00 :   ffff8000081e1a60:       b.le    ffff8000081e1a90 <bpf_fprobe_entry+0xa0>
         : 177              call_ctx->args[i] = ftrace_regs_get_argument(regs, i);
    8.33 :   ffff8000081e1a64:       sxtw    x1, w4
    0.00 :   ffff8000081e1a68:       mov     x0, #0x0                        // #0
    0.00 :   ffff8000081e1a6c:       cmp     w4, #0x7
    0.00 :   ffff8000081e1a70:       b.le    ffff8000081e1a48 <bpf_fprobe_entry+0x58>
    0.00 :   ffff8000081e1a74:       sxtw    x1, w4
         : 176              for (i = 0; i < fprobe_ctx->nr_args; i++)
    0.00 :   ffff8000081e1a78:       add     w4, w4, #0x1
         : 177              call_ctx->args[i] = ftrace_regs_get_argument(regs, i);
    0.00 :   ffff8000081e1a7c:       add     x1, x19, x1, lsl #3
    0.00 :   ffff8000081e1a80:       str     x0, [x1, #24]
         : 176              for (i = 0; i < fprobe_ctx->nr_args; i++)
    0.00 :   ffff8000081e1a84:       ldr     w0, [x24, #8]
    0.00 :   ffff8000081e1a88:       cmp     w0, w4
    0.00 :   ffff8000081e1a8c:       b.gt    ffff8000081e1a64 <bpf_fprobe_entry+0x74>
         : 179              for (i = 0; i < fentry->nr_links; i++)
    0.00 :   ffff8000081e1a90:       ldr     w1, [x23, #304]
         : 185              call_ctx->args);
    0.00 :   ffff8000081e1a94:       add     x25, x19, #0x18
    0.00 :   ffff8000081e1a98:       mov     x20, #0x0                       // #0
         : 179              for (i = 0; i < fentry->nr_links; i++)
    0.00 :   ffff8000081e1a9c:       cmp     w1, #0x0
    0.00 :   ffff8000081e1aa0:       b.le    ffff8000081e1ad4 <bpf_fprobe_entry+0xe4>
    0.00 :   ffff8000081e1aa4:       nop
         : 180              call_bpf_prog(fentry->links[i], &call_ctx->ctx, call_ctx->args);
    0.00 :   ffff8000081e1aa8:       ldr     x1, [x23, x20, lsl #3]
    0.00 :   ffff8000081e1aac:       mov     x3, x25
    0.00 :   ffff8000081e1ab0:       mov     x2, x19
         : 179              for (i = 0; i < fentry->nr_links; i++)
    0.00 :   ffff8000081e1ab4:       add     x20, x20, #0x1
         : 180              call_bpf_prog(fentry->links[i], &call_ctx->ctx, call_ctx->args);
   16.67 :   ffff8000081e1ab8:       ldr     x0, [x1, #24] // trampoline.c:180
    0.00 :   ffff8000081e1abc:       ldr     x1, [x1, #80]
    0.00 :   ffff8000081e1ac0:       bl      ffff8000081e1800 <call_bpf_prog.isra.0>
         : 179              for (i = 0; i < fentry->nr_links; i++)
    0.00 :   ffff8000081e1ac4:       ldr     w0, [x23, #304]
    0.00 :   ffff8000081e1ac8:       cmp     w0, w20
    0.00 :   ffff8000081e1acc:       b.gt    ffff8000081e1aa8 <bpf_fprobe_entry+0xb8>
    0.00 :   ffff8000081e1ad0:       ldr     w0, [x24, #8]
         : 182              call_ctx->args[fprobe_ctx->nr_args] = 0;
    0.00 :   ffff8000081e1ad4:       add     x0, x19, w0, sxtw #3
         : 183              for (i = 0; i < fmod_ret->nr_links; i++) {
    0.00 :   ffff8000081e1ad8:       add     x25, x23, #0x270
         : 185              call_ctx->args);
    0.00 :   ffff8000081e1adc:       add     x24, x19, #0x18
    0.00 :   ffff8000081e1ae0:       mov     x20, #0x0                       // #0
         : 182              call_ctx->args[fprobe_ctx->nr_args] = 0;
   25.00 :   ffff8000081e1ae4:       str     xzr, [x0, #24] // trampoline.c:182
         : 183              for (i = 0; i < fmod_ret->nr_links; i++) {
    0.00 :   ffff8000081e1ae8:       ldr     w0, [x25, #304]
    0.00 :   ffff8000081e1aec:       cmp     w0, #0x0
    0.00 :   ffff8000081e1af0:       b.gt    ffff8000081e1b04 <bpf_fprobe_entry+0x114>
   16.67 :   ffff8000081e1af4:       b       ffff8000081e1ba8 <bpf_fprobe_entry+0x1b8> // trampoline.c:183
    0.00 :   ffff8000081e1af8:       ldr     w0, [x25, #304]
    0.00 :   ffff8000081e1afc:       cmp     w0, w20
    0.00 :   ffff8000081e1b00:       b.le    ffff8000081e1ba8 <bpf_fprobe_entry+0x1b8>
         : 184              ret = call_bpf_prog(fmod_ret->links[i], &call_ctx->ctx,
    0.00 :   ffff8000081e1b04:       ldr     x1, [x25, x20, lsl #3]
    0.00 :   ffff8000081e1b08:       mov     x3, x24
    0.00 :   ffff8000081e1b0c:       mov     x2, x19
         : 183              for (i = 0; i < fmod_ret->nr_links; i++) {
    0.00 :   ffff8000081e1b10:       add     x20, x20, #0x1
         : 184              ret = call_bpf_prog(fmod_ret->links[i], &call_ctx->ctx,
    0.00 :   ffff8000081e1b14:       ldr     x0, [x1, #24]
    0.00 :   ffff8000081e1b18:       ldr     x1, [x1, #80]
    0.00 :   ffff8000081e1b1c:       bl      ffff8000081e1800 <call_bpf_prog.isra.0>
         : 187              if (ret) {
    0.00 :   ffff8000081e1b20:       cbz     w0, ffff8000081e1af8 <bpf_fprobe_entry+0x108>
         : 189              ftrace_override_function_with_return(regs);
    0.00 :   ffff8000081e1b24:       ldr     x2, [x21, #88]
         : 188              ftrace_regs_set_return_value(regs, ret);
    0.00 :   ffff8000081e1b28:       sxtw    x1, w0
    0.00 :   ffff8000081e1b2c:       str     x1, [x21]
         : 191              bpf_fprobe_exit():
         : 160              for (i = 0; i < fexit->nr_links; i++)
    0.00 :   ffff8000081e1b30:       mov     x20, #0x0                       // #0
         : 162              bpf_fprobe_entry():
         : 189              ftrace_override_function_with_return(regs);
    0.00 :   ffff8000081e1b34:       str     x2, [x21, #104]
         : 191              bpf_fprobe_exit():
         : 153              struct bpf_fprobe_context *fprobe_ctx = fp->ops.private;
    0.00 :   ffff8000081e1b38:       ldr     x2, [x22, #24]
         : 158              call_ctx->args[fprobe_ctx->nr_args] = ftrace_regs_return_value(regs);
    0.00 :   ffff8000081e1b3c:       ldrsw   x0, [x2, #8]
         : 154              struct bpf_tramp_links *links = fprobe_ctx->links;
    0.00 :   ffff8000081e1b40:       ldr     x21, [x2]
         : 158              call_ctx->args[fprobe_ctx->nr_args] = ftrace_regs_return_value(regs);
    0.00 :   ffff8000081e1b44:       add     x0, x19, x0, lsl #3
         : 160              for (i = 0; i < fexit->nr_links; i++)
    0.00 :   ffff8000081e1b48:       add     x21, x21, #0x138
         : 158              call_ctx->args[fprobe_ctx->nr_args] = ftrace_regs_return_value(regs);
    0.00 :   ffff8000081e1b4c:       str     x1, [x0, #24]
         : 160              for (i = 0; i < fexit->nr_links; i++)
    0.00 :   ffff8000081e1b50:       ldr     w0, [x21, #304]
    0.00 :   ffff8000081e1b54:       cmp     w0, #0x0
    0.00 :   ffff8000081e1b58:       b.le    ffff8000081e1b88 <bpf_fprobe_entry+0x198>
    0.00 :   ffff8000081e1b5c:       nop
         : 161              call_bpf_prog(fexit->links[i], &call_ctx->ctx, call_ctx->args);
    0.00 :   ffff8000081e1b60:       ldr     x1, [x21, x20, lsl #3]
    0.00 :   ffff8000081e1b64:       mov     x3, x24
    0.00 :   ffff8000081e1b68:       mov     x2, x19
         : 160              for (i = 0; i < fexit->nr_links; i++)
    0.00 :   ffff8000081e1b6c:       add     x20, x20, #0x1
         : 161              call_bpf_prog(fexit->links[i], &call_ctx->ctx, call_ctx->args);
    0.00 :   ffff8000081e1b70:       ldr     x0, [x1, #24]
    0.00 :   ffff8000081e1b74:       ldr     x1, [x1, #80]
    0.00 :   ffff8000081e1b78:       bl      ffff8000081e1800 <call_bpf_prog.isra.0>
         : 160              for (i = 0; i < fexit->nr_links; i++)
    0.00 :   ffff8000081e1b7c:       ldr     w0, [x21, #304]
    0.00 :   ffff8000081e1b80:       cmp     w0, w20
    0.00 :   ffff8000081e1b84:       b.gt    ffff8000081e1b60 <bpf_fprobe_entry+0x170>
         : 164              bpf_fprobe_entry():
         : 192              return false;
    0.00 :   ffff8000081e1b88:       mov     w0, #0x0                        // #0
         : 197              }
    0.00 :   ffff8000081e1b8c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081e1b90:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000081e1b94:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000081e1b98:       ldr     x25, [sp, #64]
    0.00 :   ffff8000081e1b9c:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000081e1ba0:       autiasp
    0.00 :   ffff8000081e1ba4:       ret
         : 196              return fexit->nr_links;
    0.00 :   ffff8000081e1ba8:       ldr     w0, [x23, #616]
         : 197              }
    0.00 :   ffff8000081e1bac:       ldp     x19, x20, [sp, #16]
         : 196              return fexit->nr_links;
    0.00 :   ffff8000081e1bb0:       cmp     w0, #0x0
    0.00 :   ffff8000081e1bb4:       cset    w0, ne  // ne = any
         : 197              }
    0.00 :   ffff8000081e1bb8:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000081e1bbc:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff8000081e1bc0:       ldr     x25, [sp, #64]
    0.00 :   ffff8000081e1bc4:       ldp     x29, x30, [sp], #80
    0.00 :   ffff8000081e1bc8:       autiasp
   16.67 :   ffff8000081e1bcc:       ret // trampoline.c:197

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   16.67 entry-ftrace.S:59
   16.67 entry-ftrace.S:46
   16.67 entry-ftrace.S:56
   16.67 entry-ftrace.S:63
    8.33 entry-ftrace.S:51
    8.33 entry-ftrace.S:52
    8.33 entry-ftrace.S:47
    8.33 entry-ftrace.S:80
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (12 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                           3e0d8: 2
                           3e0dc: 1
                           3e0e4: 1
                           3e0e8: 1
                           3e0f0: 2
                           3e0f4: 2
                           3e0fc: 2
                           3e11c: 1
                   h->nr_samples: 12
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000802e0c4 <ftrace_caller>:
         : 6                ftrace_caller():
         : 35               * for the callsite and the ftrace entry assembly. This is not sufficient for
         : 36               * reliable stacktrace: until we create the callsite stack record, its caller
         : 37               * is missing from the LR and existing chain of frame records.
         : 38               */
         : 39               SYM_CODE_START(ftrace_caller)
         : 40               bti     c
    0.00 :   ffff80000802e0c4:       bti     c
         :
         : 39               /* Save original SP */
         : 40               mov     x10, sp
    0.00 :   ffff80000802e0c8:       mov     x10, sp
         :
         : 42               /* Make room for pt_regs, plus two frame records */
         : 43               sub     sp, sp, #(FREGS_SIZE + 32)
    0.00 :   ffff80000802e0cc:       sub     sp, sp, #0x90
         :
         : 45               /* Save function arguments */
         : 46               stp     x0, x1, [sp, #FREGS_X0]
    0.00 :   ffff80000802e0d0:       stp     x0, x1, [sp]
         : 45               stp     x2, x3, [sp, #FREGS_X2]
    0.00 :   ffff80000802e0d4:       stp     x2, x3, [sp, #16]
         : 46               stp     x4, x5, [sp, #FREGS_X4]
   16.67 :   ffff80000802e0d8:       stp     x4, x5, [sp, #32] // entry-ftrace.S:46
         : 47               stp     x6, x7, [sp, #FREGS_X6]
    8.33 :   ffff80000802e0dc:       stp     x6, x7, [sp, #48] // entry-ftrace.S:47
         : 48               str     x8,     [sp, #FREGS_X8]
    0.00 :   ffff80000802e0e0:       str     x8, [sp, #64]
         :
         : 52               /* Save the callsite's FP, LR, SP */
         : 53               str     x29, [sp, #FREGS_FP]
    8.33 :   ffff80000802e0e4:       str     x29, [sp, #80] // entry-ftrace.S:51
         : 52               str     x9,  [sp, #FREGS_LR]
    8.33 :   ffff80000802e0e8:       str     x9, [sp, #88] // entry-ftrace.S:52
         : 53               str     x10, [sp, #FREGS_SP]
    0.00 :   ffff80000802e0ec:       str     x10, [sp, #96]
         :
         : 57               /* Save the PC after the ftrace callsite */
         : 58               str     x30, [sp, #FREGS_PC]
   16.67 :   ffff80000802e0f0:       str     x30, [sp, #104] // entry-ftrace.S:56
         :
         : 60               /* Create a frame record for the callsite above the ftrace regs */
         : 61               stp     x29, x9, [sp, #FREGS_SIZE + 16]
   16.67 :   ffff80000802e0f4:       stp     x29, x9, [sp, #128] // entry-ftrace.S:59
         : 60               add     x29, sp, #FREGS_SIZE + 16
    0.00 :   ffff80000802e0f8:       add     x29, sp, #0x80
         :
         : 64               /* Create our frame record above the ftrace regs */
         : 65               stp     x29, x30, [sp, #FREGS_SIZE]
   16.67 :   ffff80000802e0fc:       stp     x29, x30, [sp, #112] // entry-ftrace.S:63
         : 64               add     x29, sp, #FREGS_SIZE
    0.00 :   ffff80000802e100:       add     x29, sp, #0x70
         :
         : 67               sub     x0, x30, #AARCH64_INSN_SIZE     // ip (callsite's BL insn)
    0.00 :   ffff80000802e104:       sub     x0, x30, #0x4
         : 67               mov     x1, x9                          // parent_ip (callsite's LR)
    0.00 :   ffff80000802e108:       mov     x1, x9
         : 68               ldr_l   x2, function_trace_op           // op
    0.00 :   ffff80000802e10c:       adrp    x2, ffff800009638000 <folio_wait_table+0x14c0>
    0.00 :   ffff80000802e110:       ldr     x2, [x2, #3320]
         : 69               mov     x3, sp                          // regs
    0.00 :   ffff80000802e114:       mov     x3, sp
         :
         : 72               ffff80000802e118 <ftrace_call>:
         :
         : 73               SYM_INNER_LABEL(ftrace_call, SYM_L_GLOBAL)
         : 74               bl      ftrace_stub
    0.00 :   ffff80000802e118:       bl      ffff80000802e144 <ftrace_stub>
         : 80               * At the callsite x0-x8 and x19-x30 were live. Any C code will have preserved
         : 81               * x19-x29 per the AAPCS, and we created frame records upon entry, so we need
         : 82               * to restore x0-x8, x29, and x30.
         : 83               */
         : 84               /* Restore function arguments */
         : 85               ldp     x0, x1, [sp, #FREGS_X0]
    8.33 :   ffff80000802e11c:       ldp     x0, x1, [sp] // entry-ftrace.S:80
         : 81               ldp     x2, x3, [sp, #FREGS_X2]
    0.00 :   ffff80000802e120:       ldp     x2, x3, [sp, #16]
         : 82               ldp     x4, x5, [sp, #FREGS_X4]
    0.00 :   ffff80000802e124:       ldp     x4, x5, [sp, #32]
         : 83               ldp     x6, x7, [sp, #FREGS_X6]
    0.00 :   ffff80000802e128:       ldp     x6, x7, [sp, #48]
         : 84               ldr     x8,     [sp, #FREGS_X8]
    0.00 :   ffff80000802e12c:       ldr     x8, [sp, #64]
         :
         : 88               /* Restore the callsite's FP, LR, PC */
         : 89               ldr     x29, [sp, #FREGS_FP]
    0.00 :   ffff80000802e130:       ldr     x29, [sp, #80]
         : 88               ldr     x30, [sp, #FREGS_LR]
    0.00 :   ffff80000802e134:       ldr     x30, [sp, #88]
         : 89               ldr     x9,  [sp, #FREGS_PC]
    0.00 :   ffff80000802e138:       ldr     x9, [sp, #104]
         :
         : 93               /* Restore the callsite's SP */
         : 94               add     sp, sp, #FREGS_SIZE + 32
    0.00 :   ffff80000802e13c:       add     sp, sp, #0x90
         :
         : 95               ret     x9
    0.00 :   ffff80000802e140:       ret     x9

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   81.82 signal.h:696
    9.09 sys.c:1153
    9.09 sys.c:1143
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (11 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                           7f3fc: 1
                           7f454: 4
                           7f458: 5
                           7f460: 1
                   h->nr_samples: 11
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000806f3f0 <__arm64_sys_getpgid>:
         : 6                __arm64_sys_getpgid():
         : 1054             * only important on a multi-user system anyway, to make sure one user
         : 1055             * can't send a signal to a process owned by another.  -TYT, 12/12/91
         : 1056             *
         : 1057             * !PF_FORKNOEXEC check to conform completely to POSIX.
         : 1058             */
         : 1059             SYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)
    0.00 :   ffff80000806f3f0:       bti     c
    0.00 :   ffff80000806f3f4:       nop
    0.00 :   ffff80000806f3f8:       nop
         : 1153             out:
         : 1154             rcu_read_unlock();
         : 1155             return retval;
         : 1156             }
         :
         : 1158             SYSCALL_DEFINE1(getpgid, pid_t, pid)
    9.09 :   ffff80000806f3fc:       paciasp // sys.c:1153
    0.00 :   ffff80000806f400:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff80000806f404:       mov     x29, sp
    0.00 :   ffff80000806f408:       str     x19, [sp, #16]
         : 1163             __se_sys_getpgid():
    0.00 :   ffff80000806f40c:       ldr     x19, [x0]
         : 1154             rcu_read_lock():
         : 704              * read-side critical sections may be preempted and they may also block, but
         : 705              * only when acquiring spinlocks that are subject to priority inheritance.
         : 706              */
         : 707              static __always_inline void rcu_read_lock(void)
         : 708              {
         : 709              __rcu_read_lock();
    0.00 :   ffff80000806f410:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 711              do_getpgid():
         : 1132             if (!pid)
    0.00 :   ffff80000806f414:       cbnz    w19, ffff80000806f448 <__arm64_sys_getpgid+0x58>
         : 1134             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80000806f418:       mrs     x0, sp_el0
         : 26               task_pgrp():
         : 696              * the result of task_pgrp/task_session even if task == current,
         : 697              * we can race with another thread doing sys_setsid/sys_setpgid.
         : 698              */
         : 699              static inline struct pid *task_pgrp(struct task_struct *task)
         : 700              {
         : 701              return task->signal->pids[PIDTYPE_PGID];
    0.00 :   ffff80000806f41c:       ldr     x0, [x0, #1712]
    0.00 :   ffff80000806f420:       ldr     x19, [x0, #376]
         : 704              do_getpgid():
         : 1147             retval = pid_vnr(grp);
    0.00 :   ffff80000806f424:       mov     x0, x19
    0.00 :   ffff80000806f428:       bl      ffff80000807c370 <pid_vnr>
         : 1150             __do_sys_getpgid():
         : 1155             {
         : 1156             return do_getpgid(pid);
    0.00 :   ffff80000806f42c:       sxtw    x19, w0
         : 1158             rcu_read_unlock():
         : 738              static inline void rcu_read_unlock(void)
         : 739              {
         : 740              RCU_LOCKDEP_WARN(!rcu_is_watching(),
         : 741              "rcu_read_unlock() used illegally while idle");
         : 742              __release(RCU);
         : 743              __rcu_read_unlock();
    0.00 :   ffff80000806f430:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 745              __arm64_sys_getpgid():
         : 1153             SYSCALL_DEFINE1(getpgid, pid_t, pid)
    0.00 :   ffff80000806f434:       mov     x0, x19
    0.00 :   ffff80000806f438:       ldr     x19, [sp, #16]
    0.00 :   ffff80000806f43c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff80000806f440:       autiasp
    0.00 :   ffff80000806f444:       ret
         : 1159             do_getpgid():
         : 1136             p = find_task_by_vpid(pid);
    0.00 :   ffff80000806f448:       mov     w0, w19
    0.00 :   ffff80000806f44c:       bl      ffff80000807d2c0 <find_task_by_vpid>
         : 1137             if (!p)
    0.00 :   ffff80000806f450:       cbz     x0, ffff80000806f46c <__arm64_sys_getpgid+0x7c>
         : 1139             task_pgrp():
   36.36 :   ffff80000806f454:       ldr     x1, [x0, #1712] // signal.h:696
   45.45 :   ffff80000806f458:       ldr     x19, [x1, #376]
         : 698              do_getpgid():
         : 1140             if (!grp)
    0.00 :   ffff80000806f45c:       cbz     x19, ffff80000806f46c <__arm64_sys_getpgid+0x7c>
         : 1143             retval = security_task_getpgid(p);
    9.09 :   ffff80000806f460:       bl      ffff8000084854f0 <security_task_getpgid> // sys.c:1143
         : 1144             if (retval)
    0.00 :   ffff80000806f464:       cbz     w0, ffff80000806f424 <__arm64_sys_getpgid+0x34>
    0.00 :   ffff80000806f468:       b       ffff80000806f42c <__arm64_sys_getpgid+0x3c>
    0.00 :   ffff80000806f46c:       mov     x19, #0xfffffffffffffffd        // #-3
    0.00 :   ffff80000806f470:       b       ffff80000806f430 <__arm64_sys_getpgid+0x40>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   30.00 fprobe.c:66
   20.00 trace_recursion.h:180
   20.00 fprobe.h:49
   10.00 fprobe.c:44
   10.00 current.h:19
   10.00 trace_recursion.h:144
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (10 samples, percent: local period)
----------------------------------------------------------------------------------------------------------------
                          1b2048: 2
                          1b2054: 1
                          1b208c: 1
                          1b20bc: 1
                          1b2118: 1
                          1b2124: 1
                          1b2134: 2
                          1b213c: 1
                   h->nr_samples: 10
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081a2020 <fprobe_handler>:
         : 6                fprobe_handler():
         : 25               char private[];
         : 26               };
         :
         : 28               static void fprobe_handler(unsigned long ip, unsigned long parent_ip,
         : 29               struct ftrace_ops *ops, struct ftrace_regs *fregs)
         : 30               {
    0.00 :   ffff8000081a2020:       paciasp
    0.00 :   ffff8000081a2024:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000081a2028:       mov     x29, sp
    0.00 :   ffff8000081a202c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff8000081a2030:       mov     x19, x2
    0.00 :   ffff8000081a2034:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000081a2038:       mov     x22, x3
    0.00 :   ffff8000081a203c:       str     x23, [sp, #48]
    0.00 :   ffff8000081a2040:       mov     x23, x0
         : 40               fprobe_disabled():
         : 49               */
         : 50               #define FPROBE_FL_KPROBE_SHARED 2
         :
         : 52               static inline bool fprobe_disabled(struct fprobe *fp)
         : 53               {
         : 54               return (fp) ? fp->flags & FPROBE_FL_DISABLED : false;
    0.00 :   ffff8000081a2044:       cbz     x2, ffff8000081a2050 <fprobe_handler+0x30>
   20.00 :   ffff8000081a2048:       ldr     w0, [x2, #192] // fprobe.h:49
    0.00 :   ffff8000081a204c:       tbnz    w0, #0, ffff8000081a2128 <fprobe_handler+0x108>
         : 58               get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000081a2050:       mrs     x0, sp_el0
         : 26               trace_test_and_set_recursion():
         : 144              * Preemption is promised to be disabled when return bit >= 0.
         : 145              */
         : 146              static __always_inline int trace_test_and_set_recursion(unsigned long ip, unsigned long pip,
         : 147              int start)
         : 148              {
         : 149              unsigned int val = READ_ONCE(current->trace_recursion);
   10.00 :   ffff8000081a2054:       ldr     x9, [x0, #2520] // trace_recursion.h:144
         : 151              trace_get_context_bit():
         : 121              return TRACE_CTX_NORMAL - bit;
    0.00 :   ffff8000081a2058:       mov     w6, #0x3                        // #3
         : 123              preempt_count():
         : 13               #define PREEMPT_NEED_RESCHED    BIT(32)
         : 14               #define PREEMPT_ENABLED (PREEMPT_NEED_RESCHED)
         :
         : 16               static inline int preempt_count(void)
         : 17               {
         : 18               return READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000081a205c:       ldr     w8, [x0, #8]
         : 20               trace_test_and_set_recursion():
         : 148              int bit;
         :
         : 150              bit = trace_get_context_bit() + start;
         : 151              if (unlikely(val & (1 << bit))) {
    0.00 :   ffff8000081a2060:       mov     w4, #0x1                        // #1
         : 153              interrupt_context_level():
         : 94               static __always_inline unsigned char interrupt_context_level(void)
         : 95               {
         : 96               unsigned long pc = preempt_count();
         : 97               unsigned char level = 0;
         :
         : 99               level += !!(pc & (NMI_MASK));
    0.00 :   ffff8000081a2064:       tst     w8, #0xf00000
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK));
         : 97               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff8000081a2068:       and     w7, w8, #0xffff00
         : 94               level += !!(pc & (NMI_MASK));
    0.00 :   ffff8000081a206c:       cset    w5, ne  // ne = any
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff8000081a2070:       and     w7, w7, #0xffff01ff
         : 95               level += !!(pc & (NMI_MASK | HARDIRQ_MASK));
    0.00 :   ffff8000081a2074:       tst     w8, #0xff0000
    0.00 :   ffff8000081a2078:       cinc    w5, w5, ne      // ne = any
         : 96               level += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));
    0.00 :   ffff8000081a207c:       cmp     w7, #0x0
         : 98               trace_get_context_bit():
         : 121              return TRACE_CTX_NORMAL - bit;
    0.00 :   ffff8000081a2080:       cinc    w5, w5, ne      // ne = any
    0.00 :   ffff8000081a2084:       sub     w5, w6, w5
         : 124              trace_test_and_set_recursion():
         : 148              if (unlikely(val & (1 << bit))) {
    0.00 :   ffff8000081a2088:       lsl     w4, w4, w5
         : 150              trace_clear_recursion():
         : 180              */
         : 181              static __always_inline void trace_clear_recursion(int bit)
         : 182              {
         : 183              preempt_enable_notrace();
         : 184              barrier();
         : 185              trace_recursion_clear(bit);
   10.00 :   ffff8000081a208c:       mvn     w20, w4 // trace_recursion.h:180
    0.00 :   ffff8000081a2090:       sxtw    x20, w20
         : 188              trace_test_and_set_recursion():
         : 148              if (unlikely(val & (1 << bit))) {
    0.00 :   ffff8000081a2094:       tst     w4, w9
    0.00 :   ffff8000081a2098:       b.ne    ffff8000081a2194 <fprobe_handler+0x174>  // b.any
         : 165              current->trace_recursion = val;
    0.00 :   ffff8000081a209c:       orr     w4, w4, w9
         : 167              get_current():
    0.00 :   ffff8000081a20a0:       mrs     x5, sp_el0
         : 20               trace_test_and_set_recursion():
    0.00 :   ffff8000081a20a4:       str     x4, [x5, #2520]
         : 166              __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000081a20a8:       ldr     w4, [x5, #8]
         : 48               pc += val;
    0.00 :   ffff8000081a20ac:       add     w4, w4, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff8000081a20b0:       str     w4, [x5, #8]
         : 51               fprobe_handler():
         : 43               if (bit < 0) {
         : 44               fp->nmissed++;
         : 45               return;
         : 46               }
         :
         : 48               if (fp->exit_handler) {
    0.00 :   ffff8000081a20b4:       ldr     x0, [x19, #224]
    0.00 :   ffff8000081a20b8:       cbz     x0, ffff8000081a2140 <fprobe_handler+0x120>
         : 44               rh = rethook_try_get(fp->rethook);
   10.00 :   ffff8000081a20bc:       ldr     x0, [x19, #200] // fprobe.c:44
    0.00 :   ffff8000081a20c0:       bl      ffff8000081a2a54 <rethook_try_get>
    0.00 :   ffff8000081a20c4:       mov     x21, x0
         : 45               if (!rh) {
    0.00 :   ffff8000081a20c8:       cbz     x0, ffff8000081a21a4 <fprobe_handler+0x184>
         : 50               fp->nmissed++;
         : 51               goto out;
         : 52               }
         : 53               fpr = container_of(rh, struct fprobe_rethook_node, node);
         : 54               fpr->entry_ip = ip;
    0.00 :   ffff8000081a20cc:       str     x23, [x0, #48]
         : 54               private = fpr->private;
         : 55               }
         :
         : 57               if (fp->entry_handler)
    0.00 :   ffff8000081a20d0:       ldr     x4, [x19, #216]
    0.00 :   ffff8000081a20d4:       cbz     x4, ffff8000081a2180 <fprobe_handler+0x160>
         : 55               should_rethook = fp->entry_handler(fp, ip, fregs, fpr->private);
    0.00 :   ffff8000081a20d8:       mov     x1, x23
    0.00 :   ffff8000081a20dc:       mov     x0, x19
    0.00 :   ffff8000081a20e0:       add     x3, x21, #0x38
    0.00 :   ffff8000081a20e4:       mov     x2, x22
    0.00 :   ffff8000081a20e8:       blr     x4
         :
         : 59               if (rh) {
         : 60               if (should_rethook)
    0.00 :   ffff8000081a20ec:       tst     w0, #0xff
    0.00 :   ffff8000081a20f0:       b.ne    ffff8000081a2180 <fprobe_handler+0x160>  // b.any
         : 61               rethook_hook(rh, fregs, true);
         : 62               else
         : 63               rethook_recycle(rh);
    0.00 :   ffff8000081a20f4:       mov     x0, x21
    0.00 :   ffff8000081a20f8:       bl      ffff8000081a2bf0 <rethook_recycle>
         : 66               get_current():
    0.00 :   ffff8000081a20fc:       mrs     x1, sp_el0
         : 20               __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081a2100:       ldr     x0, [x1, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000081a2104:       sub     x0, x0, #0x1
    0.00 :   ffff8000081a2108:       str     w0, [x1, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081a210c:       cbnz    x0, ffff8000081a2170 <fprobe_handler+0x150>
         : 81               trace_clear_recursion():
         : 178              preempt_enable_notrace();
    0.00 :   ffff8000081a2110:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000081a2114:       nop
         : 181              get_current():
   10.00 :   ffff8000081a2118:       mrs     x1, sp_el0 // current.h:19
         : 20               trace_clear_recursion():
         : 180              trace_recursion_clear(bit);
    0.00 :   ffff8000081a211c:       ldr     x0, [x1, #2520]
    0.00 :   ffff8000081a2120:       and     x0, x0, x20
   10.00 :   ffff8000081a2124:       str     x0, [x1, #2520] // trace_recursion.h:180
         : 184              fprobe_handler():
         : 66               }
         :
         : 68               out:
         : 69               ftrace_test_recursion_unlock(bit);
         : 70               }
    0.00 :   ffff8000081a2128:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000081a212c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000081a2130:       ldr     x23, [sp, #48]
   20.00 :   ffff8000081a2134:       ldp     x29, x30, [sp], #64 // fprobe.c:66
    0.00 :   ffff8000081a2138:       autiasp
   10.00 :   ffff8000081a213c:       ret
         : 54               if (fp->entry_handler)
    0.00 :   ffff8000081a2140:       ldr     x4, [x19, #216]
    0.00 :   ffff8000081a2144:       cbz     x4, ffff8000081a215c <fprobe_handler+0x13c>
         : 55               should_rethook = fp->entry_handler(fp, ip, fregs, fpr->private);
    0.00 :   ffff8000081a2148:       mov     x2, x22
    0.00 :   ffff8000081a214c:       mov     x1, x23
    0.00 :   ffff8000081a2150:       mov     x0, x19
    0.00 :   ffff8000081a2154:       mov     x3, #0x38                       // #56
    0.00 :   ffff8000081a2158:       blr     x4
         : 61               get_current():
    0.00 :   ffff8000081a215c:       mrs     x1, sp_el0
         : 20               __preempt_count_dec_and_test():
         : 62               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081a2160:       ldr     x0, [x1, #8]
         : 65               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000081a2164:       sub     x0, x0, #0x1
    0.00 :   ffff8000081a2168:       str     w0, [x1, #8]
         : 74               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000081a216c:       cbz     x0, ffff8000081a2110 <fprobe_handler+0xf0>
    0.00 :   ffff8000081a2170:       ldr     x0, [x1, #8]
    0.00 :   ffff8000081a2174:       cbnz    x0, ffff8000081a2118 <fprobe_handler+0xf8>
         : 78               trace_clear_recursion():
         : 178              preempt_enable_notrace();
    0.00 :   ffff8000081a2178:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000081a217c:       b       ffff8000081a2118 <fprobe_handler+0xf8>
         : 181              fprobe_handler():
         : 59               rethook_hook(rh, fregs, true);
    0.00 :   ffff8000081a2180:       mov     x1, x22
    0.00 :   ffff8000081a2184:       mov     x0, x21
    0.00 :   ffff8000081a2188:       mov     w2, #0x1                        // #1
    0.00 :   ffff8000081a218c:       bl      ffff8000081a27d0 <rethook_hook>
    0.00 :   ffff8000081a2190:       b       ffff8000081a215c <fprobe_handler+0x13c>
         : 65               trace_test_and_set_recursion():
         : 158              if (val & (1 << bit)) {
    0.00 :   ffff8000081a2194:       tbnz    w9, #4, ffff8000081a21b4 <fprobe_handler+0x194>
    0.00 :   ffff8000081a2198:       mov     x20, #0xffffffffffffffef        // #-17
    0.00 :   ffff8000081a219c:       mov     w4, #0x10                       // #16
    0.00 :   ffff8000081a21a0:       b       ffff8000081a209c <fprobe_handler+0x7c>
         : 163              fprobe_handler():
         : 46               fp->nmissed++;
    0.00 :   ffff8000081a21a4:       ldr     x0, [x19, #184]
    0.00 :   ffff8000081a21a8:       add     x0, x0, #0x1
    0.00 :   ffff8000081a21ac:       str     x0, [x19, #184]
         : 47               goto out;
    0.00 :   ffff8000081a21b0:       b       ffff8000081a215c <fprobe_handler+0x13c>
         : 39               fp->nmissed++;
    0.00 :   ffff8000081a21b4:       ldr     x0, [x19, #184]
    0.00 :   ffff8000081a21b8:       add     x0, x0, #0x1
    0.00 :   ffff8000081a21bc:       str     x0, [x19, #184]
         : 40               return;
    0.00 :   ffff8000081a21c0:       b       ffff8000081a2128 <fprobe_handler+0x108>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   75.00 pid.h:155
   25.00 pid.c:418
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (8 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           8d2c0: 1
                           8d2c8: 1
                           8d2e4: 2
                           8d2ec: 4
                   h->nr_samples: 8
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000807d2c0 <find_task_by_vpid>:
         : 6                find_task_by_vpid():
         : 418              struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)
         : 419              {
         : 420              RCU_LOCKDEP_WARN(!rcu_read_lock_held(),
         : 421              "find_task_by_pid_ns() needs rcu_read_lock() protection");
         : 422              return pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);
         : 423              }
   12.50 :   ffff80000807d2c0:       bti     c // pid.c:418
    0.00 :   ffff80000807d2c4:       nop
   12.50 :   ffff80000807d2c8:       nop
         :
         : 422              struct task_struct *find_task_by_vpid(pid_t vnr)
         : 423              {
    0.00 :   ffff80000807d2cc:       paciasp
    0.00 :   ffff80000807d2d0:       stp     x29, x30, [sp, #-16]!
         : 426              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80000807d2d4:       mrs     x1, sp_el0
         : 26               find_task_by_vpid():
    0.00 :   ffff80000807d2d8:       mov     x29, sp
         : 422              task_pid():
         : 1534             */
         : 1535             };
         :
         : 1537             static inline struct pid *task_pid(struct task_struct *task)
         : 1538             {
         : 1539             return task->thread_pid;
    0.00 :   ffff80000807d2dc:       ldr     x2, [x1, #1264]
         : 1541             ns_of_pid():
         : 154              *       the resulting NULL pid-ns.
         : 155              */
         : 156              static inline struct pid_namespace *ns_of_pid(struct pid *pid)
         : 157              {
         : 158              struct pid_namespace *ns = NULL;
         : 159              if (pid)
    0.00 :   ffff80000807d2e0:       cbz     x2, ffff80000807d2f0 <find_task_by_vpid+0x30>
         : 155              ns = pid->numbers[pid->level].ns;
   25.00 :   ffff80000807d2e4:       ldr     w1, [x2, #4] // pid.h:155
    0.00 :   ffff80000807d2e8:       add     x1, x2, x1, lsl #4
   50.00 :   ffff80000807d2ec:       ldr     x2, [x1, #104]
         : 159              find_pid_ns():
         : 311              return idr_find(&ns->idr, nr);
    0.00 :   ffff80000807d2f0:       sxtw    x1, w0
    0.00 :   ffff80000807d2f4:       mov     x0, x2
    0.00 :   ffff80000807d2f8:       bl      ffff80000854caa0 <idr_find>
         : 315              pid_task():
         : 399              if (pid) {
    0.00 :   ffff80000807d2fc:       cbz     x0, ffff80000807d310 <find_task_by_vpid+0x50>
         : 401              first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
    0.00 :   ffff80000807d300:       ldr     x0, [x0, #16]
         : 404              result = hlist_entry(first, struct task_struct, pid_links[(type)]);
    0.00 :   ffff80000807d304:       cmp     x0, #0x0
    0.00 :   ffff80000807d308:       sub     x1, x0, #0x4f8
    0.00 :   ffff80000807d30c:       csel    x0, x1, x0, ne  // ne = any
         : 408              find_task_by_vpid():
         : 423              return find_task_by_pid_ns(vnr, task_active_pid_ns(current));
         : 424              }
    0.00 :   ffff80000807d310:       ldp     x29, x30, [sp], #16
    0.00 :   ffff80000807d314:       autiasp
    0.00 :   ffff80000807d318:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   42.86 freelist.h:81
   14.29 rethook.c:171
   14.29 freelist.h:88
   14.29 atomic-instrumented.h:28
   14.28 freelist.h:98
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (7 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          1b2a80: 1
                          1b2a84: 2
                          1b2a9c: 1
                          1b2ac8: 1
                          1b2ae8: 1
                          1b2b24: 1
                   h->nr_samples: 7
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081a2a54 <rethook_try_get>:
         : 6                rethook_try_get():
         : 147              *
         : 148              * Get an unused rethook node from @rh. If the node pool is empty, this
         : 149              * will return NULL. Caller must disable preemption.
         : 150              */
         : 151              struct rethook_node *rethook_try_get(struct rethook *rh)
         : 152              {
    0.00 :   ffff8000081a2a54:       bti     c
         : 148              rethook_handler_t handler = READ_ONCE(rh->handler);
    0.00 :   ffff8000081a2a58:       ldr     x1, [x0, #8]
         : 154              struct freelist_node *fn;
         :
         : 156              lockdep_assert_preemption_disabled();
         :
         : 158              /* Check whether @rh is going to be freed. */
         : 159              if (unlikely(!handler))
    0.00 :   ffff8000081a2a5c:       cbz     x1, ffff8000081a2bd0 <rethook_try_get+0x17c>
         : 147              {
    0.00 :   ffff8000081a2a60:       paciasp
    0.00 :   ffff8000081a2a64:       stp     x29, x30, [sp, #-32]!
    0.00 :   ffff8000081a2a68:       mov     x29, sp
    0.00 :   ffff8000081a2a6c:       str     x19, [sp, #16]
    0.00 :   ffff8000081a2a70:       mov     x19, x0
         : 163              * This expects the caller will set up a rethook on a function entry.
         : 164              * When the function returns, the rethook will eventually be reclaimed
         : 165              * or released in the rethook_recycle() with call_rcu().
         : 166              * This means the caller must be run in the RCU-availabe context.
         : 167              */
         : 168              if (unlikely(!rcu_is_watching()))
    0.00 :   ffff8000081a2a74:       bl      ffff8000080e5770 <rcu_is_watching>
    0.00 :   ffff8000081a2a78:       tst     w0, #0xff
    0.00 :   ffff8000081a2a7c:       b.eq    ffff8000081a2ab0 <rethook_try_get+0x5c>  // b.none
         : 172              freelist_try_get():
         : 81               }
         : 82               }
         :
         : 84               static inline struct freelist_node *freelist_try_get(struct freelist_head *list)
         : 85               {
         : 86               struct freelist_node *prev, *next, *head = smp_load_acquire(&list->head);
   14.29 :   ffff8000081a2a80:       add     x4, x19, #0x10 // freelist.h:81
   28.57 :   ffff8000081a2a84:       ldar    x3, [x4]
         : 84               unsigned int refs;
         :
         : 86               while (head) {
    0.00 :   ffff8000081a2a88:       cbz     x3, ffff8000081a2ab0 <rethook_try_get+0x5c>
         : 122              /*
         : 123              * OK, the head must have changed on us, but we still need to decrement
         : 124              * the refcount we increased.
         : 125              */
         : 126              refs = atomic_fetch_add(-1, &prev->refs);
         : 127              if (refs == REFS_ON_FREELIST + 1)
    0.00 :   ffff8000081a2a8c:       mov     w7, #0x80000001                 // #-2147483647
         : 129              arch_atomic_set_release():
         : 243              #ifndef arch_atomic_set_release
         : 244              static __always_inline void
         : 245              arch_atomic_set_release(atomic_t *v, int i)
         : 246              {
         : 247              if (__native_word(atomic_t)) {
         : 248              smp_store_release(&(v)->counter, i);
    0.00 :   ffff8000081a2a90:       mov     w8, #0x1                        // #1
         : 250              __ll_sc_atomic_fetch_add_release():
         : 111              ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         : 112              ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         : 113              ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         : 114              ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         : 116              ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000081a2a94:       mov     w9, #0x7fffffff                 // #2147483647
         : 118              __ll_sc_atomic_fetch_add():
    0.00 :   ffff8000081a2a98:       mov     w6, #0xffffffff                 // #-1
         : 112              atomic_read():
         :
         : 29               static __always_inline int
         : 30               atomic_read(const atomic_t *v)
         : 31               {
         : 32               instrument_atomic_read(v, sizeof(*v));
         : 33               return arch_atomic_read(v);
   14.29 :   ffff8000081a2a9c:       ldr     w1, [x3] // atomic-instrumented.h:28
         : 35               freelist_try_get():
         : 87               if ((refs & REFS_MASK) == 0 ||
    0.00 :   ffff8000081a2aa0:       tst     x1, #0x7fffffff
    0.00 :   ffff8000081a2aa4:       b.ne    ffff8000081a2ac8 <rethook_try_get+0x74>  // b.any
         : 89               head = smp_load_acquire(&list->head);
    0.00 :   ffff8000081a2aa8:       ldar    x3, [x4]
         : 84               while (head) {
    0.00 :   ffff8000081a2aac:       cbnz    x3, ffff8000081a2a9c <rethook_try_get+0x48>
         : 86               rethook_try_get():
         : 155              return NULL;
    0.00 :   ffff8000081a2ab0:       mov     x3, #0x0                        // #0
         : 171              fn = freelist_try_get(&rh->pool);
         : 172              if (!fn)
         : 173              return NULL;
         :
         : 175              return container_of(fn, struct rethook_node, freelist);
         : 176              }
    0.00 :   ffff8000081a2ab4:       mov     x0, x3
    0.00 :   ffff8000081a2ab8:       ldr     x19, [sp, #16]
    0.00 :   ffff8000081a2abc:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000081a2ac0:       autiasp
    0.00 :   ffff8000081a2ac4:       ret
         : 182              freelist_try_get():
         : 88               !atomic_try_cmpxchg_acquire(&head->refs, &refs, refs+1)) {
   14.29 :   ffff8000081a2ac8:       add     w2, w1, #0x1 // freelist.h:88
         : 90               arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff8000081a2acc:       b       ffff8000081a2b38 <rethook_try_get+0xe4>
         : 45               __lse__cmpxchg_case_acq_32():
         : 266              __CMPXCHG_CASE(w, h,     , 16,   )
         : 267              __CMPXCHG_CASE(w,  ,     , 32,   )
         : 268              __CMPXCHG_CASE(x,  ,     , 64,   )
         : 269              __CMPXCHG_CASE(w, b, acq_,  8,  a, "memory")
         : 270              __CMPXCHG_CASE(w, h, acq_, 16,  a, "memory")
         : 271              __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
    0.00 :   ffff8000081a2ad0:       mov     x0, x3
    0.00 :   ffff8000081a2ad4:       mov     w5, w1
    0.00 :   ffff8000081a2ad8:       casa    w5, w2, [x3]
    0.00 :   ffff8000081a2adc:       mov     w0, w5
         : 276              freelist_try_get():
         : 87               if ((refs & REFS_MASK) == 0 ||
    0.00 :   ffff8000081a2ae0:       cmp     w1, w0
    0.00 :   ffff8000081a2ae4:       b.ne    ffff8000081a2aa8 <rethook_try_get+0x54>  // b.any
         : 98               next = READ_ONCE(head->next);
   14.28 :   ffff8000081a2ae8:       ldr     x2, [x3, #8] // freelist.h:98
         : 100              arch_static_branch_jump():
    0.00 :   ffff8000081a2aec:       b       ffff8000081a2b48 <rethook_try_get+0xf4>
         : 39               __lse__cmpxchg_case_acq_64():
         : 267              __CMPXCHG_CASE(x,  , acq_, 64,  a, "memory")
    0.00 :   ffff8000081a2af0:       mov     x0, x4
    0.00 :   ffff8000081a2af4:       mov     x1, x3
    0.00 :   ffff8000081a2af8:       mov     x5, x1
    0.00 :   ffff8000081a2afc:       casa    x5, x2, [x4]
    0.00 :   ffff8000081a2b00:       mov     x0, x5
    0.00 :   ffff8000081a2b04:       mov     x5, x0
         : 274              freelist_try_get():
         : 99               if (try_cmpxchg_acquire(&list->head, &head, next)) {
    0.00 :   ffff8000081a2b08:       cmp     x5, x3
    0.00 :   ffff8000081a2b0c:       b.ne    ffff8000081a2b68 <rethook_try_get+0x114>  // b.any
         : 102              atomic_read():
    0.00 :   ffff8000081a2b10:       ldr     w0, [x3]
         : 29               freelist_try_get():
         : 106              WARN_ON_ONCE(atomic_read(&head->refs) & REFS_ON_FREELIST);
    0.00 :   ffff8000081a2b14:       tbnz    w0, #31, ffff8000081a2b5c <rethook_try_get+0x108>
         : 108              arch_static_branch_jump():
    0.00 :   ffff8000081a2b18:       b       ffff8000081a2b50 <rethook_try_get+0xfc>
         : 39               __lse_atomic_fetch_add():
         : 60               ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff8000081a2b1c:       mov     w0, #0xfffffffe                 // #-2
    0.00 :   ffff8000081a2b20:       ldaddal w0, w0, [x3]
         : 63               rethook_try_get():
   14.29 :   ffff8000081a2b24:       mov     x0, x3 // rethook.c:171
    0.00 :   ffff8000081a2b28:       ldr     x19, [sp, #16]
    0.00 :   ffff8000081a2b2c:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000081a2b30:       autiasp
    0.00 :   ffff8000081a2b34:       ret
         : 176              __ll_sc__cmpxchg_case_acq_32():
         : 305              __CMPXCHG_CASE(w, h,     , 16,        ,  ,  ,         , K)
         : 306              __CMPXCHG_CASE(w,  ,     , 32,        ,  ,  ,         , K)
         : 307              __CMPXCHG_CASE( ,  ,     , 64,        ,  ,  ,         , L)
         : 308              __CMPXCHG_CASE(w, b, acq_,  8,        , a,  , "memory", K)
         : 309              __CMPXCHG_CASE(w, h, acq_, 16,        , a,  , "memory", K)
         : 310              __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
    0.00 :   ffff8000081a2b38:       mov     w0, w1
    0.00 :   ffff8000081a2b3c:       b       ffff8000081a30c4 <rethook_trampoline_handler+0x164>
    0.00 :   ffff8000081a2b40:       mov     w0, w5
    0.00 :   ffff8000081a2b44:       b       ffff8000081a2ae0 <rethook_try_get+0x8c>
         : 315              __ll_sc__cmpxchg_case_acq_64():
         : 306              __CMPXCHG_CASE( ,  , acq_, 64,        , a,  , "memory", L)
    0.00 :   ffff8000081a2b48:       b       ffff8000081a30e0 <rethook_trampoline_handler+0x180>
    0.00 :   ffff8000081a2b4c:       b       ffff8000081a2b08 <rethook_try_get+0xb4>
         : 309              __ll_sc_atomic_fetch_add():
         : 111              ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000081a2b50:       mov     w0, #0xfffffffe                 // #-2
    0.00 :   ffff8000081a2b54:       b       ffff8000081a30fc <rethook_trampoline_handler+0x19c>
    0.00 :   ffff8000081a2b58:       b       ffff8000081a2b24 <rethook_try_get+0xd0>
         : 115              freelist_try_get():
    0.00 :   ffff8000081a2b5c:       brk     #0x800
         : 107              arch_static_branch_jump():
    0.00 :   ffff8000081a2b60:       b       ffff8000081a2b50 <rethook_try_get+0xfc>
    0.00 :   ffff8000081a2b64:       b       ffff8000081a2b1c <rethook_try_get+0xc8>
    0.00 :   ffff8000081a2b68:       b       ffff8000081a2b84 <rethook_try_get+0x130>
         : 41               __lse_atomic_fetch_add():
    0.00 :   ffff8000081a2b6c:       ldaddal w6, w0, [x3]
         : 61               freelist_try_get():
         : 122              if (refs == REFS_ON_FREELIST + 1)
    0.00 :   ffff8000081a2b70:       cmp     w0, w7
    0.00 :   ffff8000081a2b74:       b.eq    ffff8000081a2b8c <rethook_try_get+0x138>  // b.none
    0.00 :   ffff8000081a2b78:       mov     x3, x5
         : 84               while (head) {
    0.00 :   ffff8000081a2b7c:       cbnz    x3, ffff8000081a2a9c <rethook_try_get+0x48>
    0.00 :   ffff8000081a2b80:       b       ffff8000081a2ab0 <rethook_try_get+0x5c>
         : 87               __ll_sc_atomic_fetch_add():
    0.00 :   ffff8000081a2b84:       b       ffff8000081a3118 <rethook_trampoline_handler+0x1b8>
    0.00 :   ffff8000081a2b88:       b       ffff8000081a2b70 <rethook_try_get+0x11c>
         : 113              __freelist_add():
         : 46               struct freelist_node *head = READ_ONCE(list->head);
    0.00 :   ffff8000081a2b8c:       ldr     x1, [x19, #16]
         : 49               WRITE_ONCE(node->next, head);
    0.00 :   ffff8000081a2b90:       str     x1, [x3, #8]
         : 51               arch_atomic_set_release():
    0.00 :   ffff8000081a2b94:       stlr    w8, [x3]
         : 244              arch_static_branch_jump():
    0.00 :   ffff8000081a2b98:       b       ffff8000081a2bdc <rethook_try_get+0x188>
         : 39               __lse__cmpxchg_case_rel_64():
         : 271              __CMPXCHG_CASE(w, b, rel_,  8,  l, "memory")
         : 272              __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         : 273              __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         : 274              __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
    0.00 :   ffff8000081a2b9c:       mov     x0, x4
    0.00 :   ffff8000081a2ba0:       mov     x2, x3
    0.00 :   ffff8000081a2ba4:       mov     x10, x1
    0.00 :   ffff8000081a2ba8:       casl    x10, x2, [x4]
    0.00 :   ffff8000081a2bac:       mov     x0, x10
         : 280              __freelist_add():
         : 52               if (!try_cmpxchg_release(&list->head, &head, node)) {
    0.00 :   ffff8000081a2bb0:       cmp     x1, x0
    0.00 :   ffff8000081a2bb4:       b.eq    ffff8000081a2b78 <rethook_try_get+0x124>  // b.none
         : 55               arch_static_branch_jump():
    0.00 :   ffff8000081a2bb8:       b       ffff8000081a2be4 <rethook_try_get+0x190>
         : 39               __lse_atomic_fetch_add_release():
         : 60               ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff8000081a2bbc:       ldaddl  w9, w1, [x3]
         : 62               __freelist_add():
         : 57               if (atomic_fetch_add_release(REFS_ON_FREELIST - 1, &node->refs) == 1)
    0.00 :   ffff8000081a2bc0:       cmp     w1, #0x1
    0.00 :   ffff8000081a2bc4:       b.ne    ffff8000081a2b78 <rethook_try_get+0x124>  // b.any
    0.00 :   ffff8000081a2bc8:       mov     x1, x0
    0.00 :   ffff8000081a2bcc:       b       ffff8000081a2b90 <rethook_try_get+0x13c>
         : 62               rethook_try_get():
         : 155              return NULL;
    0.00 :   ffff8000081a2bd0:       mov     x3, #0x0                        // #0
         : 171              }
    0.00 :   ffff8000081a2bd4:       mov     x0, x3
    0.00 :   ffff8000081a2bd8:       ret
         : 174              __ll_sc__cmpxchg_case_rel_64():
         : 310              __CMPXCHG_CASE(w, b, rel_,  8,        ,  , l, "memory", K)
         : 311              __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         : 312              __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         : 313              __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
    0.00 :   ffff8000081a2bdc:       b       ffff8000081a3134 <rethook_trampoline_handler+0x1d4>
    0.00 :   ffff8000081a2be0:       b       ffff8000081a2bb0 <rethook_try_get+0x15c>
         : 316              __ll_sc_atomic_fetch_add_release():
         : 111              ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000081a2be4:       b       ffff8000081a3150 <rethook_trampoline_handler+0x1f0>
    0.00 :   ffff8000081a2be8:       b       ffff8000081a2bc0 <rethook_try_get+0x16c>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   66.67 core.c:2231
   16.67 core.c:2228
   16.67 current.h:19
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (6 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           a1dcc: 1
                           a1e10: 1
                           a1e34: 2
                           a1e3c: 2
                   h->nr_samples: 6
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008091dc0 <migrate_disable>:
         : 6                migrate_disable():
         : 8293             */
         : 8294             SYSCALL_DEFINE0(sched_yield)
         : 8295             {
         : 8296             do_sched_yield();
         : 8297             return 0;
         : 8298             }
    0.00 :   ffff800008091dc0:       bti     c
    0.00 :   ffff800008091dc4:       nop
    0.00 :   ffff800008091dc8:       nop
         : 8302             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
   16.67 :   ffff800008091dcc:       mrs     x1, sp_el0 // current.h:19
         : 26               migrate_disable():
         : 2222             if (p->migration_disabled) {
    0.00 :   ffff800008091dd0:       ldrh    w0, [x1, #768]
    0.00 :   ffff800008091dd4:       cbz     w0, ffff800008091de4 <migrate_disable+0x24>
         : 2223             p->migration_disabled++;
    0.00 :   ffff800008091dd8:       add     w0, w0, #0x1
    0.00 :   ffff800008091ddc:       strh    w0, [x1, #768]
         : 2224             return;
    0.00 :   ffff800008091de0:       ret
         : 2219             {
    0.00 :   ffff800008091de4:       paciasp
    0.00 :   ffff800008091de8:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff800008091dec:       mov     x29, sp
         : 2223             __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff800008091df0:       ldr     w0, [x1, #8]
         : 48               pc += val;
    0.00 :   ffff800008091df4:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff800008091df8:       str     w0, [x1, #8]
         : 51               migrate_disable():
         : 2228             this_rq()->nr_pinned++;
    0.00 :   ffff800008091dfc:       adrp    x0, ffff800009395000 <cpu_worker_pools+0x240>
    0.00 :   ffff800008091e00:       add     x0, x0, #0x3c0
         : 2231             __kern_my_cpu_offset():
         :
         : 41               /*
         : 42               * We want to allow caching the value, so avoid using volatile and
         : 43               * instead use a fake stack read to hazard against barrier().
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008091e04:       mrs     x2, tpidr_el1
         : 47               migrate_disable():
    0.00 :   ffff800008091e08:       add     x0, x0, x2
         : 2229             p->migration_disabled = 1;
    0.00 :   ffff800008091e0c:       mov     w3, #0x1                        // #1
         : 2228             this_rq()->nr_pinned++;
   16.67 :   ffff800008091e10:       ldr     w2, [x0, #3088] // core.c:2228
    0.00 :   ffff800008091e14:       add     w2, w2, #0x1
    0.00 :   ffff800008091e18:       str     w2, [x0, #3088]
         : 2229             p->migration_disabled = 1;
    0.00 :   ffff800008091e1c:       strh    w3, [x1, #768]
         : 2231             __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff800008091e20:       ldr     x0, [x1, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800008091e24:       sub     x0, x0, #0x1
    0.00 :   ffff800008091e28:       str     w0, [x1, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800008091e2c:       cbnz    x0, ffff800008091e40 <migrate_disable+0x80>
         : 81               migrate_disable():
         : 2230             preempt_enable();
    0.00 :   ffff800008091e30:       bl      ffff800008ae8860 <preempt_schedule>
         : 2231             }
   33.33 :   ffff800008091e34:       ldp     x29, x30, [sp], #16 // core.c:2231
    0.00 :   ffff800008091e38:       autiasp
   33.33 :   ffff800008091e3c:       ret
         : 2235             __preempt_count_dec_and_test():
    0.00 :   ffff800008091e40:       ldr     x0, [x1, #8]
    0.00 :   ffff800008091e44:       cbnz    x0, ffff800008091e34 <migrate_disable+0x74>
         : 76               migrate_disable():
         : 2230             preempt_enable();
    0.00 :   ffff800008091e48:       bl      ffff800008ae8860 <preempt_schedule>
    0.00 :   ffff800008091e4c:       b       ffff800008091e34 <migrate_disable+0x74>

Sorted summary for file /home/huawei/bpf-selftest/bpf/bench
----------------------------------------------

   66.66 bench[cfc4]
   33.34 bench[cfc0]
 Percent |	Source code & Disassembly of bench for cycles (6 samples, percent: local period)
------------------------------------------------------------------------------------------------
                            cfc0: 2
                            cfc4: 4
                   h->nr_samples: 6
         :
         :
         :
         : 3    Disassembly of section .plt:
         :
         : 5    000000000000cfc0 <syscall@plt>:
   33.34 :   cfc0:   adrp    x16, 9d000 <bench_parsers+0xc8> // bench[cfc0]
   66.66 :   cfc4:   ldr     x17, [x16, #3936] // bench[cfc4]
    0.00 :   cfc8:   add     x16, x16, #0xf60
    0.00 :   cfcc:   br      x17

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   50.00 core.c:2262
   16.67 preempt.h:65
   16.67 core.c:2235
   16.66 core.c:2251
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (6 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           a5e2c: 1
                           a5e4c: 1
                           a5eac: 1
                           a5ec0: 1
                           a5ed4: 2
                   h->nr_samples: 6
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008095e00 <migrate_enable>:
         : 6                migrate_enable():
         : 3040             cpuset_cpus_allowed(p, new_mask);
         : 3041             override_mask = new_mask;
         :
         : 3043             out_set_mask:
         : 3044             if (printk_ratelimit()) {
         : 3045             printk_deferred("Overriding affinity for process %d (%s) to CPUs %*pbl\n",
    0.00 :   ffff800008095e00:       bti     c
    0.00 :   ffff800008095e04:       nop
    0.00 :   ffff800008095e08:       nop
         : 2235             {
    0.00 :   ffff800008095e0c:       paciasp
    0.00 :   ffff800008095e10:       stp     x29, x30, [sp, #-48]!
    0.00 :   ffff800008095e14:       mrs     x0, sp_el0
    0.00 :   ffff800008095e18:       mov     x29, sp
    0.00 :   ffff800008095e1c:       stp     x19, x20, [sp, #16]
         : 2241             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800008095e20:       mrs     x19, sp_el0
         : 26               migrate_enable():
         : 2238             if (p->migration_disabled > 1) {
    0.00 :   ffff800008095e24:       ldrh    w1, [x19, #768]
         : 2235             {
    0.00 :   ffff800008095e28:       ldr     x2, [x0, #1168]
   16.67 :   ffff800008095e2c:       str     x2, [sp, #40] // core.c:2235
    0.00 :   ffff800008095e30:       mov     x2, #0x0                        // #0
         : 2238             if (p->migration_disabled > 1) {
    0.00 :   ffff800008095e34:       cmp     w1, #0x1
    0.00 :   ffff800008095e38:       b.hi    ffff800008095ef0 <migrate_enable+0xf0>  // b.pmore
         : 2243             if (WARN_ON_ONCE(!p->migration_disabled))
    0.00 :   ffff800008095e3c:       b.ne    ffff800008095efc <migrate_enable+0xfc>  // b.any
         : 2245             __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff800008095e40:       ldr     w0, [x19, #8]
         : 48               pc += val;
    0.00 :   ffff800008095e44:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff800008095e48:       str     w0, [x19, #8]
         : 51               migrate_enable():
         : 2251             if (p->cpus_ptr != &p->cpus_mask)
   16.66 :   ffff800008095e4c:       ldr     x0, [x19, #712] // core.c:2251
    0.00 :   ffff800008095e50:       add     x20, x19, #0x2d8
    0.00 :   ffff800008095e54:       cmp     x0, x20
    0.00 :   ffff800008095e58:       b.eq    ffff800008095e80 <migrate_enable+0x80>  // b.none
         : 2256             __set_cpus_allowed_ptr():
         : 2940             rq = task_rq_lock(p, &rf);
    0.00 :   ffff800008095e5c:       add     x1, sp, #0x20
    0.00 :   ffff800008095e60:       mov     x0, x19
    0.00 :   ffff800008095e64:       bl      ffff800008092240 <task_rq_lock>
         : 2941             return __set_cpus_allowed_ptr_locked(p, new_mask, flags, rq, &rf);
    0.00 :   ffff800008095e68:       mov     x3, x0
    0.00 :   ffff800008095e6c:       add     x4, sp, #0x20
    0.00 :   ffff800008095e70:       mov     x1, x20
    0.00 :   ffff800008095e74:       mov     x0, x19
    0.00 :   ffff800008095e78:       mov     w2, #0x4                        // #4
    0.00 :   ffff800008095e7c:       bl      ffff800008095990 <__set_cpus_allowed_ptr_locked>
         : 2948             migrate_enable():
         : 2260             this_rq()->nr_pinned--;
    0.00 :   ffff800008095e80:       adrp    x0, ffff800009395000 <cpu_worker_pools+0x240>
    0.00 :   ffff800008095e84:       add     x0, x0, #0x3c0
         : 2259             p->migration_disabled = 0;
    0.00 :   ffff800008095e88:       strh    wzr, [x19, #768]
         : 2261             __kern_my_cpu_offset():
         :
         : 41               /*
         : 42               * We want to allow caching the value, so avoid using volatile and
         : 43               * instead use a fake stack read to hazard against barrier().
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008095e8c:       mrs     x1, tpidr_el1
         : 47               migrate_enable():
         : 2260             this_rq()->nr_pinned--;
    0.00 :   ffff800008095e90:       add     x0, x0, x1
    0.00 :   ffff800008095e94:       ldr     w1, [x0, #3088]
    0.00 :   ffff800008095e98:       sub     w1, w1, #0x1
    0.00 :   ffff800008095e9c:       str     w1, [x0, #3088]
         : 2265             get_current():
    0.00 :   ffff800008095ea0:       mrs     x1, sp_el0
         : 20               __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff800008095ea4:       ldr     x0, [x1, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff800008095ea8:       sub     x0, x0, #0x1
   16.67 :   ffff800008095eac:       str     w0, [x1, #8] // preempt.h:65
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff800008095eb0:       cbnz    x0, ffff800008095ee0 <migrate_enable+0xe0>
         : 81               migrate_enable():
         : 2261             preempt_enable();
    0.00 :   ffff800008095eb4:       bl      ffff800008ae8860 <preempt_schedule>
         : 2262             }
    0.00 :   ffff800008095eb8:       mrs     x0, sp_el0
    0.00 :   ffff800008095ebc:       ldr     x2, [sp, #40]
   16.67 :   ffff800008095ec0:       ldr     x1, [x0, #1168] // core.c:2262
    0.00 :   ffff800008095ec4:       subs    x2, x2, x1
    0.00 :   ffff800008095ec8:       mov     x1, #0x0                        // #0
    0.00 :   ffff800008095ecc:       b.ne    ffff800008095f04 <migrate_enable+0x104>  // b.any
    0.00 :   ffff800008095ed0:       ldp     x19, x20, [sp, #16]
   33.34 :   ffff800008095ed4:       ldp     x29, x30, [sp], #48
    0.00 :   ffff800008095ed8:       autiasp
    0.00 :   ffff800008095edc:       ret
         : 2273             __preempt_count_dec_and_test():
    0.00 :   ffff800008095ee0:       ldr     x0, [x1, #8]
    0.00 :   ffff800008095ee4:       cbnz    x0, ffff800008095eb8 <migrate_enable+0xb8>
         : 76               migrate_enable():
         : 2261             preempt_enable();
    0.00 :   ffff800008095ee8:       bl      ffff800008ae8860 <preempt_schedule>
    0.00 :   ffff800008095eec:       b       ffff800008095eb8 <migrate_enable+0xb8>
         : 2239             p->migration_disabled--;
    0.00 :   ffff800008095ef0:       sub     w1, w1, #0x1
    0.00 :   ffff800008095ef4:       strh    w1, [x19, #768]
         : 2240             return;
    0.00 :   ffff800008095ef8:       b       ffff800008095eb8 <migrate_enable+0xb8>
         : 2243             if (WARN_ON_ONCE(!p->migration_disabled))
    0.00 :   ffff800008095efc:       brk     #0x800
    0.00 :   ffff800008095f00:       b       ffff800008095eb8 <migrate_enable+0xb8>
         : 2262             }
    0.00 :   ffff800008095f04:       bl      ffff800008ae5de0 <__stack_chk_fail>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 rethook.c:132
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (5 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          1b2bf4: 1
                          1b2bf8: 3
                          1b2c00: 1
                   h->nr_samples: 5
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000081a2bf0 <rethook_recycle>:
         : 6                rethook_recycle():
         : 129              *
         : 130              * Return back the @node to @node::rethook. If the @node::rethook is already
         : 131              * marked as freed, this will free the @node.
         : 132              */
         : 133              void rethook_recycle(struct rethook_node *node)
         : 134              {
    0.00 :   ffff8000081a2bf0:       bti     c
         : 132              lockdep_assert_preemption_disabled();
         :
         : 134              if (likely(READ_ONCE(node->rethook->handler)))
   18.86 :   ffff8000081a2bf4:       ldr     x3, [x0, #24] // rethook.c:132
   62.28 :   ffff8000081a2bf8:       ldr     x1, [x3, #8]
    0.00 :   ffff8000081a2bfc:       cbz     x1, ffff8000081a2c70 <rethook_recycle+0x80>
   18.86 :   ffff8000081a2c00:       mov     x2, x0
         : 139              arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff8000081a2c04:       b       ffff8000081a2c60 <rethook_recycle+0x70>
         : 45               __lse_atomic_fetch_add_release():
         : 60               ATOMIC_FETCH_OP(        , al, op, asm_op, "memory")
         :
         : 62               ATOMIC_FETCH_OPS(andnot, ldclr)
         : 63               ATOMIC_FETCH_OPS(or, ldset)
         : 64               ATOMIC_FETCH_OPS(xor, ldeor)
         : 65               ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff8000081a2c08:       mov     w1, #0x80000000                 // #-2147483648
    0.00 :   ffff8000081a2c0c:       ldaddl  w1, w1, [x0]
         : 68               freelist_add():
         : 70               {
         : 71               /*
         : 72               * We know that the should-be-on-freelist bit is 0 at this point, so
         : 73               * it's safe to set it using a fetch_add.
         : 74               */
         : 75               if (!atomic_fetch_add_release(REFS_ON_FREELIST, &node->refs)) {
    0.00 :   ffff8000081a2c10:       cbz     w1, ffff8000081a2c18 <rethook_recycle+0x28>
    0.00 :   ffff8000081a2c14:       ret
         : 78               __freelist_add():
         : 46               struct freelist_node *head = READ_ONCE(list->head);
    0.00 :   ffff8000081a2c18:       ldr     x1, [x3, #16]!
         : 48               arch_atomic_set_release():
         : 243              #ifndef arch_atomic_set_release
         : 244              static __always_inline void
         : 245              arch_atomic_set_release(atomic_t *v, int i)
         : 246              {
         : 247              if (__native_word(atomic_t)) {
         : 248              smp_store_release(&(v)->counter, i);
    0.00 :   ffff8000081a2c1c:       mov     w4, #0x1                        // #1
         : 250              __ll_sc_atomic_fetch_add_release():
         : 111              ATOMIC_FETCH_OP (        , dmb ish,  , l, "memory", __VA_ARGS__)\
         : 112              ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         : 113              ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         : 114              ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         : 116              ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000081a2c20:       mov     w5, #0x7fffffff                 // #2147483647
         : 118              __freelist_add():
         : 49               WRITE_ONCE(node->next, head);
    0.00 :   ffff8000081a2c24:       str     x1, [x2, #8]
         : 51               arch_atomic_set_release():
    0.00 :   ffff8000081a2c28:       stlr    w4, [x2]
         : 244              arch_static_branch_jump():
    0.00 :   ffff8000081a2c2c:       b       ffff8000081a2c94 <rethook_recycle+0xa4>
         : 39               __lse__cmpxchg_case_rel_64():
         : 271              __CMPXCHG_CASE(w,  , acq_, 32,  a, "memory")
         : 272              __CMPXCHG_CASE(x,  , acq_, 64,  a, "memory")
         : 273              __CMPXCHG_CASE(w, b, rel_,  8,  l, "memory")
         : 274              __CMPXCHG_CASE(w, h, rel_, 16,  l, "memory")
         : 275              __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         : 276              __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
    0.00 :   ffff8000081a2c30:       mov     x0, x3
    0.00 :   ffff8000081a2c34:       mov     x6, x1
    0.00 :   ffff8000081a2c38:       casl    x6, x2, [x3]
    0.00 :   ffff8000081a2c3c:       mov     x0, x6
         : 281              __freelist_add():
         : 52               if (!try_cmpxchg_release(&list->head, &head, node)) {
    0.00 :   ffff8000081a2c40:       cmp     x1, x0
    0.00 :   ffff8000081a2c44:       b.eq    ffff8000081a2c14 <rethook_recycle+0x24>  // b.none
         : 55               arch_static_branch_jump():
    0.00 :   ffff8000081a2c48:       b       ffff8000081a2c9c <rethook_recycle+0xac>
         : 39               __lse_atomic_fetch_add_release():
         : 60               ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff8000081a2c4c:       ldaddl  w5, w1, [x2]
         : 62               __freelist_add():
         : 57               if (atomic_fetch_add_release(REFS_ON_FREELIST - 1, &node->refs) == 1)
    0.00 :   ffff8000081a2c50:       cmp     w1, #0x1
    0.00 :   ffff8000081a2c54:       b.ne    ffff8000081a2c14 <rethook_recycle+0x24>  // b.any
    0.00 :   ffff8000081a2c58:       mov     x1, x0
    0.00 :   ffff8000081a2c5c:       b       ffff8000081a2c24 <rethook_recycle+0x34>
         : 62               __ll_sc_atomic_fetch_add_release():
    0.00 :   ffff8000081a2c60:       mov     w0, #0x80000000                 // #-2147483648
    0.00 :   ffff8000081a2c64:       b       ffff8000081a3168 <rethook_trampoline_handler+0x208>
         : 113              freelist_add():
         : 70               if (!atomic_fetch_add_release(REFS_ON_FREELIST, &node->refs)) {
    0.00 :   ffff8000081a2c68:       cbnz    w1, ffff8000081a2c14 <rethook_recycle+0x24>
    0.00 :   ffff8000081a2c6c:       b       ffff8000081a2c18 <rethook_recycle+0x28>
         : 73               rethook_recycle():
         : 129              {
    0.00 :   ffff8000081a2c70:       paciasp
    0.00 :   ffff8000081a2c74:       stp     x29, x30, [sp, #-16]!
         : 135              freelist_add(&node->freelist, &node->rethook->pool);
         : 136              else
         : 137              call_rcu(&node->rcu, free_rethook_node_rcu);
    0.00 :   ffff8000081a2c78:       adrp    x1, ffff8000081a2000 <destroy_local_trace_uprobe+0x90>
         : 129              {
    0.00 :   ffff8000081a2c7c:       mov     x29, sp
         : 135              call_rcu(&node->rcu, free_rethook_node_rcu);
    0.00 :   ffff8000081a2c80:       add     x1, x1, #0x9b0
    0.00 :   ffff8000081a2c84:       bl      ffff8000080e90e0 <call_rcu>
         : 136              }
    0.00 :   ffff8000081a2c88:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000081a2c8c:       autiasp
    0.00 :   ffff8000081a2c90:       ret
         : 140              __ll_sc__cmpxchg_case_rel_64():
         : 310              __CMPXCHG_CASE(w,  , acq_, 32,        , a,  , "memory", K)
         : 311              __CMPXCHG_CASE( ,  , acq_, 64,        , a,  , "memory", L)
         : 312              __CMPXCHG_CASE(w, b, rel_,  8,        ,  , l, "memory", K)
         : 313              __CMPXCHG_CASE(w, h, rel_, 16,        ,  , l, "memory", K)
         : 314              __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         : 315              __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
    0.00 :   ffff8000081a2c94:       b       ffff8000081a3180 <rethook_trampoline_handler+0x220>
    0.00 :   ffff8000081a2c98:       b       ffff8000081a2c40 <rethook_recycle+0x50>
         : 318              __ll_sc_atomic_fetch_add_release():
         : 111              ATOMIC_OPS(add, add, I)
    0.00 :   ffff8000081a2c9c:       b       ffff8000081a319c <rethook_trampoline_handler+0x23c>
    0.00 :   ffff8000081a2ca0:       b       ffff8000081a2c50 <rethook_recycle+0x60>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   60.00 tree_stall.h:124
   20.00 tree_plugin.h:377
   20.00 tree_plugin.h:406
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (5 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           f3d70: 1
                           f3d78: 2
                           f3d80: 1
                           f3d90: 1
                   h->nr_samples: 5
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000080e3d70 <__rcu_read_lock>:
         : 6                __rcu_read_lock():
         : 124              /* Don't print RCU CPU stall warnings during a kernel panic. */
         : 125              static int rcu_panic(struct notifier_block *this, unsigned long ev, void *ptr)
         : 126              {
         : 127              rcu_cpu_stall_suppress = 1;
         : 128              return NOTIFY_DONE;
         : 129              }
   20.00 :   ffff8000080e3d70:       bti     c // tree_stall.h:124
    0.00 :   ffff8000080e3d74:       nop
   40.00 :   ffff8000080e3d78:       nop
         : 133              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000080e3d7c:       mrs     x1, sp_el0
         : 26               rcu_preempt_read_enter():
         : 377              /* limit value for ->rcu_read_lock_nesting. */
         : 378              #define RCU_NEST_PMAX (INT_MAX / 2)
         :
         : 380              static void rcu_preempt_read_enter(void)
         : 381              {
         : 382              WRITE_ONCE(current->rcu_read_lock_nesting, READ_ONCE(current->rcu_read_lock_nesting) + 1);
   20.00 :   ffff8000080e3d80:       ldr     w0, [x1, #772] // tree_plugin.h:377
         : 384              __rcu_read_lock():
         : 399              * Preemptible RCU implementation for rcu_read_lock().
         : 400              * Just increment ->rcu_read_lock_nesting, shared state will be updated
         : 401              * if we block.
         : 402              */
         : 403              void __rcu_read_lock(void)
         : 404              {
    0.00 :   ffff8000080e3d84:       paciasp
         : 406              rcu_preempt_read_enter():
         : 377              WRITE_ONCE(current->rcu_read_lock_nesting, READ_ONCE(current->rcu_read_lock_nesting) + 1);
    0.00 :   ffff8000080e3d88:       add     w0, w0, #0x1
    0.00 :   ffff8000080e3d8c:       str     w0, [x1, #772]
         : 380              __rcu_read_lock():
         : 406              if (IS_ENABLED(CONFIG_PROVE_LOCKING))
         : 407              WARN_ON_ONCE(rcu_preempt_depth() > RCU_NEST_PMAX);
         : 408              if (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) && rcu_state.gp_kthread)
         : 409              WRITE_ONCE(current->rcu_read_unlock_special.b.need_qs, true);
         : 410              barrier();  /* critical section after entry code. */
         : 411              }
   20.00 :   ffff8000080e3d90:       autiasp // tree_plugin.h:406
    0.00 :   ffff8000080e3d94:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   50.00 context_tracking.h:121
   25.00 tree.c:721
   25.00 preempt.h:74
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (4 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           f5798: 2
                           f57c8: 1
                           f57d0: 1
                   h->nr_samples: 4
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000080e5770 <rcu_is_watching>:
         : 6                rcu_is_watching():
         : 714              *
         : 715              * Make notrace because it can be called by the internal functions of
         : 716              * ftrace, and making this notrace removes unnecessary recursion calls.
         : 717              */
         : 718              notrace bool rcu_is_watching(void)
         : 719              {
    0.00 :   ffff8000080e5770:       paciasp
    0.00 :   ffff8000080e5774:       stp     x29, x30, [sp, #-32]!
         : 722              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000080e5778:       mrs     x1, sp_el0
         : 26               rcu_is_watching():
    0.00 :   ffff8000080e577c:       mov     x29, sp
         : 715              __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000080e5780:       ldr     w0, [x1, #8]
         : 48               pc += val;
    0.00 :   ffff8000080e5784:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff8000080e5788:       str     w0, [x1, #8]
         : 51               rcu_dynticks_curr_cpu_in_eqs():
         : 121              *
         : 122              * No ordering, as we are sampling CPU-local information.
         : 123              */
         : 124              static __always_inline bool rcu_dynticks_curr_cpu_in_eqs(void)
         : 125              {
         : 126              return !(arch_atomic_read(this_cpu_ptr(&context_tracking.state)) & RCU_DYNTICKS_IDX);
    0.00 :   ffff8000080e578c:       adrp    x0, ffff800009392000 <bpf_bprintf_bufs+0x110>
    0.00 :   ffff8000080e5790:       add     x0, x0, #0xb80
         : 129              __kern_my_cpu_offset():
         :
         : 41               /*
         : 42               * We want to allow caching the value, so avoid using volatile and
         : 43               * instead use a fake stack read to hazard against barrier().
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff8000080e5794:       mrs     x2, tpidr_el1
         : 47               rcu_dynticks_curr_cpu_in_eqs():
   50.00 :   ffff8000080e5798:       ldr     w0, [x0, x2] // context_tracking.h:121
         : 122              rcu_is_watching():
         : 718              bool ret;
         :
         : 720              preempt_disable_notrace();
         : 721              ret = !rcu_dynticks_curr_cpu_in_eqs();
    0.00 :   ffff8000080e579c:       ubfx    x0, x0, #2, #1
         : 723              __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000080e57a0:       ldr     x2, [x1, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000080e57a4:       sub     x2, x2, #0x1
    0.00 :   ffff8000080e57a8:       str     w2, [x1, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000080e57ac:       cbnz    x2, ffff8000080e57c8 <rcu_is_watching+0x58>
    0.00 :   ffff8000080e57b0:       str     w0, [sp, #28]
         : 82               rcu_is_watching():
         : 719              preempt_enable_notrace();
    0.00 :   ffff8000080e57b4:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000080e57b8:       ldr     w0, [sp, #28]
         : 721              return ret;
         : 722              }
    0.00 :   ffff8000080e57bc:       ldp     x29, x30, [sp], #32
    0.00 :   ffff8000080e57c0:       autiasp
    0.00 :   ffff8000080e57c4:       ret
         : 726              __preempt_count_dec_and_test():
   25.00 :   ffff8000080e57c8:       ldr     x1, [x1, #8] // preempt.h:74
    0.00 :   ffff8000080e57cc:       cbz     x1, ffff8000080e57b0 <rcu_is_watching+0x40>
         : 76               rcu_is_watching():
   25.00 :   ffff8000080e57d0:       ldp     x29, x30, [sp], #32 // tree.c:721
    0.00 :   ffff8000080e57d4:       autiasp
    0.00 :   ffff8000080e57d8:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   50.00 idr.c:175
   50.00 idr.c:174
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (4 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          55caac: 2
                          55cab8: 1
                          55cac0: 1
                   h->nr_samples: 4
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000854caa0 <idr_find>:
         : 6                idr_find():
         : 173              * pointers lifetimes are correctly managed.
         : 174              *
         : 175              * Return: The pointer associated with this ID.
         : 176              */
         : 177              void *idr_find(const struct idr *idr, unsigned long id)
         : 178              {
    0.00 :   ffff80000854caa0:       paciasp
    0.00 :   ffff80000854caa4:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff80000854caa8:       mov     x29, sp
         : 174              return radix_tree_lookup(&idr->idr_rt, id - idr->idr_base);
   50.00 :   ffff80000854caac:       ldr     w2, [x0, #16] // idr.c:174
    0.00 :   ffff80000854cab0:       sub     x1, x1, x2
    0.00 :   ffff80000854cab4:       bl      ffff8000085524e0 <radix_tree_lookup>
         : 175              }
   25.00 :   ffff80000854cab8:       ldp     x29, x30, [sp], #16 // idr.c:175
    0.00 :   ffff80000854cabc:       autiasp
   25.00 :   ffff80000854cac0:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   66.67 tree_plugin.h:382
   33.33 atomic_ll_sc.h:222
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (3 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           fa804: 1
                           fa810: 2
                   h->nr_samples: 3
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000080ea800 <__rcu_read_unlock>:
         : 6                __rcu_read_unlock():
         : 222              ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         : 223              ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         : 224              ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         : 226              ATOMIC64_OPS(and, and, L)
         : 227              ATOMIC64_OPS(or, orr, L)
    0.00 :   ffff8000080ea800:       bti     c
   33.33 :   ffff8000080ea804:       nop // atomic_ll_sc.h:222
    0.00 :   ffff8000080ea808:       nop
         : 231              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000080ea80c:       mrs     x0, sp_el0
         : 26               rcu_preempt_read_exit():
         : 382              WRITE_ONCE(current->rcu_read_lock_nesting, READ_ONCE(current->rcu_read_lock_nesting) + 1);
         : 383              }
         :
         : 385              static int rcu_preempt_read_exit(void)
         : 386              {
         : 387              int ret = READ_ONCE(current->rcu_read_lock_nesting) - 1;
   66.67 :   ffff8000080ea810:       ldr     w1, [x0, #772] // tree_plugin.h:382
    0.00 :   ffff8000080ea814:       sub     w1, w1, #0x1
         :
         : 385              WRITE_ONCE(current->rcu_read_lock_nesting, ret);
    0.00 :   ffff8000080ea818:       str     w1, [x0, #772]
         : 387              __rcu_read_unlock():
         : 421              void __rcu_read_unlock(void)
         : 422              {
         : 423              struct task_struct *t = current;
         :
         : 425              barrier();  // critical section before exit code.
         : 426              if (rcu_preempt_read_exit() == 0) {
    0.00 :   ffff8000080ea81c:       cbnz    w1, ffff8000080ea828 <__rcu_read_unlock+0x28>
         : 423              barrier();  // critical-section exit before .s check.
         : 424              if (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))
    0.00 :   ffff8000080ea820:       ldr     w1, [x0, #776]
    0.00 :   ffff8000080ea824:       cbnz    w1, ffff8000080ea82c <__rcu_read_unlock+0x2c>
    0.00 :   ffff8000080ea828:       ret
         : 417              {
    0.00 :   ffff8000080ea82c:       paciasp
    0.00 :   ffff8000080ea830:       stp     x29, x30, [sp, #-16]!
    0.00 :   ffff8000080ea834:       mov     x29, sp
         : 424              rcu_read_unlock_special(t);
    0.00 :   ffff8000080ea838:       bl      ffff8000080ea674 <rcu_read_unlock_special>
         : 431              if (IS_ENABLED(CONFIG_PROVE_LOCKING)) {
         : 432              int rrln = rcu_preempt_depth();
         :
         : 434              WARN_ON_ONCE(rrln < 0 || rrln > RCU_NEST_PMAX);
         : 435              }
         : 436              }
    0.00 :   ffff8000080ea83c:       ldp     x29, x30, [sp], #16
    0.00 :   ffff8000080ea840:       autiasp
    0.00 :   ffff8000080ea844:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

   50.00 lsm_hook_defs.h:205
   50.00 lsm_hook_defs.h:206
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (2 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          213440: 1
                          213458: 1
                   h->nr_samples: 2
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008203440 <bpf_lsm_task_getpgid>:
         : 6                bpf_lsm_task_getpgid():
         : 205              LSM_HOOK(int, 0, task_fix_setuid, struct cred *new, const struct cred *old,
         : 206              int flags)
         : 207              LSM_HOOK(int, 0, task_fix_setgid, struct cred *new, const struct cred * old,
         : 208              int flags)
         : 209              LSM_HOOK(int, 0, task_fix_setgroups, struct cred *new, const struct cred * old)
         : 210              LSM_HOOK(int, 0, task_setpgid, struct task_struct *p, pid_t pgid)
   50.00 :   ffff800008203440:       bti     c // lsm_hook_defs.h:205
    0.00 :   ffff800008203444:       nop
    0.00 :   ffff800008203448:       nop
         : 206              LSM_HOOK(int, 0, task_getpgid, struct task_struct *p)
    0.00 :   ffff80000820344c:       mov     w0, #0x0                        // #0
    0.00 :   ffff800008203450:       paciasp
    0.00 :   ffff800008203454:       autiasp
   50.00 :   ffff800008203458:       ret // lsm_hook_defs.h:206

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 security.c:1819
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (1 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          49551c: 1
                   h->nr_samples: 1
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000084854f0 <security_task_getpgid>:
         : 6                security_task_getpgid():
         : 1815             }
         :
         : 1817             int security_task_setpgid(struct task_struct *p, pid_t pgid)
         : 1818             {
         : 1819             return call_int_hook(task_setpgid, 0, p, pgid);
         : 1820             }
    0.00 :   ffff8000084854f0:       bti     c
    0.00 :   ffff8000084854f4:       nop
    0.00 :   ffff8000084854f8:       nop
         :
         : 1819             int security_task_getpgid(struct task_struct *p)
         : 1820             {
    0.00 :   ffff8000084854fc:       paciasp
    0.00 :   ffff800008485500:       stp     x29, x30, [sp, #-32]!
         : 1819             return call_int_hook(task_getpgid, 0, p);
    0.00 :   ffff800008485504:       adrp    x1, ffff800008e22000 <family+0x20>
         : 1818             {
    0.00 :   ffff800008485508:       mov     x29, sp
    0.00 :   ffff80000848550c:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800008485510:       mov     x20, x0
         : 1819             return call_int_hook(task_getpgid, 0, p);
    0.00 :   ffff800008485514:       ldr     x19, [x1, #2376]
    0.00 :   ffff800008485518:       cbz     x19, ffff800008485538 <security_task_getpgid+0x48>
  100.00 :   ffff80000848551c:       nop // security.c:1819
    0.00 :   ffff800008485520:       ldr     x1, [x19, #24]
    0.00 :   ffff800008485524:       mov     x0, x20
    0.00 :   ffff800008485528:       blr     x1
    0.00 :   ffff80000848552c:       cbnz    w0, ffff80000848553c <security_task_getpgid+0x4c>
    0.00 :   ffff800008485530:       ldr     x19, [x19]
    0.00 :   ffff800008485534:       cbnz    x19, ffff800008485520 <security_task_getpgid+0x30>
         : 1818             {
    0.00 :   ffff800008485538:       mov     w0, #0x0                        // #0
         : 1820             }
    0.00 :   ffff80000848553c:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008485540:       ldp     x29, x30, [sp], #32
    0.00 :   ffff800008485544:       autiasp
    0.00 :   ffff800008485548:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 sched.h:1534
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (1 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           8c380: 1
                   h->nr_samples: 1
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000807c370 <pid_vnr>:
         : 6                pid_vnr():
         : 483              upid = &pid->numbers[ns->level];
         : 484              if (upid->ns == ns)
         : 485              nr = upid->nr;
         : 486              }
         : 487              return nr;
         : 488              }
    0.00 :   ffff80000807c370:       bti     c
    0.00 :   ffff80000807c374:       nop
    0.00 :   ffff80000807c378:       nop
         : 492              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80000807c37c:       mrs     x1, sp_el0
         : 26               task_pid():
         : 1534             */
         : 1535             };
         :
         : 1537             static inline struct pid *task_pid(struct task_struct *task)
         : 1538             {
         : 1539             return task->thread_pid;
  100.00 :   ffff80000807c380:       ldr     x2, [x1, #1264] // sched.h:1534
         : 1541             pid_vnr():
         : 487              EXPORT_SYMBOL_GPL(pid_nr_ns);
         :
         : 489              pid_t pid_vnr(struct pid *pid)
         : 490              {
    0.00 :   ffff80000807c384:       paciasp
    0.00 :   ffff80000807c388:       mov     x1, x0
         : 493              ns_of_pid():
         : 154              *       the resulting NULL pid-ns.
         : 155              */
         : 156              static inline struct pid_namespace *ns_of_pid(struct pid *pid)
         : 157              {
         : 158              struct pid_namespace *ns = NULL;
         : 159              if (pid)
    0.00 :   ffff80000807c38c:       cbz     x2, ffff80000807c39c <pid_vnr+0x2c>
         : 155              ns = pid->numbers[pid->level].ns;
    0.00 :   ffff80000807c390:       ldr     w0, [x2, #4]
    0.00 :   ffff80000807c394:       add     x0, x2, x0, lsl #4
    0.00 :   ffff80000807c398:       ldr     x2, [x0, #104]
         : 159              pid_nr_ns():
         : 475              pid_t nr = 0;
    0.00 :   ffff80000807c39c:       mov     w0, #0x0                        // #0
         : 477              if (pid && ns->level <= pid->level) {
    0.00 :   ffff80000807c3a0:       cbz     x1, ffff80000807c3c8 <pid_vnr+0x58>
    0.00 :   ffff80000807c3a4:       ldr     w4, [x1, #4]
    0.00 :   ffff80000807c3a8:       ldr     w3, [x2, #64]
    0.00 :   ffff80000807c3ac:       cmp     w3, w4
    0.00 :   ffff80000807c3b0:       b.hi    ffff80000807c3c8 <pid_vnr+0x58>  // b.pmore
         : 479              if (upid->ns == ns)
    0.00 :   ffff80000807c3b4:       ubfiz   x3, x3, #4, #32
    0.00 :   ffff80000807c3b8:       add     x1, x1, x3
    0.00 :   ffff80000807c3bc:       ldr     x3, [x1, #104]
    0.00 :   ffff80000807c3c0:       cmp     x3, x2
    0.00 :   ffff80000807c3c4:       b.eq    ffff80000807c3d0 <pid_vnr+0x60>  // b.none
         : 485              pid_vnr():
         : 489              return pid_nr_ns(pid, task_active_pid_ns(current));
         : 490              }
    0.00 :   ffff80000807c3c8:       autiasp
    0.00 :   ffff80000807c3cc:       ret
         : 493              pid_nr_ns():
         : 480              nr = upid->nr;
    0.00 :   ffff80000807c3d0:       ldr     w0, [x1, #96]
         : 482              pid_vnr():
         : 489              }
    0.00 :   ffff80000807c3d4:       autiasp
    0.00 :   ffff80000807c3d8:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 radix-tree.c:818
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (1 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          5624f8: 1
                   h->nr_samples: 1
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000085524e0 <radix_tree_lookup>:
         : 6                radix_tree_lookup():
         : 816              *       must manage lifetimes of leaf nodes (eg. RCU may also be used to free
         : 817              *       them safely). No RCU barriers are required to access or modify the
         : 818              *       returned item, however.
         : 819              */
         : 820              void *radix_tree_lookup(const struct radix_tree_root *root, unsigned long index)
         : 821              {
    0.00 :   ffff8000085524e0:       paciasp
    0.00 :   ffff8000085524e4:       stp     x29, x30, [sp, #-16]!
         : 817              return __radix_tree_lookup(root, index, NULL, NULL);
    0.00 :   ffff8000085524e8:       mov     x3, #0x0                        // #0
         : 816              {
    0.00 :   ffff8000085524ec:       mov     x29, sp
         : 817              return __radix_tree_lookup(root, index, NULL, NULL);
    0.00 :   ffff8000085524f0:       mov     x2, #0x0                        // #0
    0.00 :   ffff8000085524f4:       bl      ffff8000085523d0 <__radix_tree_lookup>
         : 818              }
  100.00 :   ffff8000085524f8:       ldp     x29, x30, [sp], #16 // radix-tree.c:818
    0.00 :   ffff8000085524fc:       autiasp
    0.00 :   ffff800008552500:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 copy_template.S:177
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (1 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          55337c: 1
                   h->nr_samples: 1
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008543180 <__arch_copy_to_user>:
         : 6                __arch_copy_to_user():
         : 56               user_stp 9997f, \reg1, \reg2, \ptr, \val
         : 57               .endm
         :
         : 59               end     .req    x5
         : 60               srcin   .req    x15
         : 61               SYM_FUNC_START(__arch_copy_to_user)
    0.00 :   ffff800008543180:       bti     c
         : 57               add     end, x0, x2
    0.00 :   ffff800008543184:       add     x5, x0, x2
         : 58               mov     srcin, x1
    0.00 :   ffff800008543188:       mov     x15, x1
         : 42               C_l     .req    x11
         : 43               C_h     .req    x12
         : 44               D_l     .req    x13
         : 45               D_h     .req    x14
         :
         : 47               mov     dst, dstin
    0.00 :   ffff80000854318c:       mov     x6, x0
         : 43               cmp     count, #16
    0.00 :   ffff800008543190:       cmp     x2, #0x10
         : 45               /*When memory length is less than 16, the accessed are not aligned.*/
         : 46               b.lo    .Ltiny15
    0.00 :   ffff800008543194:       b.cc    ffff800008543234 <__arch_copy_to_user+0xb4>  // b.lo, b.ul, b.last
         :
         : 48               neg     tmp2, src
    0.00 :   ffff800008543198:       neg     x4, x1
         : 48               ands    tmp2, tmp2, #15/* Bytes to reach alignment. */
    0.00 :   ffff80000854319c:       ands    x4, x4, #0xf
         : 49               b.eq    .LSrcAligned
    0.00 :   ffff8000085431a0:       b.eq    ffff8000085431e8 <__arch_copy_to_user+0x68>  // b.none
         : 50               sub     count, count, tmp2
    0.00 :   ffff8000085431a4:       sub     x2, x2, x4
         : 57               * Copy the leading memory data from src to dst in an increasing
         : 58               * address order.By this way,the risk of overwriting the source
         : 59               * memory data is eliminated when the distance between src and
         : 60               * dst is less than 16. The memory accesses here are alignment.
         : 61               */
         : 62               tbz     tmp2, #0, 1f
    0.00 :   ffff8000085431a8:       tbz     w4, #0, ffff8000085431b8 <__arch_copy_to_user+0x38>
         : 58               ldrb1   tmp1w, src, #1
    0.00 :   ffff8000085431ac:       ldrb    w3, [x1], #1
         : 59               strb1   tmp1w, dst, #1
    0.00 :   ffff8000085431b0:       sttrb   w3, [x6]
    0.00 :   ffff8000085431b4:       add     x6, x6, #0x1
         : 61               1:
         : 62               tbz     tmp2, #1, 2f
    0.00 :   ffff8000085431b8:       tbz     w4, #1, ffff8000085431c8 <__arch_copy_to_user+0x48>
         : 62               ldrh1   tmp1w, src, #2
    0.00 :   ffff8000085431bc:       ldrh    w3, [x1], #2
         : 63               strh1   tmp1w, dst, #2
    0.00 :   ffff8000085431c0:       sttrh   w3, [x6]
    0.00 :   ffff8000085431c4:       add     x6, x6, #0x2
         : 65               2:
         : 66               tbz     tmp2, #2, 3f
    0.00 :   ffff8000085431c8:       tbz     w4, #2, ffff8000085431d8 <__arch_copy_to_user+0x58>
         : 66               ldr1    tmp1w, src, #4
    0.00 :   ffff8000085431cc:       ldr     w3, [x1], #4
         : 67               str1    tmp1w, dst, #4
    0.00 :   ffff8000085431d0:       sttr    w3, [x6]
    0.00 :   ffff8000085431d4:       add     x6, x6, #0x4
         : 69               3:
         : 70               tbz     tmp2, #3, .LSrcAligned
    0.00 :   ffff8000085431d8:       tbz     w4, #3, ffff8000085431e8 <__arch_copy_to_user+0x68>
         : 70               ldr1    tmp1, src, #8
    0.00 :   ffff8000085431dc:       ldr     x3, [x1], #8
         : 71               str1    tmp1, dst, #8
    0.00 :   ffff8000085431e0:       sttr    x3, [x6]
    0.00 :   ffff8000085431e4:       add     x6, x6, #0x8
         :
         : 75               .LSrcAligned:
         : 76               cmp     count, #64
    0.00 :   ffff8000085431e8:       cmp     x2, #0x40
         : 75               b.ge    .Lcpy_over64
    0.00 :   ffff8000085431ec:       b.ge    ffff800008543278 <__arch_copy_to_user+0xf8>  // b.tcont
         : 85               .Ltail63:
         : 86               /*
         : 87               * Copy up to 48 bytes of data. At this point we only need the
         : 88               * bottom 6 bits of count to be accurate.
         : 89               */
         : 90               ands    tmp1, count, #0x30
    0.00 :   ffff8000085431f0:       ands    x3, x2, #0x30
         : 86               b.eq    .Ltiny15
    0.00 :   ffff8000085431f4:       b.eq    ffff800008543234 <__arch_copy_to_user+0xb4>  // b.none
         : 87               cmp     tmp1w, #0x20
    0.00 :   ffff8000085431f8:       cmp     w3, #0x20
         : 88               b.eq    1f
    0.00 :   ffff8000085431fc:       b.eq    ffff800008543214 <__arch_copy_to_user+0x94>  // b.none
         : 89               b.lt    2f
    0.00 :   ffff800008543200:       b.lt    ffff800008543224 <__arch_copy_to_user+0xa4>  // b.tstop
         : 90               ldp1    A_l, A_h, src, #16
    0.00 :   ffff800008543204:       ldp     x7, x8, [x1], #16
         : 91               stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543208:       sttr    x7, [x6]
    0.00 :   ffff80000854320c:       sttr    x8, [x6, #8]
    0.00 :   ffff800008543210:       add     x6, x6, #0x10
         : 93               1:
         : 94               ldp1    A_l, A_h, src, #16
    0.00 :   ffff800008543214:       ldp     x7, x8, [x1], #16
         : 94               stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543218:       sttr    x7, [x6]
    0.00 :   ffff80000854321c:       sttr    x8, [x6, #8]
    0.00 :   ffff800008543220:       add     x6, x6, #0x10
         : 96               2:
         : 97               ldp1    A_l, A_h, src, #16
    0.00 :   ffff800008543224:       ldp     x7, x8, [x1], #16
         : 97               stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543228:       sttr    x7, [x6]
    0.00 :   ffff80000854322c:       sttr    x8, [x6, #8]
    0.00 :   ffff800008543230:       add     x6, x6, #0x10
         : 110              * precondition that src address is at least 16 bytes bigger than dst
         : 111              * address,otherwise some source data will be overwritten when memove
         : 112              * call memcpy directly. To make memmove simpler and decouple the
         : 113              * memcpy's dependency on memmove, withdrew the original process.
         : 114              */
         : 115              tbz     count, #3, 1f
    0.00 :   ffff800008543234:       tbz     w2, #3, ffff800008543244 <__arch_copy_to_user+0xc4>
         : 111              ldr1    tmp1, src, #8
    0.00 :   ffff800008543238:       ldr     x3, [x1], #8
         : 112              str1    tmp1, dst, #8
    0.00 :   ffff80000854323c:       sttr    x3, [x6]
    0.00 :   ffff800008543240:       add     x6, x6, #0x8
         : 114              1:
         : 115              tbz     count, #2, 2f
    0.00 :   ffff800008543244:       tbz     w2, #2, ffff800008543254 <__arch_copy_to_user+0xd4>
         : 115              ldr1    tmp1w, src, #4
    0.00 :   ffff800008543248:       ldr     w3, [x1], #4
         : 116              str1    tmp1w, dst, #4
    0.00 :   ffff80000854324c:       sttr    w3, [x6]
    0.00 :   ffff800008543250:       add     x6, x6, #0x4
         : 118              2:
         : 119              tbz     count, #1, 3f
    0.00 :   ffff800008543254:       tbz     w2, #1, ffff800008543264 <__arch_copy_to_user+0xe4>
         : 119              ldrh1   tmp1w, src, #2
    0.00 :   ffff800008543258:       ldrh    w3, [x1], #2
         : 120              strh1   tmp1w, dst, #2
    0.00 :   ffff80000854325c:       sttrh   w3, [x6]
    0.00 :   ffff800008543260:       add     x6, x6, #0x2
         : 122              3:
         : 123              tbz     count, #0, .Lexitfunc
    0.00 :   ffff800008543264:       tbz     w2, #0, ffff800008543390 <__arch_copy_to_user+0x210>
         : 123              ldrb1   tmp1w, src, #1
    0.00 :   ffff800008543268:       ldrb    w3, [x1], #1
         : 124              strb1   tmp1w, dst, #1
    0.00 :   ffff80000854326c:       sttrb   w3, [x6]
    0.00 :   ffff800008543270:       add     x6, x6, #0x1
         :
         : 127              b       .Lexitfunc
    0.00 :   ffff800008543274:       b       ffff800008543390 <__arch_copy_to_user+0x210>
         :
         : 130              .Lcpy_over64:
         : 131              subs    count, count, #128
    0.00 :   ffff800008543278:       subs    x2, x2, #0x80
         : 130              b.ge    .Lcpy_body_large
    0.00 :   ffff80000854327c:       b.ge    ffff800008543300 <__arch_copy_to_user+0x180>  // b.tcont
         : 135              /*
         : 136              * Less than 128 bytes to copy, so handle 64 here and then jump
         : 137              * to the tail.
         : 138              */
         : 139              ldp1    A_l, A_h, src, #16
    0.00 :   ffff800008543280:       ldp     x7, x8, [x1], #16
         : 136              stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543284:       sttr    x7, [x6]
    0.00 :   ffff800008543288:       sttr    x8, [x6, #8]
    0.00 :   ffff80000854328c:       add     x6, x6, #0x10
         : 137              ldp1    B_l, B_h, src, #16
    0.00 :   ffff800008543290:       ldp     x9, x10, [x1], #16
         : 138              ldp1    C_l, C_h, src, #16
    0.00 :   ffff800008543294:       ldp     x11, x12, [x1], #16
         : 139              stp1    B_l, B_h, dst, #16
    0.00 :   ffff800008543298:       sttr    x9, [x6]
    0.00 :   ffff80000854329c:       sttr    x10, [x6, #8]
    0.00 :   ffff8000085432a0:       add     x6, x6, #0x10
         : 140              stp1    C_l, C_h, dst, #16
    0.00 :   ffff8000085432a4:       sttr    x11, [x6]
    0.00 :   ffff8000085432a8:       sttr    x12, [x6, #8]
    0.00 :   ffff8000085432ac:       add     x6, x6, #0x10
         : 141              ldp1    D_l, D_h, src, #16
    0.00 :   ffff8000085432b0:       ldp     x13, x14, [x1], #16
         : 142              stp1    D_l, D_h, dst, #16
    0.00 :   ffff8000085432b4:       sttr    x13, [x6]
    0.00 :   ffff8000085432b8:       sttr    x14, [x6, #8]
    0.00 :   ffff8000085432bc:       add     x6, x6, #0x10
         :
         : 145              tst     count, #0x3f
    0.00 :   ffff8000085432c0:       tst     x2, #0x3f
         : 145              b.ne    .Ltail63
    0.00 :   ffff8000085432c4:       b.ne    ffff8000085431f0 <__arch_copy_to_user+0x70>  // b.any
         : 146              b       .Lexitfunc
    0.00 :   ffff8000085432c8:       b       ffff800008543390 <__arch_copy_to_user+0x210>
    0.00 :   ffff8000085432cc:       nop
    0.00 :   ffff8000085432d0:       nop
    0.00 :   ffff8000085432d4:       nop
    0.00 :   ffff8000085432d8:       nop
    0.00 :   ffff8000085432dc:       nop
    0.00 :   ffff8000085432e0:       nop
    0.00 :   ffff8000085432e4:       nop
    0.00 :   ffff8000085432e8:       nop
    0.00 :   ffff8000085432ec:       nop
    0.00 :   ffff8000085432f0:       nop
    0.00 :   ffff8000085432f4:       nop
    0.00 :   ffff8000085432f8:       nop
    0.00 :   ffff8000085432fc:       nop
         : 155              * 64 bytes per line this ensures the entire loop is in one line.
         : 156              */
         : 157              .p2align        L1_CACHE_SHIFT
         : 158              .Lcpy_body_large:
         : 159              /* pre-get 64 bytes data. */
         : 160              ldp1    A_l, A_h, src, #16
    0.00 :   ffff800008543300:       ldp     x7, x8, [x1], #16
         : 156              ldp1    B_l, B_h, src, #16
    0.00 :   ffff800008543304:       ldp     x9, x10, [x1], #16
         : 157              ldp1    C_l, C_h, src, #16
    0.00 :   ffff800008543308:       ldp     x11, x12, [x1], #16
         : 158              ldp1    D_l, D_h, src, #16
    0.00 :   ffff80000854330c:       ldp     x13, x14, [x1], #16
         : 164              1:
         : 165              /*
         : 166              * interlace the load of next 64 bytes data block with store of the last
         : 167              * loaded 64 bytes data.
         : 168              */
         : 169              stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543310:       sttr    x7, [x6]
    0.00 :   ffff800008543314:       sttr    x8, [x6, #8]
    0.00 :   ffff800008543318:       add     x6, x6, #0x10
         : 165              ldp1    A_l, A_h, src, #16
    0.00 :   ffff80000854331c:       ldp     x7, x8, [x1], #16
         : 166              stp1    B_l, B_h, dst, #16
    0.00 :   ffff800008543320:       sttr    x9, [x6]
    0.00 :   ffff800008543324:       sttr    x10, [x6, #8]
    0.00 :   ffff800008543328:       add     x6, x6, #0x10
         : 167              ldp1    B_l, B_h, src, #16
    0.00 :   ffff80000854332c:       ldp     x9, x10, [x1], #16
         : 168              stp1    C_l, C_h, dst, #16
    0.00 :   ffff800008543330:       sttr    x11, [x6]
    0.00 :   ffff800008543334:       sttr    x12, [x6, #8]
    0.00 :   ffff800008543338:       add     x6, x6, #0x10
         : 169              ldp1    C_l, C_h, src, #16
    0.00 :   ffff80000854333c:       ldp     x11, x12, [x1], #16
         : 170              stp1    D_l, D_h, dst, #16
    0.00 :   ffff800008543340:       sttr    x13, [x6]
    0.00 :   ffff800008543344:       sttr    x14, [x6, #8]
    0.00 :   ffff800008543348:       add     x6, x6, #0x10
         : 171              ldp1    D_l, D_h, src, #16
    0.00 :   ffff80000854334c:       ldp     x13, x14, [x1], #16
         : 172              subs    count, count, #64
    0.00 :   ffff800008543350:       subs    x2, x2, #0x40
         : 173              b.ge    1b
    0.00 :   ffff800008543354:       b.ge    ffff800008543310 <__arch_copy_to_user+0x190>  // b.tcont
         : 174              stp1    A_l, A_h, dst, #16
    0.00 :   ffff800008543358:       sttr    x7, [x6]
    0.00 :   ffff80000854335c:       sttr    x8, [x6, #8]
    0.00 :   ffff800008543360:       add     x6, x6, #0x10
         : 175              stp1    B_l, B_h, dst, #16
    0.00 :   ffff800008543364:       sttr    x9, [x6]
    0.00 :   ffff800008543368:       sttr    x10, [x6, #8]
    0.00 :   ffff80000854336c:       add     x6, x6, #0x10
         : 176              stp1    C_l, C_h, dst, #16
    0.00 :   ffff800008543370:       sttr    x11, [x6]
    0.00 :   ffff800008543374:       sttr    x12, [x6, #8]
    0.00 :   ffff800008543378:       add     x6, x6, #0x10
         : 177              stp1    D_l, D_h, dst, #16
  100.00 :   ffff80000854337c:       sttr    x13, [x6] // copy_template.S:177
    0.00 :   ffff800008543380:       sttr    x14, [x6, #8]
    0.00 :   ffff800008543384:       add     x6, x6, #0x10
         :
         : 180              tst     count, #0x3f
    0.00 :   ffff800008543388:       tst     x2, #0x3f
         : 180              b.ne    .Ltail63
    0.00 :   ffff80000854338c:       b.ne    ffff8000085431f0 <__arch_copy_to_user+0x70>  // b.any
         : 60               #include "copy_template.S"
         : 61               mov     x0, #0
    0.00 :   ffff800008543390:       mov     x0, #0x0                        // #0
         : 61               ret
    0.00 :   ffff800008543394:       ret
         :
         : 65               // Exception fixups
         : 66               9997:   cmp     dst, dstin
    0.00 :   ffff800008543398:       cmp     x6, x0
         : 65               b.ne    9998f
    0.00 :   ffff80000854339c:       b.ne    ffff8000085433ac <__arch_copy_to_user+0x22c>  // b.any
         : 67               // Before being absolutely sure we couldn't copy anything, try harder
         : 68               ldrb    tmp1w, [srcin]
    0.00 :   ffff8000085433a0:       ldrb    w3, [x15]
         : 68               USER(9998f, sttrb tmp1w, [dst])
    0.00 :   ffff8000085433a4:       sttrb   w3, [x6]
         : 69               add     dst, dst, #1
    0.00 :   ffff8000085433a8:       add     x6, x6, #0x1
         : 70               9998:   sub     x0, end, dst                    // bytes not copied
    0.00 :   ffff8000085433ac:       sub     x0, x5, x6
         : 71               ret
    0.00 :   ffff8000085433b0:       ret

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 percpu.h:127
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (2 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          2ff5f8: 2
                   h->nr_samples: 2
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff8000082ef560 <get_mem_cgroup_from_mm>:
         : 6                get_mem_cgroup_from_mm():
         : 210              ATOMIC64_FETCH_OP (, dmb ish,  , l, "memory", __VA_ARGS__)      \
         : 211              ATOMIC64_FETCH_OP (_relaxed,,  ,  ,         , __VA_ARGS__)      \
         : 212              ATOMIC64_FETCH_OP (_acquire,, a,  , "memory", __VA_ARGS__)      \
         : 213              ATOMIC64_FETCH_OP (_release,,  , l, "memory", __VA_ARGS__)
         :
         : 215              ATOMIC64_OPS(add, add, I)
    0.00 :   ffff8000082ef560:       bti     c
    0.00 :   ffff8000082ef564:       nop
    0.00 :   ffff8000082ef568:       nop
         : 938              * 2) current->mm->memcg, if available
         : 939              * 3) root memcg
         : 940              * If mem_cgroup is disabled, NULL is returned.
         : 941              */
         : 942              struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)
         : 943              {
    0.00 :   ffff8000082ef56c:       paciasp
    0.00 :   ffff8000082ef570:       stp     x29, x30, [sp, #-64]!
    0.00 :   ffff8000082ef574:       mov     x29, sp
    0.00 :   ffff8000082ef578:       stp     x19, x20, [sp, #16]
         : 948              arch_static_branch():
         : 21               #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         : 23               static __always_inline bool arch_static_branch(struct static_key *key,
         : 24               bool branch)
         : 25               {
         : 26               asm_volatile_goto(
    0.00 :   ffff8000082ef57c:       nop
         : 28               get_mem_cgroup_from_mm():
         : 953              *
         : 954              * No need to css_get on root memcg as the reference
         : 955              * counting is disabled on the root level in the
         : 956              * cgroup core. See CSS_NO_REF.
         : 957              */
         : 958              if (unlikely(!mm)) {
    0.00 :   ffff8000082ef580:       stp     x21, x22, [sp, #32]
    0.00 :   ffff8000082ef584:       mov     x21, x0
    0.00 :   ffff8000082ef588:       cbz     x0, ffff8000082ef688 <get_mem_cgroup_from_mm+0x128>
         :
         : 970              rcu_read_lock();
         : 971              do {
         : 972              memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));
         : 973              if (unlikely(!memcg))
         : 974              memcg = root_mem_cgroup;
    0.00 :   ffff8000082ef58c:       adrp    x22, ffff80000963b000 <mm_slots_hash+0x19d0>
         : 976              rcu_read_lock():
         : 704              * read-side critical sections may be preempted and they may also block, but
         : 705              * only when acquiring spinlocks that are subject to priority inheritance.
         : 706              */
         : 707              static __always_inline void rcu_read_lock(void)
         : 708              {
         : 709              __rcu_read_lock();
    0.00 :   ffff8000082ef590:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 711              get_mem_cgroup_from_mm():
         : 967              memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));
    0.00 :   ffff8000082ef594:       ldr     x1, [x21, #800]
         : 969              mem_cgroup_from_task():
         : 911              if (unlikely(!p))
    0.00 :   ffff8000082ef598:       cbz     x1, ffff8000082ef624 <get_mem_cgroup_from_mm+0xc4>
         : 913              task_css():
         : 495              * See task_css_check().
         : 496              */
         : 497              static inline struct cgroup_subsys_state *task_css(struct task_struct *task,
         : 498              int subsys_id)
         : 499              {
         : 500              return task_css_check(task, subsys_id, false);
    0.00 :   ffff8000082ef59c:       ldr     x1, [x1, #2080]
    0.00 :   ffff8000082ef5a0:       ldr     x19, [x1, #32]
         : 503              mem_cgroup_from_css():
         : 791              }
         : 792              #endif
         :
         : 794              static inline
         : 795              struct mem_cgroup *mem_cgroup_from_css(struct cgroup_subsys_state *css){
         : 796              return css ? container_of(css, struct mem_cgroup, css) : NULL;
    0.00 :   ffff8000082ef5a4:       cbz     x19, ffff8000082ef624 <get_mem_cgroup_from_mm+0xc4>
         : 798              css_tryget():
         : 353              if (!(css->flags & CSS_NO_REF))
    0.00 :   ffff8000082ef5a8:       ldr     w1, [x19, #84]
    0.00 :   ffff8000082ef5ac:       tbz     w1, #0, ffff8000082ef5cc <get_mem_cgroup_from_mm+0x6c>
         : 356              rcu_read_unlock():
         : 738              static inline void rcu_read_unlock(void)
         : 739              {
         : 740              RCU_LOCKDEP_WARN(!rcu_is_watching(),
         : 741              "rcu_read_unlock() used illegally while idle");
         : 742              __release(RCU);
         : 743              __rcu_read_unlock();
    0.00 :   ffff8000082ef5b0:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 745              get_mem_cgroup_from_mm():
         : 972              } while (!css_tryget(&memcg->css));
         : 973              rcu_read_unlock();
         : 974              return memcg;
    0.00 :   ffff8000082ef5b4:       ldp     x21, x22, [sp, #32]
         : 973              }
    0.00 :   ffff8000082ef5b8:       mov     x0, x19
    0.00 :   ffff8000082ef5bc:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000082ef5c0:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000082ef5c4:       autiasp
    0.00 :   ffff8000082ef5c8:       ret
         : 979              rcu_read_lock():
         : 704              __rcu_read_lock();
    0.00 :   ffff8000082ef5cc:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 706              __ref_is_percpu():
         : 174              * READ_ONCE() is required when fetching it.
         : 175              *
         : 176              * The dependency ordering from the READ_ONCE() pairs
         : 177              * with smp_store_release() in __percpu_ref_switch_to_percpu().
         : 178              */
         : 179              percpu_ptr = READ_ONCE(ref->percpu_count_ptr);
    0.00 :   ffff8000082ef5d0:       ldr     x20, [x19, #16]
         : 182              * Theoretically, the following could test just ATOMIC; however,
         : 183              * then we'd have to mask off DEAD separately as DEAD may be
         : 184              * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
         : 185              * implies ATOMIC anyway.  Test them together.
         : 186              */
         : 187              if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
    0.00 :   ffff8000082ef5d4:       tst     x20, #0x3
    0.00 :   ffff8000082ef5d8:       b.ne    ffff8000082ef654 <get_mem_cgroup_from_mm+0xf4>  // b.any
         : 190              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff8000082ef5dc:       mrs     x21, sp_el0
         : 26               __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000082ef5e0:       ldr     w0, [x21, #8]
         : 48               pc += val;
    0.00 :   ffff8000082ef5e4:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff8000082ef5e8:       str     w0, [x21, #8]
         : 51               percpu_ref_tryget_many():
         : 244              bool ret;
         :
         : 246              rcu_read_lock();
         :
         : 248              if (__ref_is_percpu(ref, &percpu_count)) {
         : 249              this_cpu_add(*percpu_count, nr);
    0.00 :   ffff8000082ef5ec:       bl      ffff8000082ea4e0 <__kern_my_cpu_offset>
         : 251              __percpu_add_case_64():
         :
         : 128              PERCPU_RW_OPS(8)
         : 129              PERCPU_RW_OPS(16)
         : 130              PERCPU_RW_OPS(32)
         : 131              PERCPU_RW_OPS(64)
         : 132              PERCPU_OP(add, add, stadd)
    0.00 :   ffff8000082ef5f0:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000082ef5f4:       add     x0, x20, x0
  100.00 :   ffff8000082ef5f8:       ldxr    x3, [x0] // percpu.h:127
    0.00 :   ffff8000082ef5fc:       add     x3, x3, x1
    0.00 :   ffff8000082ef600:       stxr    w2, x3, [x0]
    0.00 :   ffff8000082ef604:       cbnz    w2, ffff8000082ef5f8 <get_mem_cgroup_from_mm+0x98>
         : 139              __preempt_count_dec_and_test():
         : 62               }
         :
         : 64               static inline bool __preempt_count_dec_and_test(void)
         : 65               {
         : 66               struct thread_info *ti = current_thread_info();
         : 67               u64 pc = READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000082ef608:       ldr     x0, [x21, #8]
         :
         : 66               /* Update only the count field, leaving need_resched unchanged */
         : 67               WRITE_ONCE(ti->preempt.count, --pc);
    0.00 :   ffff8000082ef60c:       sub     x0, x0, #0x1
    0.00 :   ffff8000082ef610:       str     w0, [x21, #8]
         : 74               * need of a reschedule. Otherwise, we need to reload the
         : 75               * preempt_count in case the need_resched flag was cleared by an
         : 76               * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
         : 77               * pair.
         : 78               */
         : 79               return !pc || !READ_ONCE(ti->preempt_count);
    0.00 :   ffff8000082ef614:       cbnz    x0, ffff8000082ef62c <get_mem_cgroup_from_mm+0xcc>
         : 81               percpu_ref_tryget_many():
    0.00 :   ffff8000082ef618:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
         : 245              rcu_read_unlock():
         : 738              __rcu_read_unlock();
    0.00 :   ffff8000082ef61c:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 740              percpu_ref_tryget_many():
         : 252              ret = atomic_long_add_unless(&ref->data->count, nr, 0);
         : 253              }
         :
         : 255              rcu_read_unlock();
         :
         : 257              return ret;
    0.00 :   ffff8000082ef620:       b       ffff8000082ef5b0 <get_mem_cgroup_from_mm+0x50>
         : 259              get_mem_cgroup_from_mm():
         : 969              memcg = root_mem_cgroup;
    0.00 :   ffff8000082ef624:       ldr     x19, [x22, #1584]
    0.00 :   ffff8000082ef628:       b       ffff8000082ef5a8 <get_mem_cgroup_from_mm+0x48>
         : 972              __preempt_count_dec_and_test():
    0.00 :   ffff8000082ef62c:       ldr     x0, [x21, #8]
    0.00 :   ffff8000082ef630:       cbz     x0, ffff8000082ef618 <get_mem_cgroup_from_mm+0xb8>
         : 76               rcu_read_unlock():
    0.00 :   ffff8000082ef634:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 739              get_mem_cgroup_from_mm():
    0.00 :   ffff8000082ef638:       b       ffff8000082ef5b0 <get_mem_cgroup_from_mm+0x50>
         : 942              return NULL;
    0.00 :   ffff8000082ef63c:       mov     x19, #0x0                       // #0
         : 973              }
    0.00 :   ffff8000082ef640:       mov     x0, x19
    0.00 :   ffff8000082ef644:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff8000082ef648:       ldp     x29, x30, [sp], #64
    0.00 :   ffff8000082ef64c:       autiasp
    0.00 :   ffff8000082ef650:       ret
         : 979              percpu_ref_tryget_many():
         : 247              ret = atomic_long_add_unless(&ref->data->count, nr, 0);
    0.00 :   ffff8000082ef654:       str     x23, [sp, #48]
    0.00 :   ffff8000082ef658:       ldr     x23, [x19, #24]
         : 250              arch_atomic64_fetch_add_unless():
         : 2363             * Returns original value of @v
         : 2364             */
         : 2365             static __always_inline s64
         : 2366             arch_atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
         : 2367             {
         : 2368             s64 c = arch_atomic64_read(v);
    0.00 :   ffff8000082ef65c:       ldr     x20, [x23]
         : 2370             __cmpxchg_mb():
         : 175              }
         :
         : 177              __CMPXCHG_GEN()
         : 178              __CMPXCHG_GEN(_acq)
         : 179              __CMPXCHG_GEN(_rel)
         : 180              __CMPXCHG_GEN(_mb)
    0.00 :   ffff8000082ef660:       add     x2, x20, #0x1
    0.00 :   ffff8000082ef664:       mov     x1, x20
    0.00 :   ffff8000082ef668:       mov     x0, x23
         : 184              arch_atomic64_fetch_add_unless():
         :
         : 2367             do {
         : 2368             if (unlikely(c == u))
    0.00 :   ffff8000082ef66c:       cbz     x20, ffff8000082ef708 <get_mem_cgroup_from_mm+0x1a8>
         : 2370             __cmpxchg_mb():
    0.00 :   ffff8000082ef670:       bl      ffff8000082ea910 <__cmpxchg_case_mb_64>
         : 176              arch_atomic64_try_cmpxchg():
         : 2196             if (unlikely(r != o))
    0.00 :   ffff8000082ef674:       cmp     x20, x0
    0.00 :   ffff8000082ef678:       b.ne    ffff8000082ef714 <get_mem_cgroup_from_mm+0x1b4>  // b.any
    0.00 :   ffff8000082ef67c:       ldr     x23, [sp, #48]
         : 2200             rcu_read_unlock():
    0.00 :   ffff8000082ef680:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 739              percpu_ref_tryget_many():
         : 252              return ret;
    0.00 :   ffff8000082ef684:       b       ffff8000082ef5b0 <get_mem_cgroup_from_mm+0x50>
         : 254              get_current():
    0.00 :   ffff8000082ef688:       mrs     x1, sp_el0
         : 20               preempt_count():
         : 13               return READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000082ef68c:       ldr     w0, [x1, #8]
    0.00 :   ffff8000082ef690:       ldr     w3, [x1, #8]
    0.00 :   ffff8000082ef694:       ldr     w2, [x1, #8]
         : 17               active_memcg():
         : 920              if (!in_task())
    0.00 :   ffff8000082ef698:       and     w0, w0, #0xf00000
    0.00 :   ffff8000082ef69c:       and     w3, w3, #0xf0000
    0.00 :   ffff8000082ef6a0:       orr     w0, w0, w3
    0.00 :   ffff8000082ef6a4:       and     w2, w2, #0x100
    0.00 :   ffff8000082ef6a8:       orr     w0, w0, w2
    0.00 :   ffff8000082ef6ac:       cbnz    w0, ffff8000082ef6d4 <get_mem_cgroup_from_mm+0x174>
         : 923              return current->active_memcg;
    0.00 :   ffff8000082ef6b0:       ldr     x19, [x1, #2552]
         : 925              get_mem_cgroup_from_mm():
         : 955              if (unlikely(memcg)) {
    0.00 :   ffff8000082ef6b4:       cbnz    x19, ffff8000082ef71c <get_mem_cgroup_from_mm+0x1bc>
         : 957              get_current():
    0.00 :   ffff8000082ef6b8:       mrs     x0, sp_el0
         : 20               get_mem_cgroup_from_mm():
         : 960              mm = current->mm;
    0.00 :   ffff8000082ef6bc:       ldr     x21, [x0, #976]
         : 961              if (unlikely(!mm))
    0.00 :   ffff8000082ef6c0:       cbnz    x21, ffff8000082ef58c <get_mem_cgroup_from_mm+0x2c>
         : 962              return root_mem_cgroup;
    0.00 :   ffff8000082ef6c4:       adrp    x0, ffff80000963b000 <mm_slots_hash+0x19d0>
    0.00 :   ffff8000082ef6c8:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000082ef6cc:       ldr     x19, [x0, #1584]
    0.00 :   ffff8000082ef6d0:       b       ffff8000082ef5b8 <get_mem_cgroup_from_mm+0x58>
         : 967              __preempt_count_add():
         : 47               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff8000082ef6d4:       ldr     w0, [x1, #8]
         : 48               pc += val;
    0.00 :   ffff8000082ef6d8:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff8000082ef6dc:       str     w0, [x1, #8]
         : 51               active_memcg():
         : 921              return this_cpu_read(int_active_memcg);
    0.00 :   ffff8000082ef6e0:       adrp    x19, ffff800009393000 <vm_event_states+0xe8>
    0.00 :   ffff8000082ef6e4:       add     x19, x19, #0x570
    0.00 :   ffff8000082ef6e8:       bl      ffff8000082ea4e0 <__kern_my_cpu_offset>
    0.00 :   ffff8000082ef6ec:       add     x19, x19, #0x58
         : 926              __percpu_read_64():
         : 126              PERCPU_RW_OPS(64)
    0.00 :   ffff8000082ef6f0:       ldr     x19, [x19, x0]
         : 128              active_memcg():
    0.00 :   ffff8000082ef6f4:       bl      ffff8000082ea4b0 <__preempt_count_dec_and_test>
    0.00 :   ffff8000082ef6f8:       tst     w0, #0xff
    0.00 :   ffff8000082ef6fc:       b.eq    ffff8000082ef6b4 <get_mem_cgroup_from_mm+0x154>  // b.none
    0.00 :   ffff8000082ef700:       bl      ffff800008ae88d0 <preempt_schedule_notrace>
    0.00 :   ffff8000082ef704:       b       ffff8000082ef6b4 <get_mem_cgroup_from_mm+0x154>
         : 926              rcu_read_unlock():
    0.00 :   ffff8000082ef708:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 739              percpu_ref_tryget_many():
    0.00 :   ffff8000082ef70c:       ldr     x23, [sp, #48]
    0.00 :   ffff8000082ef710:       b       ffff8000082ef594 <get_mem_cgroup_from_mm+0x34>
         : 254              arch_atomic64_try_cmpxchg():
    0.00 :   ffff8000082ef714:       mov     x20, x0
    0.00 :   ffff8000082ef718:       b       ffff8000082ef660 <get_mem_cgroup_from_mm+0x100>
         : 254              css_get():
         : 323              if (!(css->flags & CSS_NO_REF))
    0.00 :   ffff8000082ef71c:       ldr     w0, [x19, #84]
    0.00 :   ffff8000082ef720:       tbz     w0, #0, ffff8000082ef72c <get_mem_cgroup_from_mm+0x1cc>
    0.00 :   ffff8000082ef724:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000082ef728:       b       ffff8000082ef5b8 <get_mem_cgroup_from_mm+0x58>
         : 328              percpu_ref_get():
         : 222              percpu_ref_get_many(ref, 1);
    0.00 :   ffff8000082ef72c:       add     x0, x19, #0x10
    0.00 :   ffff8000082ef730:       mov     x1, #0x1                        // #1
    0.00 :   ffff8000082ef734:       bl      ffff8000082ef4c0 <percpu_ref_get_many>
         : 223              }
    0.00 :   ffff8000082ef738:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff8000082ef73c:       b       ffff8000082ef5b8 <get_mem_cgroup_from_mm+0x58>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 atomic_ll_sc.h:314
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (1 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          22c6d0: 1
                   h->nr_samples: 1
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff80000821c484 <perf_output_begin>:
         : 6                perf_output_begin():
         : 279              int perf_output_begin_backward(struct perf_output_handle *handle,
         : 280              struct perf_sample_data *data,
         : 281              struct perf_event *event, unsigned int size)
         : 282              {
         : 283              return __perf_output_begin(handle, data, event, size, true);
         : 284              }
    0.00 :   ffff80000821c484:       bti     c
    0.00 :   ffff80000821c488:       nop
    0.00 :   ffff80000821c48c:       nop
         :
         : 285              int perf_output_begin(struct perf_output_handle *handle,
         : 286              struct perf_sample_data *data,
         : 287              struct perf_event *event, unsigned int size)
         : 288              {
    0.00 :   ffff80000821c490:       paciasp
    0.00 :   ffff80000821c494:       stp     x29, x30, [sp, #-96]!
    0.00 :   ffff80000821c498:       mrs     x4, sp_el0
    0.00 :   ffff80000821c49c:       mov     x29, sp
    0.00 :   ffff80000821c4a0:       stp     x19, x20, [sp, #16]
    0.00 :   ffff80000821c4a4:       mov     x20, x0
    0.00 :   ffff80000821c4a8:       mov     x19, x1
    0.00 :   ffff80000821c4ac:       stp     x21, x22, [sp, #32]
    0.00 :   ffff80000821c4b0:       stp     x23, x24, [sp, #48]
    0.00 :   ffff80000821c4b4:       mov     x23, x2
    0.00 :   ffff80000821c4b8:       mov     w24, w3
    0.00 :   ffff80000821c4bc:       ldr     x0, [x4, #1168]
    0.00 :   ffff80000821c4c0:       str     x0, [sp, #88]
    0.00 :   ffff80000821c4c4:       mov     x0, #0x0                        // #0
         : 303              is_write_backward():
         : 1426             return event->pmu->setup_aux;
         : 1427             }
         :
         : 1429             static inline bool is_write_backward(struct perf_event *event)
         : 1430             {
         : 1431             return !!event->attr.write_backward;
    0.00 :   ffff80000821c4c8:       ldr     x22, [x2, #256]
         : 1433             rcu_read_lock():
         : 704              * read-side critical sections may be preempted and they may also block, but
         : 705              * only when acquiring spinlocks that are subject to priority inheritance.
         : 706              */
         : 707              static __always_inline void rcu_read_lock(void)
         : 708              {
         : 709              __rcu_read_lock();
    0.00 :   ffff80000821c4cc:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 711              __perf_output_begin():
         : 167              if (event->parent)
    0.00 :   ffff80000821c4d0:       ldr     x21, [x23, #624]
    0.00 :   ffff80000821c4d4:       cmp     x21, #0x0
    0.00 :   ffff80000821c4d8:       csel    x21, x21, x23, ne       // ne = any
         : 170              rb = rcu_dereference(event->rb);
    0.00 :   ffff80000821c4dc:       ldr     x4, [x21, #704]
         : 171              if (unlikely(!rb))
    0.00 :   ffff80000821c4e0:       cbz     x4, ffff80000821c694 <perf_output_begin+0x210>
         : 174              if (unlikely(rb->paused)) {
    0.00 :   ffff80000821c4e4:       ldr     w0, [x4, #32]
    0.00 :   ffff80000821c4e8:       cbnz    w0, ffff80000821c6d8 <perf_output_begin+0x254>
         : 182              handle->rb    = rb;
    0.00 :   ffff80000821c4ec:       stp     x21, x4, [x20]
         : 184              arch_atomic_long_read():
         : 29               #ifdef CONFIG_64BIT
         :
         : 31               static __always_inline long
         : 32               arch_atomic_long_read(const atomic_long_t *v)
         : 33               {
         : 34               return arch_atomic64_read(v);
    0.00 :   ffff80000821c4f0:       and     x22, x22, #0x8000000
    0.00 :   ffff80000821c4f4:       ldr     x7, [x4, #72]
         : 37               __perf_output_begin():
         : 185              have_lost = local_read(&rb->lost);
    0.00 :   ffff80000821c4f8:       mov     w8, w7
         : 186              if (unlikely(have_lost)) {
    0.00 :   ffff80000821c4fc:       cbnz    w7, ffff80000821c704 <perf_output_begin+0x280>
         : 188              get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff80000821c500:       mrs     x1, sp_el0
         : 26               __preempt_count_add():
         : 47               return !current_thread_info()->preempt.need_resched;
         : 48               }
         :
         : 50               static inline void __preempt_count_add(int val)
         : 51               {
         : 52               u32 pc = READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff80000821c504:       ldr     w0, [x1, #8]
         : 48               pc += val;
    0.00 :   ffff80000821c508:       add     w0, w0, #0x1
         : 49               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff80000821c50c:       str     w0, [x1, #8]
         : 51               perf_output_get_handle():
         : 46               (*(volatile unsigned int *)&rb->nest)++;
    0.00 :   ffff80000821c510:       ldr     w0, [x4, #48]
    0.00 :   ffff80000821c514:       mov     w6, w24
         : 49               arch_atomic_long_cmpxchg():
         : 413              }
         :
         : 415              static __always_inline long
         : 416              arch_atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
         : 417              {
         : 418              return arch_atomic64_cmpxchg(v, old, new);
    0.00 :   ffff80000821c518:       add     x3, x4, #0x28
         : 420              perf_output_get_handle():
    0.00 :   ffff80000821c51c:       add     w0, w0, #0x1
    0.00 :   ffff80000821c520:       str     w0, [x4, #48]
         : 48               arch_atomic_long_read():
         : 29               return arch_atomic64_read(v);
    0.00 :   ffff80000821c524:       ldr     x0, [x4, #64]
         : 31               perf_output_get_handle():
         : 47               handle->wakeup = local_read(&rb->wakeup);
    0.00 :   ffff80000821c528:       str     x0, [x20, #16]
    0.00 :   ffff80000821c52c:       nop
         : 50               __perf_output_begin():
         : 195              tail = READ_ONCE(rb->user_page->data_tail);
    0.00 :   ffff80000821c530:       ldr     x1, [x4, #232]
         : 197              if (!rb->overwrite) {
    0.00 :   ffff80000821c534:       ldr     w2, [x4, #28]
         : 195              tail = READ_ONCE(rb->user_page->data_tail);
    0.00 :   ffff80000821c538:       ldr     x5, [x1, #1032]
         : 197              arch_atomic_long_read():
    0.00 :   ffff80000821c53c:       ldr     x1, [x4, #40]
         : 30               __perf_output_begin():
         : 197              if (!rb->overwrite) {
    0.00 :   ffff80000821c540:       cbnz    w2, ffff80000821c64c <perf_output_begin+0x1c8>
         : 199              perf_data_size():
         : 126              return rb->nr_pages << page_order(rb);
         : 127              }
         :
         : 129              static inline unsigned long perf_data_size(struct perf_buffer *rb)
         : 130              {
         : 131              return rb->nr_pages << (PAGE_SHIFT + page_order(rb));
    0.00 :   ffff80000821c544:       ldr     w2, [x4, #24]
    0.00 :   ffff80000821c548:       lsl     w2, w2, #12
    0.00 :   ffff80000821c54c:       sxtw    x2, w2
         : 135              ring_buffer_has_space():
         : 143              return CIRC_SPACE(head, tail, data_size) >= size;
    0.00 :   ffff80000821c550:       sub     x2, x2, #0x1
         : 142              if (!backward)
    0.00 :   ffff80000821c554:       cbnz    x22, ffff80000821c658 <perf_output_begin+0x1d4>
         : 143              return CIRC_SPACE(head, tail, data_size) >= size;
    0.00 :   ffff80000821c558:       sub     x5, x5, #0x1
    0.00 :   ffff80000821c55c:       sub     x5, x5, x1
    0.00 :   ffff80000821c560:       and     x5, x5, x2
         : 147              __perf_output_begin():
         : 198              if (unlikely(!ring_buffer_has_space(head, tail,
    0.00 :   ffff80000821c564:       cmp     x6, x5
    0.00 :   ffff80000821c568:       b.hi    ffff80000821c66c <perf_output_begin+0x1e8>  // b.pmore
         : 217              head += size;
    0.00 :   ffff80000821c56c:       add     x2, x6, x1
         : 219              arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff80000821c570:       b       ffff80000821c6d0 <perf_output_begin+0x24c>
         : 45               __lse__cmpxchg_case_mb_64():
         : 275              __CMPXCHG_CASE(w,  , rel_, 32,  l, "memory")
         : 276              __CMPXCHG_CASE(x,  , rel_, 64,  l, "memory")
         : 277              __CMPXCHG_CASE(w, b,  mb_,  8, al, "memory")
         : 278              __CMPXCHG_CASE(w, h,  mb_, 16, al, "memory")
         : 279              __CMPXCHG_CASE(w,  ,  mb_, 32, al, "memory")
         : 280              __CMPXCHG_CASE(x,  ,  mb_, 64, al, "memory")
    0.00 :   ffff80000821c574:       mov     x0, x3
    0.00 :   ffff80000821c578:       mov     x5, x1
    0.00 :   ffff80000821c57c:       casal   x5, x2, [x3]
    0.00 :   ffff80000821c580:       mov     x0, x5
         : 285              __perf_output_begin():
         : 220              } while (local_cmpxchg(&rb->head, offset, head) != offset);
    0.00 :   ffff80000821c584:       cmp     x1, x0
    0.00 :   ffff80000821c588:       b.ne    ffff80000821c530 <perf_output_begin+0xac>  // b.any
         : 222              if (backward) {
    0.00 :   ffff80000821c58c:       cbz     x22, ffff80000821c598 <perf_output_begin+0x114>
         : 224              head = (u64)(-head);
    0.00 :   ffff80000821c590:       mov     x1, x2
    0.00 :   ffff80000821c594:       neg     x2, x2
         : 227              arch_atomic_long_read():
    0.00 :   ffff80000821c598:       ldr     x3, [x4, #64]
         : 30               __perf_output_begin():
         : 232              if (unlikely(head - local_read(&rb->wakeup) > rb->watermark))
    0.00 :   ffff80000821c59c:       ldr     x0, [x4, #80]
    0.00 :   ffff80000821c5a0:       sub     x2, x2, x3
    0.00 :   ffff80000821c5a4:       cmp     x2, x0
    0.00 :   ffff80000821c5a8:       b.hi    ffff80000821c738 <perf_output_begin+0x2b4>  // b.pmore
         : 237              handle->page = (offset >> page_shift) & (rb->nr_pages - 1);
    0.00 :   ffff80000821c5ac:       ldr     w0, [x4, #24]
    0.00 :   ffff80000821c5b0:       lsr     x3, x1, #12
         : 238              offset &= (1UL << page_shift) - 1;
    0.00 :   ffff80000821c5b4:       and     x2, x1, #0xfff
         : 240              handle->size = (1UL << page_shift) - offset;
    0.00 :   ffff80000821c5b8:       mov     x1, #0x1000                     // #4096
         : 237              handle->page = (offset >> page_shift) & (rb->nr_pages - 1);
    0.00 :   ffff80000821c5bc:       sub     w0, w0, #0x1
         : 240              handle->size = (1UL << page_shift) - offset;
    0.00 :   ffff80000821c5c0:       sub     x1, x1, x2
         : 237              handle->page = (offset >> page_shift) & (rb->nr_pages - 1);
    0.00 :   ffff80000821c5c4:       and     w0, w0, w3
    0.00 :   ffff80000821c5c8:       str     w0, [x20, #48]
         : 239              handle->addr = rb->data_pages[handle->page] + offset;
    0.00 :   ffff80000821c5cc:       add     x0, x4, w0, sxtw #3
    0.00 :   ffff80000821c5d0:       ldr     x0, [x0, #240]
         : 240              handle->size = (1UL << page_shift) - offset;
    0.00 :   ffff80000821c5d4:       str     x1, [x20, #24]
         : 239              handle->addr = rb->data_pages[handle->page] + offset;
    0.00 :   ffff80000821c5d8:       add     x0, x0, x2
    0.00 :   ffff80000821c5dc:       str     x0, [x20, #40]
         : 242              if (unlikely(have_lost)) {
    0.00 :   ffff80000821c5e0:       cbz     w7, ffff80000821c69c <perf_output_begin+0x218>
         : 244              lost_event.header.type = PERF_RECORD_LOST;
    0.00 :   ffff80000821c5e4:       mov     x2, #0x2                        // #2
         : 246              __xchg_case_mb_64():
         : 60               __XCHG_CASE(w,  , rel_, 32,        ,    ,  ,  , l, "memory")
         : 61               __XCHG_CASE( ,  , rel_, 64,        ,    ,  ,  , l, "memory")
         : 62               __XCHG_CASE(w, b,  mb_,  8, dmb ish, nop,  , a, l, "memory")
         : 63               __XCHG_CASE(w, h,  mb_, 16, dmb ish, nop,  , a, l, "memory")
         : 64               __XCHG_CASE(w,  ,  mb_, 32, dmb ish, nop,  , a, l, "memory")
         : 65               __XCHG_CASE( ,  ,  mb_, 64, dmb ish, nop,  , a, l, "memory")
    0.00 :   ffff80000821c5e8:       mov     x0, #0x0                        // #0
         : 67               __perf_output_begin():
    0.00 :   ffff80000821c5ec:       movk    x2, #0x18, lsl #48
         : 246              lost_event.id          = event->id;
    0.00 :   ffff80000821c5f0:       ldr     x1, [x21, #920]
    0.00 :   ffff80000821c5f4:       stp     x2, x1, [sp, #64]
         : 249              __xchg_case_mb_64():
    0.00 :   ffff80000821c5f8:       add     x2, x4, #0x48
    0.00 :   ffff80000821c5fc:       prfm    pstl1strm, [x2]
    0.00 :   ffff80000821c600:       ldxr    x3, [x2]
    0.00 :   ffff80000821c604:       stlxr   w1, x0, [x2]
    0.00 :   ffff80000821c608:       cbnz    w1, ffff80000821c600 <perf_output_begin+0x17c>
    0.00 :   ffff80000821c60c:       dmb     ish
         : 66               __perf_output_begin():
         : 250              perf_event_header__init_id(&lost_event.header, data, event);
    0.00 :   ffff80000821c610:       mov     x2, x21
    0.00 :   ffff80000821c614:       add     x0, sp, #0x40
    0.00 :   ffff80000821c618:       mov     x1, x19
         : 247              lost_event.lost        = local_xchg(&rb->lost, 0);
    0.00 :   ffff80000821c61c:       str     x3, [sp, #80]
         : 250              perf_event_header__init_id(&lost_event.header, data, event);
    0.00 :   ffff80000821c620:       bl      ffff800008216a30 <perf_event_header__init_id>
         : 251              perf_output_put(handle, lost_event);
    0.00 :   ffff80000821c624:       add     x1, sp, #0x40
    0.00 :   ffff80000821c628:       mov     w2, #0x18                       // #24
    0.00 :   ffff80000821c62c:       mov     x0, x20
    0.00 :   ffff80000821c630:       bl      ffff80000821be50 <perf_output_copy>
         : 252              perf_event__output_id_sample(event, handle, data);
    0.00 :   ffff80000821c634:       mov     x2, x19
    0.00 :   ffff80000821c638:       mov     x1, x20
    0.00 :   ffff80000821c63c:       mov     x0, x21
    0.00 :   ffff80000821c640:       bl      ffff800008216a64 <perf_event__output_id_sample>
         : 255              return 0;
    0.00 :   ffff80000821c644:       mov     w8, #0x0                        // #0
    0.00 :   ffff80000821c648:       b       ffff80000821c69c <perf_output_begin+0x218>
         : 216              if (!backward)
    0.00 :   ffff80000821c64c:       cbz     x22, ffff80000821c56c <perf_output_begin+0xe8>
         : 219              head -= size;
    0.00 :   ffff80000821c650:       sub     x2, x1, x6
    0.00 :   ffff80000821c654:       b       ffff80000821c570 <perf_output_begin+0xec>
         : 222              ring_buffer_has_space():
         : 145              return CIRC_SPACE(tail, head, data_size) >= size;
    0.00 :   ffff80000821c658:       sub     x0, x1, #0x1
    0.00 :   ffff80000821c65c:       sub     x0, x0, x5
    0.00 :   ffff80000821c660:       and     x2, x0, x2
         : 149              __perf_output_begin():
         : 198              if (unlikely(!ring_buffer_has_space(head, tail,
    0.00 :   ffff80000821c664:       cmp     x6, x2
    0.00 :   ffff80000821c668:       b.ls    ffff80000821c650 <perf_output_begin+0x1cc>  // b.plast
         : 201              arch_static_branch_jump():
    0.00 :   ffff80000821c66c:       b       ffff80000821c728 <perf_output_begin+0x2a4>
         : 39               __lse_atomic64_add():
         : 127              ATOMIC64_OP(add, stadd)
    0.00 :   ffff80000821c670:       mov     x0, #0x1                        // #1
    0.00 :   ffff80000821c674:       add     x1, x4, #0x48
    0.00 :   ffff80000821c678:       stadd   x0, [x1]
         : 131              arch_static_branch_jump():
    0.00 :   ffff80000821c67c:       b       ffff80000821c71c <perf_output_begin+0x298>
         : 39               __lse_atomic64_add():
    0.00 :   ffff80000821c680:       mov     x0, #0x1                        // #1
    0.00 :   ffff80000821c684:       add     x1, x21, #0x3a0
    0.00 :   ffff80000821c688:       stadd   x0, [x1]
         : 130              __perf_output_begin():
         : 260              perf_output_put_handle(handle);
    0.00 :   ffff80000821c68c:       mov     x0, x20
    0.00 :   ffff80000821c690:       bl      ffff80000821bbd0 <perf_output_put_handle>
         : 263              rcu_read_unlock():
         : 738              static inline void rcu_read_unlock(void)
         : 739              {
         : 740              RCU_LOCKDEP_WARN(!rcu_is_watching(),
         : 741              "rcu_read_unlock() used illegally while idle");
         : 742              __release(RCU);
         : 743              __rcu_read_unlock();
    0.00 :   ffff80000821c694:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 745              __perf_output_begin():
         : 264              return -ENOSPC;
    0.00 :   ffff80000821c698:       mov     w8, #0xffffffe4                 // #-28
         : 266              perf_output_begin():
         :
         : 289              return __perf_output_begin(handle, data, event, size,
         : 290              unlikely(is_write_backward(event)));
         : 291              }
    0.00 :   ffff80000821c69c:       mrs     x0, sp_el0
    0.00 :   ffff80000821c6a0:       ldr     x2, [sp, #88]
    0.00 :   ffff80000821c6a4:       ldr     x1, [x0, #1168]
    0.00 :   ffff80000821c6a8:       subs    x2, x2, x1
    0.00 :   ffff80000821c6ac:       mov     x1, #0x0                        // #0
    0.00 :   ffff80000821c6b0:       b.ne    ffff80000821c76c <perf_output_begin+0x2e8>  // b.any
    0.00 :   ffff80000821c6b4:       mov     w0, w8
    0.00 :   ffff80000821c6b8:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff80000821c6bc:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff80000821c6c0:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff80000821c6c4:       ldp     x29, x30, [sp], #96
    0.00 :   ffff80000821c6c8:       autiasp
    0.00 :   ffff80000821c6cc:       ret
         : 305              __ll_sc__cmpxchg_case_mb_64():
         : 314              __CMPXCHG_CASE(w,  , rel_, 32,        ,  , l, "memory", K)
         : 315              __CMPXCHG_CASE( ,  , rel_, 64,        ,  , l, "memory", L)
         : 316              __CMPXCHG_CASE(w, b,  mb_,  8, dmb ish,  , l, "memory", K)
         : 317              __CMPXCHG_CASE(w, h,  mb_, 16, dmb ish,  , l, "memory", K)
         : 318              __CMPXCHG_CASE(w,  ,  mb_, 32, dmb ish,  , l, "memory", K)
         : 319              __CMPXCHG_CASE( ,  ,  mb_, 64, dmb ish,  , l, "memory", L)
  100.00 :   ffff80000821c6d0:       b       ffff80000821d4c8 <perf_mmap_to_page+0x254> // atomic_ll_sc.h:314
    0.00 :   ffff80000821c6d4:       b       ffff80000821c584 <perf_output_begin+0x100>
         : 322              __perf_output_begin():
         : 175              if (rb->nr_pages) {
    0.00 :   ffff80000821c6d8:       ldr     w0, [x4, #24]
    0.00 :   ffff80000821c6dc:       cbz     w0, ffff80000821c694 <perf_output_begin+0x210>
         : 178              arch_static_branch_jump():
    0.00 :   ffff80000821c6e0:       b       ffff80000821c748 <perf_output_begin+0x2c4>
         : 39               __lse_atomic64_add():
    0.00 :   ffff80000821c6e4:       mov     x0, #0x1                        // #1
    0.00 :   ffff80000821c6e8:       add     x1, x4, #0x48
    0.00 :   ffff80000821c6ec:       stadd   x0, [x1]
         : 130              arch_static_branch_jump():
    0.00 :   ffff80000821c6f0:       b       ffff80000821c754 <perf_output_begin+0x2d0>
         : 39               __lse_atomic64_add():
    0.00 :   ffff80000821c6f4:       add     x1, x21, #0x3a0
    0.00 :   ffff80000821c6f8:       mov     x0, #0x1                        // #1
    0.00 :   ffff80000821c6fc:       stadd   x0, [x1]
    0.00 :   ffff80000821c700:       b       ffff80000821c694 <perf_output_begin+0x210>
         : 131              __perf_output_begin():
         : 188              if (event->attr.sample_id_all)
    0.00 :   ffff80000821c704:       ldr     x0, [x21, #256]
         : 187              size += sizeof(lost_event);
    0.00 :   ffff80000821c708:       add     w24, w24, #0x18
         : 188              if (event->attr.sample_id_all)
    0.00 :   ffff80000821c70c:       tbz     w0, #18, ffff80000821c500 <perf_output_begin+0x7c>
         : 189              size += event->id_header_size;
    0.00 :   ffff80000821c710:       ldrh    w0, [x21, #346]
    0.00 :   ffff80000821c714:       add     w24, w24, w0
    0.00 :   ffff80000821c718:       b       ffff80000821c500 <perf_output_begin+0x7c>
         : 193              __ll_sc_atomic64_add():
         : 210              ATOMIC64_OPS(add, add, I)
    0.00 :   ffff80000821c71c:       add     x2, x21, #0x3a0
    0.00 :   ffff80000821c720:       b       ffff80000821d4e8 <perf_mmap_to_page+0x274>
    0.00 :   ffff80000821c724:       b       ffff80000821c68c <perf_output_begin+0x208>
    0.00 :   ffff80000821c728:       add     x2, x4, #0x48
    0.00 :   ffff80000821c72c:       b       ffff80000821d500 <perf_mmap_to_page+0x28c>
         : 216              arch_static_branch_jump():
    0.00 :   ffff80000821c730:       b       ffff80000821c71c <perf_output_begin+0x298>
    0.00 :   ffff80000821c734:       b       ffff80000821c680 <perf_output_begin+0x1fc>
    0.00 :   ffff80000821c738:       b       ffff80000821c760 <perf_output_begin+0x2dc>
         : 41               __lse_atomic64_add():
    0.00 :   ffff80000821c73c:       add     x2, x4, #0x40
    0.00 :   ffff80000821c740:       stadd   x0, [x2]
    0.00 :   ffff80000821c744:       b       ffff80000821c5ac <perf_output_begin+0x128>
         : 130              __ll_sc_atomic64_add():
    0.00 :   ffff80000821c748:       add     x2, x4, #0x48
    0.00 :   ffff80000821c74c:       b       ffff80000821d518 <perf_mmap_to_page+0x2a4>
    0.00 :   ffff80000821c750:       b       ffff80000821c6f0 <perf_output_begin+0x26c>
    0.00 :   ffff80000821c754:       add     x2, x21, #0x3a0
    0.00 :   ffff80000821c758:       b       ffff80000821d530 <perf_mmap_to_page+0x2bc>
    0.00 :   ffff80000821c75c:       b       ffff80000821c694 <perf_output_begin+0x210>
    0.00 :   ffff80000821c760:       add     x5, x4, #0x40
    0.00 :   ffff80000821c764:       b       ffff80000821d548 <perf_mmap_to_page+0x2d4>
    0.00 :   ffff80000821c768:       b       ffff80000821c5ac <perf_output_begin+0x128>
         : 219              perf_output_begin():
         : 288              }
    0.00 :   ffff80000821c76c:       bl      ffff800008ae5de0 <__stack_chk_fail>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 jump_label.h:21
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (8 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                           a1b38: 8
                   h->nr_samples: 8
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008091ab0 <finish_task_switch.isra.0>:
         : 6                finish_task_switch.isra.0():
         : 776              static enum hrtimer_restart hrtick(struct hrtimer *timer)
         : 777              {
         : 778              struct rq *rq = container_of(timer, struct rq, hrtick_timer);
         : 779              struct rq_flags rf;
         :
         : 781              WARN_ON_ONCE(cpu_of(rq) != smp_processor_id());
    0.00 :   ffff800008091ab0:       nop
    0.00 :   ffff800008091ab4:       nop
         : 784              finish_task_switch():
         : 5019             * The context switch have flipped the stack from under us and restored the
         : 5020             * local variables which were saved when this task called schedule() in the
         : 5021             * past. prev == current is still correct but we need to recalculate this_rq
         : 5022             * because prev may have moved to another CPU.
         : 5023             */
         : 5024             static struct rq *finish_task_switch(struct task_struct *prev)
    0.00 :   ffff800008091ab8:       paciasp
    0.00 :   ffff800008091abc:       stp     x29, x30, [sp, #-80]!
    0.00 :   ffff800008091ac0:       mov     x29, sp
    0.00 :   ffff800008091ac4:       stp     x19, x20, [sp, #16]
         : 5022             __releases(rq->lock)
         : 5023             {
         : 5024             struct rq *rq = this_rq();
    0.00 :   ffff800008091ac8:       adrp    x20, ffff800009395000 <cpu_worker_pools+0x240>
         : 5026             __kern_my_cpu_offset():
         :
         : 41               /*
         : 42               * We want to allow caching the value, so avoid using volatile and
         : 43               * instead use a fake stack read to hazard against barrier().
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008091acc:       mrs     x1, tpidr_el1
         : 47               finish_task_switch():
         : 5019             static struct rq *finish_task_switch(struct task_struct *prev)
    0.00 :   ffff800008091ad0:       stp     x21, x22, [sp, #32]
         : 5022             struct rq *rq = this_rq();
    0.00 :   ffff800008091ad4:       add     x20, x20, #0x3c0
         : 5019             static struct rq *finish_task_switch(struct task_struct *prev)
    0.00 :   ffff800008091ad8:       stp     x23, x24, [sp, #48]
         : 5022             struct rq *rq = this_rq();
    0.00 :   ffff800008091adc:       add     x20, x20, x1
         : 5024             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800008091ae0:       mrs     x1, sp_el0
         : 26               finish_task_switch():
         : 5019             static struct rq *finish_task_switch(struct task_struct *prev)
    0.00 :   ffff800008091ae4:       str     x25, [sp, #64]
    0.00 :   ffff800008091ae8:       mov     x24, x30
    0.00 :   ffff800008091aec:       mov     x21, x0
         : 5023             preempt_count():
         : 13               #define PREEMPT_NEED_RESCHED    BIT(32)
         : 14               #define PREEMPT_ENABLED (PREEMPT_NEED_RESCHED)
         :
         : 16               static inline int preempt_count(void)
         : 17               {
         : 18               return READ_ONCE(current_thread_info()->preempt.count);
    0.00 :   ffff800008091af0:       ldr     w2, [x1, #8]
         : 20               finish_task_switch():
         : 5023             struct mm_struct *mm = rq->prev_mm;
    0.00 :   ffff800008091af4:       ldr     x22, [x20, #2368]
         : 5037             *         __schedule()
         : 5038             *           raw_spin_lock_irq(&rq->lock)        // 2
         : 5039             *
         : 5040             * Also, see FORK_PREEMPT_COUNT.
         : 5041             */
         : 5042             if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
    0.00 :   ffff800008091af8:       cmp     w2, #0x2
    0.00 :   ffff800008091afc:       b.ne    ffff800008091c9c <finish_task_switch.isra.0+0x1ec>  // b.any
         : 5042             "corrupted preempt_count: %s/%d/0x%x\n",
         : 5043             current->comm, current->pid, preempt_count()))
         : 5044             preempt_count_set(FORK_PREEMPT_COUNT);
         :
         : 5046             rq->prev_mm = NULL;
    0.00 :   ffff800008091b00:       str     xzr, [x20, #2368]
         : 5048             get_current():
    0.00 :   ffff800008091b04:       mrs     x25, sp_el0
         : 20               finish_task_switch():
         : 5055             * We must observe prev->state before clearing prev->on_cpu (in
         : 5056             * finish_task), otherwise a concurrent wakeup can get prev
         : 5057             * running on another CPU and we could rave with its RUNNING -> DEAD
         : 5058             * transition, resulting in a double drop.
         : 5059             */
         : 5060             prev_state = READ_ONCE(prev->__state);
    0.00 :   ffff800008091b08:       ldr     w23, [x21, #24]
         : 5062             arch_static_branch():
         : 21               #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         : 23               static __always_inline bool arch_static_branch(struct static_key *key,
         : 24               bool branch)
         : 25               {
         : 26               asm_volatile_goto(
    0.00 :   ffff800008091b0c:       nop
    0.00 :   ffff800008091b10:       nop
         : 29               finish_task():
         : 4812             smp_store_release(&prev->on_cpu, 0);
    0.00 :   ffff800008091b14:       add     x1, x21, #0x34
    0.00 :   ffff800008091b18:       mov     w0, #0x0                        // #0
    0.00 :   ffff800008091b1c:       stlr    w0, [x1]
         : 4816             __splice_balance_callbacks():
         : 4856             struct callback_head *head = rq->balance_callback;
    0.00 :   ffff800008091b20:       ldr     x19, [x20, #2512]
         : 4858             if (likely(!head))
    0.00 :   ffff800008091b24:       cbnz    x19, ffff800008091c18 <finish_task_switch.isra.0+0x168>
         : 4860             raw_spin_rq_unlock():
         : 588              raw_spin_unlock(rq_lockp(rq));
    0.00 :   ffff800008091b28:       mov     x0, x20
    0.00 :   ffff800008091b2c:       bl      ffff800008af08e0 <_raw_spin_unlock>
         : 591              arch_local_irq_enable():
         : 35               u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
         :
         : 37               WARN_ON_ONCE(pmr != GIC_PRIO_IRQON && pmr != GIC_PRIO_IRQOFF);
         : 38               }
         :
         : 40               asm volatile(ALTERNATIVE(
    0.00 :   ffff800008091b30:       mov     x0, #0xe0                       // #224
    0.00 :   ffff800008091b34:       msr     daifclr, #0x3
         : 43               arch_static_branch():
  100.00 :   ffff800008091b38:       nop // jump_label.h:21
         : 22               finish_task_switch():
         : 5085             *
         : 5086             * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
         : 5087             *   provided by mmdrop(),
         : 5088             * - a sync_core for SYNC_CORE.
         : 5089             */
         : 5090             if (mm) {
    0.00 :   ffff800008091b3c:       cbz     x22, ffff800008091b68 <finish_task_switch.isra.0+0xb8>
         : 5092             get_current():
    0.00 :   ffff800008091b40:       mrs     x0, sp_el0
         : 20               membarrier_mm_sync_core_before_usermode():
         : 423              #include <asm/membarrier.h>
         : 424              #endif
         :
         : 426              static inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
         : 427              {
         : 428              if (current->mm != mm)
    0.00 :   ffff800008091b44:       ldr     x0, [x0, #976]
    0.00 :   ffff800008091b48:       cmp     x22, x0
    0.00 :   ffff800008091b4c:       b.eq    ffff800008091c40 <finish_task_switch.isra.0+0x190>  // b.none
         : 432              arch_static_branch_jump():
         : 38               }
         :
         : 40               static __always_inline bool arch_static_branch_jump(struct static_key *key,
         : 41               bool branch)
         : 42               {
         : 43               asm_volatile_goto(
    0.00 :   ffff800008091b50:       b       ffff800008091c4c <finish_task_switch.isra.0+0x19c>
         : 45               __lse_atomic_fetch_add():
         : 60               ATOMIC_FETCH_OP(        , al, op, asm_op, "memory")
         :
         : 62               ATOMIC_FETCH_OPS(andnot, ldclr)
         : 63               ATOMIC_FETCH_OPS(or, ldset)
         : 64               ATOMIC_FETCH_OPS(xor, ldeor)
         : 65               ATOMIC_FETCH_OPS(add, ldadd)
    0.00 :   ffff800008091b54:       mov     w0, #0xffffffff                 // #-1
    0.00 :   ffff800008091b58:       add     x1, x22, #0x50
    0.00 :   ffff800008091b5c:       ldaddal w0, w0, [x1]
         : 69               __lse_atomic_sub_return():
         : 92               }
         :
         : 94               ATOMIC_OP_ADD_SUB_RETURN(_relaxed)
         : 95               ATOMIC_OP_ADD_SUB_RETURN(_acquire)
         : 96               ATOMIC_OP_ADD_SUB_RETURN(_release)
         : 97               ATOMIC_OP_ADD_SUB_RETURN(        )
    0.00 :   ffff800008091b60:       sub     w0, w0, #0x1
         : 99               mmdrop():
         : 49               if (unlikely(atomic_dec_and_test(&mm->mm_count)))
    0.00 :   ffff800008091b64:       cbz     w0, ffff800008091cdc <finish_task_switch.isra.0+0x22c>
         : 51               finish_task_switch():
         : 5089             membarrier_mm_sync_core_before_usermode(mm);
         : 5090             mmdrop_sched(mm);
         : 5091             }
         : 5092             if (unlikely(prev_state == TASK_DEAD)) {
    0.00 :   ffff800008091b68:       cmp     w23, #0x80
    0.00 :   ffff800008091b6c:       b.eq    ffff800008091c5c <finish_task_switch.isra.0+0x1ac>  // b.none
         :
         : 5101             put_task_struct_rcu_user(prev);
         : 5102             }
         :
         : 5104             return rq;
         : 5105             }
    0.00 :   ffff800008091b70:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008091b74:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800008091b78:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800008091b7c:       ldr     x25, [sp, #64]
    0.00 :   ffff800008091b80:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800008091b84:       autiasp
    0.00 :   ffff800008091b88:       ret
         : 5113             arch_local_irq_enable():
         : 43               ARM64_HAS_IRQ_PRIO_MASKING)
         : 44               :
         : 45               : "r" ((unsigned long) GIC_PRIO_IRQON)
         : 46               : "memory");
         :
         : 48               pmr_sync();
    0.00 :   ffff800008091b8c:       dsb     sy
    0.00 :   ffff800008091b90:       b       ffff800008091b3c <finish_task_switch.isra.0+0x8c>
         : 51               perf_event_task_sched_in():
         :
         : 1232             static inline void perf_event_task_sched_in(struct task_struct *prev,
         : 1233             struct task_struct *task)
         : 1234             {
         : 1235             if (static_branch_unlikely(&perf_sched_events))
         : 1236             __perf_event_task_sched_in(prev, task);
    0.00 :   ffff800008091b94:       mov     x1, x25
    0.00 :   ffff800008091b98:       mov     x0, x21
    0.00 :   ffff800008091b9c:       bl      ffff800008212ad0 <__perf_event_task_sched_in>
    0.00 :   ffff800008091ba0:       b       ffff800008091b10 <finish_task_switch.isra.0+0x60>
         :
         : 1234             if (__perf_sw_enabled(PERF_COUNT_SW_CPU_MIGRATIONS) &&
    0.00 :   ffff800008091ba4:       ldrb    w0, [x25, #1084]
    0.00 :   ffff800008091ba8:       tbz     w0, #2, ffff800008091b14 <finish_task_switch.isra.0+0x64>
         : 1237             __perf_sw_event_sched():
         : 1208             struct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);
    0.00 :   ffff800008091bac:       adrp    x2, ffff800009392000 <bpf_bprintf_bufs+0x110>
    0.00 :   ffff800008091bb0:       add     x2, x2, #0x5f8
    0.00 :   ffff800008091bb4:       mov     x19, x2
    0.00 :   ffff800008091bb8:       bl      ffff80000808c3d0 <__kern_my_cpu_offset>
    0.00 :   ffff800008091bbc:       add     x2, x19, x0
         : 1214             perf_fetch_caller_regs():
         : 1189             perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800008091bc0:       mov     x30, x24
    0.00 :   ffff800008091bc4:       xpaclri
    0.00 :   ffff800008091bc8:       and     x1, x30, #0xff80ffffffffffff
    0.00 :   ffff800008091bcc:       tbz     x30, #55, ffff800008091bd4 <finish_task_switch.isra.0+0x124>
    0.00 :   ffff800008091bd0:       orr     x1, x30, #0xffff000000000000
    0.00 :   ffff800008091bd4:       mov     x4, sp
    0.00 :   ffff800008091bd8:       mov     x0, #0x5                        // #5
    0.00 :   ffff800008091bdc:       str     x29, [x2, #232]
         : 1198             __perf_sw_event_sched():
         : 1211             ___perf_sw_event(event_id, nr, regs, addr);
    0.00 :   ffff800008091be0:       mov     x3, #0x0                        // #0
         : 1213             perf_fetch_caller_regs():
         : 1189             perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800008091be4:       stp     x4, x1, [x2, #248]
         : 1191             __perf_sw_event_sched():
         : 1211             ___perf_sw_event(event_id, nr, regs, addr);
    0.00 :   ffff800008091be8:       mov     x1, #0x1                        // #1
         : 1213             perf_fetch_caller_regs():
         : 1189             perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
    0.00 :   ffff800008091bec:       str     x0, [x2, #264]
         : 1191             __perf_sw_event_sched():
         : 1211             ___perf_sw_event(event_id, nr, regs, addr);
    0.00 :   ffff800008091bf0:       mov     w0, #0x4                        // #4
    0.00 :   ffff800008091bf4:       bl      ffff800008219370 <___perf_sw_event>
         : 1214             perf_event_task_sched_in():
         : 1236             task->sched_migrated) {
         : 1237             __perf_sw_event_sched(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 0);
         : 1238             task->sched_migrated = 0;
    0.00 :   ffff800008091bf8:       ldrb    w0, [x25, #1084]
         : 1240             finish_task():
         : 4812             smp_store_release(&prev->on_cpu, 0);
    0.00 :   ffff800008091bfc:       add     x1, x21, #0x34
         : 4814             perf_event_task_sched_in():
    0.00 :   ffff800008091c00:       and     w0, w0, #0xfffffffb
    0.00 :   ffff800008091c04:       strb    w0, [x25, #1084]
         : 1238             finish_task():
    0.00 :   ffff800008091c08:       mov     w0, #0x0                        // #0
    0.00 :   ffff800008091c0c:       stlr    w0, [x1]
         : 4814             __splice_balance_callbacks():
         : 4856             struct callback_head *head = rq->balance_callback;
    0.00 :   ffff800008091c10:       ldr     x19, [x20, #2512]
         : 4858             if (likely(!head))
    0.00 :   ffff800008091c14:       cbz     x19, ffff800008091b28 <finish_task_switch.isra.0+0x78>
         : 4873             rq->balance_callback = NULL;
    0.00 :   ffff800008091c18:       str     xzr, [x20, #2512]
         : 4875             do_balance_callbacks():
         : 4825             while (head) {
    0.00 :   ffff800008091c1c:       nop
         : 4826             func = (void (*)(struct rq *))head->func;
    0.00 :   ffff800008091c20:       mov     x2, x19
         : 4831             func(rq);
    0.00 :   ffff800008091c24:       mov     x0, x20
         : 4826             func = (void (*)(struct rq *))head->func;
    0.00 :   ffff800008091c28:       ldr     x1, [x19, #8]
         : 4827             next = head->next;
    0.00 :   ffff800008091c2c:       ldr     x19, [x19]
         : 4828             head->next = NULL;
    0.00 :   ffff800008091c30:       str     xzr, [x2]
         : 4831             func(rq);
    0.00 :   ffff800008091c34:       blr     x1
         : 4825             while (head) {
    0.00 :   ffff800008091c38:       cbnz    x19, ffff800008091c20 <finish_task_switch.isra.0+0x170>
    0.00 :   ffff800008091c3c:       b       ffff800008091b28 <finish_task_switch.isra.0+0x78>
         : 4828             atomic_read():
         :
         : 29               static __always_inline int
         : 30               atomic_read(const atomic_t *v)
         : 31               {
         : 32               instrument_atomic_read(v, sizeof(*v));
         : 33               return arch_atomic_read(v);
    0.00 :   ffff800008091c40:       ldr     w0, [x22, #72]
         : 35               arch_static_branch_jump():
    0.00 :   ffff800008091c44:       b       ffff800008091c4c <finish_task_switch.isra.0+0x19c>
    0.00 :   ffff800008091c48:       b       ffff800008091b54 <finish_task_switch.isra.0+0xa4>
         : 40               __ll_sc_atomic_sub_return():
         : 112              ATOMIC_FETCH_OP (_relaxed,        ,  ,  ,         , __VA_ARGS__)\
         : 113              ATOMIC_FETCH_OP (_acquire,        , a,  , "memory", __VA_ARGS__)\
         : 114              ATOMIC_FETCH_OP (_release,        ,  , l, "memory", __VA_ARGS__)
         :
         : 116              ATOMIC_OPS(add, add, I)
         : 117              ATOMIC_OPS(sub, sub, J)
    0.00 :   ffff800008091c4c:       add     x3, x22, #0x50
    0.00 :   ffff800008091c50:       mov     w1, #0x1                        // #1
    0.00 :   ffff800008091c54:       b       ffff800008099ebc <call_trace_sched_update_nr_running+0x1dc>
    0.00 :   ffff800008091c58:       b       ffff800008091b64 <finish_task_switch.isra.0+0xb4>
         : 122              finish_task_switch():
         : 5090             if (prev->sched_class->task_dead)
    0.00 :   ffff800008091c5c:       ldr     x0, [x21, #656]
    0.00 :   ffff800008091c60:       ldr     x1, [x0, #152]
    0.00 :   ffff800008091c64:       cbz     x1, ffff800008091c70 <finish_task_switch.isra.0+0x1c0>
         : 5091             prev->sched_class->task_dead(prev);
    0.00 :   ffff800008091c68:       mov     x0, x21
    0.00 :   ffff800008091c6c:       blr     x1
         : 5094             put_task_stack(prev);
    0.00 :   ffff800008091c70:       mov     x0, x21
    0.00 :   ffff800008091c74:       bl      ffff800008050244 <put_task_stack>
         : 5096             put_task_struct_rcu_user(prev);
    0.00 :   ffff800008091c78:       mov     x0, x21
    0.00 :   ffff800008091c7c:       bl      ffff8000080589c0 <put_task_struct_rcu_user>
         : 5100             }
    0.00 :   ffff800008091c80:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008091c84:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800008091c88:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800008091c8c:       ldr     x25, [sp, #64]
    0.00 :   ffff800008091c90:       ldp     x29, x30, [sp], #80
    0.00 :   ffff800008091c94:       autiasp
    0.00 :   ffff800008091c98:       ret
         : 5037             if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
    0.00 :   ffff800008091c9c:       adrp    x4, ffff8000097bd000 <event_class_wiphy_wdev_evt+0x28>
    0.00 :   ffff800008091ca0:       ldrb    w0, [x4, #780]
    0.00 :   ffff800008091ca4:       cbnz    w0, ffff800008091ccc <finish_task_switch.isra.0+0x21c>
         : 5041             preempt_count():
    0.00 :   ffff800008091ca8:       ldr     w3, [x1, #8]
         : 14               finish_task_switch():
    0.00 :   ffff800008091cac:       mov     w5, #0x1                        // #1
    0.00 :   ffff800008091cb0:       ldr     w2, [x1, #1160]
    0.00 :   ffff800008091cb4:       adrp    x0, ffff800008d73000 <kallsyms_token_index+0xaf60>
    0.00 :   ffff800008091cb8:       add     x1, x1, #0x660
    0.00 :   ffff800008091cbc:       add     x0, x0, #0xcc0
    0.00 :   ffff800008091cc0:       strb    w5, [x4, #780]
    0.00 :   ffff800008091cc4:       bl      ffff800008ad45d4 <__warn_printk>
    0.00 :   ffff800008091cc8:       brk     #0x800
         : 5045             preempt_count_set():
         : 19               }
         :
         : 21               static inline void preempt_count_set(u64 pc)
         : 22               {
         : 23               /* Preserve existing value of PREEMPT_NEED_RESCHED */
         : 24               WRITE_ONCE(current_thread_info()->preempt.count, pc);
    0.00 :   ffff800008091ccc:       mov     w1, #0x2                        // #2
         : 26               get_current():
    0.00 :   ffff800008091cd0:       mrs     x0, sp_el0
         : 20               preempt_count_set():
    0.00 :   ffff800008091cd4:       str     w1, [x0, #8]
         : 20               }
    0.00 :   ffff800008091cd8:       b       ffff800008091b00 <finish_task_switch.isra.0+0x50>
         : 22               mmdrop():
         : 50               __mmdrop(mm);
    0.00 :   ffff800008091cdc:       mov     x0, x22
    0.00 :   ffff800008091ce0:       bl      ffff80000804f5e0 <__mmdrop>
    0.00 :   ffff800008091ce4:       b       ffff800008091b68 <finish_task_switch.isra.0+0xb8>

Sorted summary for file /home/huawei/vmlinux-bpf.indirect
----------------------------------------------

  100.00 jump_label.h:21
 Percent |	Source code & Disassembly of vmlinux-bpf.indirect for cycles (4 samples, percent: local period)
---------------------------------------------------------------------------------------------------------------
                          227d70: 4
                   h->nr_samples: 4
         :
         :
         :
         : 3                Disassembly of section .text:
         :
         : 5                ffff800008217c14 <perf_event_exec>:
         : 6                perf_event_exec():
         : 7528             perf_event_output(struct perf_event *event,
         : 7529             struct perf_sample_data *data,
         : 7530             struct pt_regs *regs)
         : 7531             {
         : 7532             return __perf_event_output(event, data, regs, perf_output_begin);
         : 7533             }
    0.00 :   ffff800008217c14:       bti     c
    0.00 :   ffff800008217c18:       nop
    0.00 :   ffff800008217c1c:       nop
         : 7685             if (restart)
         : 7686             perf_event_stop(event, 1);
         : 7687             }
         :
         : 7689             void perf_event_exec(void)
         : 7690             {
    0.00 :   ffff800008217c20:       paciasp
    0.00 :   ffff800008217c24:       stp     x29, x30, [sp, #-128]!
    0.00 :   ffff800008217c28:       mrs     x0, sp_el0
    0.00 :   ffff800008217c2c:       mov     x29, sp
    0.00 :   ffff800008217c30:       stp     x19, x20, [sp, #16]
    0.00 :   ffff800008217c34:       stp     x21, x22, [sp, #32]
         : 7689             struct perf_event_context *ctx;
         : 7690             int ctxn;
         :
         : 7692             for_each_task_context_nr(ctxn) {
    0.00 :   ffff800008217c38:       mov     w22, #0x0                       // #0
         : 7685             {
    0.00 :   ffff800008217c3c:       stp     x23, x24, [sp, #48]
    0.00 :   ffff800008217c40:       stp     x25, x26, [sp, #64]
    0.00 :   ffff800008217c44:       stp     x27, x28, [sp, #80]
    0.00 :   ffff800008217c48:       ldr     x1, [x0, #1168]
    0.00 :   ffff800008217c4c:       str     x1, [sp, #120]
    0.00 :   ffff800008217c50:       mov     x1, #0x0                        // #0
         : 7692             get_current():
         : 19               */
         : 20               static __always_inline struct task_struct *get_current(void)
         : 21               {
         : 22               unsigned long sp_el0;
         :
         : 24               asm ("mrs %0, sp_el0" : "=r" (sp_el0));
    0.00 :   ffff800008217c54:       mrs     x0, sp_el0
    0.00 :   ffff800008217c58:       str     x0, [sp, #104]
         : 27               perf_event_enable_on_exec():
         : 4220             local_irq_save(flags);
    0.00 :   ffff800008217c5c:       bl      ffff800008206bb0 <arch_local_irq_save>
         : 4221             ctx = current->perf_event_ctxp[ctxn];
    0.00 :   ffff800008217c60:       sxtw    x20, w22
    0.00 :   ffff800008217c64:       ldr     x1, [sp, #104]
         : 4220             local_irq_save(flags);
    0.00 :   ffff800008217c68:       mov     x19, x0
         : 4221             ctx = current->perf_event_ctxp[ctxn];
    0.00 :   ffff800008217c6c:       add     x1, x1, w22, sxtw #3
    0.00 :   ffff800008217c70:       ldr     x23, [x1, #2184]
         : 4222             if (!ctx || !ctx->nr_events)
    0.00 :   ffff800008217c74:       cbz     x23, ffff800008217d6c <perf_event_exec+0x158>
    0.00 :   ffff800008217c78:       ldr     w0, [x23, #144]
    0.00 :   ffff800008217c7c:       cbz     w0, ffff800008217fa8 <perf_event_exec+0x394>
         : 4226             __get_cpu_context():
         : 160              return this_cpu_ptr(ctx->pmu->pmu_cpu_context);
    0.00 :   ffff800008217c80:       mov     x24, x23
         : 162              perf_event_enable_on_exec():
         : 4228             list_for_each_entry(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217c84:       mov     x28, x23
         : 4230             __kern_my_cpu_offset():
         :
         : 41               /*
         : 42               * We want to allow caching the value, so avoid using volatile and
         : 43               * instead use a fake stack read to hazard against barrier().
         : 44               */
         : 45               asm(ALTERNATIVE("mrs %0, tpidr_el1",
    0.00 :   ffff800008217c88:       mrs     x0, tpidr_el1
         : 47               __get_cpu_context():
         : 160              return this_cpu_ptr(ctx->pmu->pmu_cpu_context);
    0.00 :   ffff800008217c8c:       ldr     x1, [x24], #8
    0.00 :   ffff800008217c90:       ldr     x21, [x1, #72]
    0.00 :   ffff800008217c94:       add     x21, x21, x0
         : 164              perf_ctx_lock():
         : 166              raw_spin_lock(&cpuctx->ctx.lock);
    0.00 :   ffff800008217c98:       add     x25, x21, #0x8
    0.00 :   ffff800008217c9c:       mov     x0, x25
    0.00 :   ffff800008217ca0:       bl      ffff800008af0c90 <_raw_spin_lock>
         : 168              raw_spin_lock(&ctx->lock);
    0.00 :   ffff800008217ca4:       mov     x0, x24
    0.00 :   ffff800008217ca8:       bl      ffff800008af0c90 <_raw_spin_lock>
         : 171              perf_event_enable_on_exec():
         : 4227             ctx_sched_out(ctx, cpuctx, EVENT_TIME);
    0.00 :   ffff800008217cac:       mov     x1, x21
    0.00 :   ffff800008217cb0:       mov     x0, x23
    0.00 :   ffff800008217cb4:       mov     w2, #0x4                        // #4
    0.00 :   ffff800008217cb8:       bl      ffff8000082107e0 <ctx_sched_out>
         : 4228             list_for_each_entry(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217cbc:       ldr     x26, [x28, #96]!
    0.00 :   ffff800008217cc0:       cmp     x26, x28
    0.00 :   ffff800008217cc4:       b.eq    ffff800008217fe4 <perf_event_exec+0x3d0>  // b.none
         : 4214             enum event_type_t event_type = 0;
    0.00 :   ffff800008217cc8:       mov     w27, #0x0                       // #0
         : 4218             int enabled = 0;
    0.00 :   ffff800008217ccc:       mov     w2, #0x0                        // #0
         : 4220             event_enable_on_exec():
         : 4195             if (!event->attr.enable_on_exec)
    0.00 :   ffff800008217cd0:       ldr     x0, [x26, #256]
    0.00 :   ffff800008217cd4:       tbz     w0, #12, ffff800008217cf8 <perf_event_exec+0xe4>
         : 4199             if (event->state >= PERF_EVENT_STATE_INACTIVE)
    0.00 :   ffff800008217cd8:       ldr     w1, [x26, #168]
         : 4198             event->attr.enable_on_exec = 0;
    0.00 :   ffff800008217cdc:       and     x0, x0, #0xffffffffffffefff
    0.00 :   ffff800008217ce0:       str     x0, [x26, #256]
         : 4199             if (event->state >= PERF_EVENT_STATE_INACTIVE)
    0.00 :   ffff800008217ce4:       tbz     w1, #31, ffff800008217cf8 <perf_event_exec+0xe4>
         : 4202             perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
    0.00 :   ffff800008217ce8:       mov     x0, x26
    0.00 :   ffff800008217cec:       mov     w1, #0x0                        // #0
    0.00 :   ffff800008217cf0:       bl      ffff800008209404 <perf_event_set_state>
         : 4204             return 1;
    0.00 :   ffff800008217cf4:       mov     w2, #0x1                        // #1
         : 4206             get_event_type():
         : 1536             event_type = event->attr.pinned ? EVENT_PINNED : EVENT_FLEXIBLE;
    0.00 :   ffff800008217cf8:       ldr     x1, [x26, #144]
         : 1524             struct perf_event_context *ctx = event->ctx;
    0.00 :   ffff800008217cfc:       ldr     x0, [x26, #544]
         : 1536             event_type = event->attr.pinned ? EVENT_PINNED : EVENT_FLEXIBLE;
    0.00 :   ffff800008217d00:       ldr     x1, [x1, #256]
         : 1537             if (!ctx->task)
    0.00 :   ffff800008217d04:       ldr     x0, [x0, #184]
         : 1536             event_type = event->attr.pinned ? EVENT_PINNED : EVENT_FLEXIBLE;
    0.00 :   ffff800008217d08:       ubfx    w1, w1, #2, #1
    0.00 :   ffff800008217d0c:       add     w1, w1, #0x1
         : 1538             event_type |= EVENT_CPU;
    0.00 :   ffff800008217d10:       cmp     x0, #0x0
         : 1540             perf_event_enable_on_exec():
         : 4228             list_for_each_entry(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217d14:       ldr     x26, [x26]
         : 4230             get_event_type():
         : 1538             event_type |= EVENT_CPU;
    0.00 :   ffff800008217d18:       orr     w0, w1, #0x8
    0.00 :   ffff800008217d1c:       csel    w1, w0, w1, eq  // eq = none
         : 1541             perf_event_enable_on_exec():
         : 4230             event_type |= get_event_type(event);
    0.00 :   ffff800008217d20:       orr     w27, w27, w1
         : 4228             list_for_each_entry(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217d24:       cmp     x26, x28
    0.00 :   ffff800008217d28:       b.ne    ffff800008217cd0 <perf_event_exec+0xbc>  // b.any
         : 4236             if (enabled) {
    0.00 :   ffff800008217d2c:       cbz     w2, ffff800008217fe4 <perf_event_exec+0x3d0>
         : 4238             unclone_ctx():
         : 1327             struct perf_event_context *parent_ctx = ctx->parent_ctx;
    0.00 :   ffff800008217d30:       ldr     x5, [x23, #216]
         : 1331             if (parent_ctx)
    0.00 :   ffff800008217d34:       cbz     x5, ffff800008217d3c <perf_event_exec+0x128>
         : 1332             ctx->parent_ctx = NULL;
    0.00 :   ffff800008217d38:       str     xzr, [x23, #216]
         : 1333             ctx->generation++;
    0.00 :   ffff800008217d3c:       ldr     x4, [x23, #232]
         : 1335             perf_event_enable_on_exec():
         : 4238             ctx_resched(cpuctx, ctx, event_type);
    0.00 :   ffff800008217d40:       mov     x1, x23
    0.00 :   ffff800008217d44:       mov     w2, w27
    0.00 :   ffff800008217d48:       mov     x0, x21
         : 4242             unclone_ctx():
         : 1333             ctx->generation++;
    0.00 :   ffff800008217d4c:       add     x4, x4, #0x1
    0.00 :   ffff800008217d50:       str     x4, [x23, #232]
         : 1327             struct perf_event_context *parent_ctx = ctx->parent_ctx;
    0.00 :   ffff800008217d54:       mov     x23, x5
         : 1329             perf_event_enable_on_exec():
         : 4238             ctx_resched(cpuctx, ctx, event_type);
    0.00 :   ffff800008217d58:       bl      ffff8000082124b0 <ctx_resched>
         : 4240             perf_ctx_unlock():
         : 175              raw_spin_unlock(&ctx->lock);
    0.00 :   ffff800008217d5c:       mov     x0, x24
    0.00 :   ffff800008217d60:       bl      ffff800008af08e0 <_raw_spin_unlock>
         : 176              raw_spin_unlock(&cpuctx->ctx.lock);
    0.00 :   ffff800008217d64:       mov     x0, x25
    0.00 :   ffff800008217d68:       bl      ffff800008af08e0 <_raw_spin_unlock>
         : 179              arch_local_irq_restore():
         : 122              /*
         : 123              * restore saved IRQ state
         : 124              */
         : 125              static inline void arch_local_irq_restore(unsigned long flags)
         : 126              {
         : 127              asm volatile(ALTERNATIVE(
    0.00 :   ffff800008217d6c:       msr     daif, x19
         : 129              arch_static_branch():
         : 21               #define JUMP_LABEL_NOP_SIZE             AARCH64_INSN_SIZE
         :
         : 23               static __always_inline bool arch_static_branch(struct static_key *key,
         : 24               bool branch)
         : 25               {
         : 26               asm_volatile_goto(
  100.00 :   ffff800008217d70:       nop // jump_label.h:21
         : 28               perf_event_enable_on_exec():
         : 4247             if (clone_ctx)
    0.00 :   ffff800008217d74:       cbz     x23, ffff800008217d80 <perf_event_exec+0x16c>
         : 4248             put_ctx(clone_ctx);
    0.00 :   ffff800008217d78:       mov     x0, x23
    0.00 :   ffff800008217d7c:       bl      ffff8000082095d0 <put_ctx>
         : 4251             perf_pin_task_context():
         : 1447             ctx = perf_lock_task_context(task, ctxn, &flags);
    0.00 :   ffff800008217d80:       add     x2, sp, #0x70
    0.00 :   ffff800008217d84:       mov     w1, w22
         : 1450             get_current():
    0.00 :   ffff800008217d88:       mrs     x19, sp_el0
         : 20               perf_pin_task_context():
    0.00 :   ffff800008217d8c:       mov     x0, x19
    0.00 :   ffff800008217d90:       bl      ffff80000820a840 <perf_lock_task_context>
    0.00 :   ffff800008217d94:       mov     x24, x0
         : 1448             if (ctx) {
    0.00 :   ffff800008217d98:       cbz     x0, ffff800008217e78 <perf_event_exec+0x264>
         : 1449             ++ctx->pin_count;
    0.00 :   ffff800008217d9c:       ldr     w2, [x24, #240]
         : 1450             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217da0:       add     x27, x0, #0x8
    0.00 :   ffff800008217da4:       ldr     x1, [sp, #112]
         : 1449             ++ctx->pin_count;
    0.00 :   ffff800008217da8:       add     w2, w2, #0x1
    0.00 :   ffff800008217dac:       str     w2, [x24, #240]
         : 1450             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217db0:       mov     x0, x27
         : 1452             perf_event_remove_on_exec():
         : 4270             mutex_lock(&ctx->mutex);
    0.00 :   ffff800008217db4:       add     x28, x24, #0x10
         : 4272             perf_pin_task_context():
         : 1450             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217db8:       bl      ffff800008af0df0 <_raw_spin_unlock_irqrestore>
         : 1452             perf_event_remove_on_exec():
         : 4270             mutex_lock(&ctx->mutex);
    0.00 :   ffff800008217dbc:       mov     x0, x28
    0.00 :   ffff800008217dc0:       bl      ffff800008aeb4d0 <mutex_lock>
         : 4272             if (WARN_ON_ONCE(ctx->task != current))
    0.00 :   ffff800008217dc4:       ldr     x0, [x24, #184]
    0.00 :   ffff800008217dc8:       cmp     x0, x19
    0.00 :   ffff800008217dcc:       b.ne    ffff800008217ffc <perf_event_exec+0x3e8>  // b.any
         : 4275             list_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217dd0:       mov     x25, x24
         : 4264             bool modified = false;
    0.00 :   ffff800008217dd4:       mov     w26, #0x0                       // #0
         : 4275             list_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217dd8:       ldr     x19, [x25, #96]!
    0.00 :   ffff800008217ddc:       ldr     x21, [x19]
    0.00 :   ffff800008217de0:       cmp     x25, x19
    0.00 :   ffff800008217de4:       b.eq    ffff800008217fb0 <perf_event_exec+0x39c>  // b.none
         : 4276             if (!event->attr.remove_on_exec)
    0.00 :   ffff800008217de8:       ldr     x0, [x19, #256]
    0.00 :   ffff800008217dec:       tbz     x0, #36, ffff800008217e14 <perf_event_exec+0x200>
         : 4279             is_kernel_event():
         : 183              return READ_ONCE(event->owner) == TASK_TOMBSTONE;
    0.00 :   ffff800008217df0:       ldr     x0, [x19, #656]
         : 185              perf_event_remove_on_exec():
         : 4279             if (!is_kernel_event(event))
    0.00 :   ffff800008217df4:       cmn     x0, #0x1
    0.00 :   ffff800008217df8:       b.eq    ffff800008217e04 <perf_event_exec+0x1f0>  // b.none
         : 4280             perf_remove_from_owner(event);
    0.00 :   ffff800008217dfc:       mov     x0, x19
    0.00 :   ffff800008217e00:       bl      ffff80000820c460 <perf_remove_from_owner>
         : 4284             perf_event_exit_event(event, ctx);
    0.00 :   ffff800008217e04:       mov     x0, x19
    0.00 :   ffff800008217e08:       mov     x1, x24
         : 4282             modified = true;
    0.00 :   ffff800008217e0c:       mov     w26, #0x1                       // #1
         : 4284             perf_event_exit_event(event, ctx);
    0.00 :   ffff800008217e10:       bl      ffff800008216934 <perf_event_exit_event>
         : 4275             list_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217e14:       mov     x19, x21
    0.00 :   ffff800008217e18:       ldr     x21, [x21]
    0.00 :   ffff800008217e1c:       cmp     x25, x19
    0.00 :   ffff800008217e20:       b.ne    ffff800008217de8 <perf_event_exec+0x1d4>  // b.any
         : 4287             raw_spin_lock_irqsave(&ctx->lock, flags);
    0.00 :   ffff800008217e24:       mov     x0, x27
    0.00 :   ffff800008217e28:       bl      ffff800008af0820 <_raw_spin_lock_irqsave>
    0.00 :   ffff800008217e2c:       mov     x1, x0
         : 4288             if (modified)
    0.00 :   ffff800008217e30:       cbz     w26, ffff800008217fbc <perf_event_exec+0x3a8>
         : 4290             unclone_ctx():
         : 1327             struct perf_event_context *parent_ctx = ctx->parent_ctx;
    0.00 :   ffff800008217e34:       ldr     x19, [x24, #216]
         : 1333             ctx->generation++;
    0.00 :   ffff800008217e38:       ldr     x0, [x24, #232]
         : 1335             perf_event_remove_on_exec():
         : 4290             --ctx->pin_count;
    0.00 :   ffff800008217e3c:       ldr     w2, [x24, #240]
         : 4292             unclone_ctx():
         : 1333             ctx->generation++;
    0.00 :   ffff800008217e40:       add     x0, x0, #0x1
         : 1335             perf_event_remove_on_exec():
         : 4290             --ctx->pin_count;
    0.00 :   ffff800008217e44:       sub     w2, w2, #0x1
         : 4292             unclone_ctx():
         : 1331             if (parent_ctx)
    0.00 :   ffff800008217e48:       cbz     x19, ffff800008218014 <perf_event_exec+0x400>
         : 1332             ctx->parent_ctx = NULL;
    0.00 :   ffff800008217e4c:       str     xzr, [x24, #216]
         : 1333             ctx->generation++;
    0.00 :   ffff800008217e50:       str     x0, [x24, #232]
         : 1335             perf_event_remove_on_exec():
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217e54:       mov     x0, x27
         : 4290             --ctx->pin_count;
    0.00 :   ffff800008217e58:       str     w2, [x24, #240]
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217e5c:       bl      ffff800008af0df0 <_raw_spin_unlock_irqrestore>
         : 4294             mutex_unlock(&ctx->mutex);
    0.00 :   ffff800008217e60:       mov     x0, x28
    0.00 :   ffff800008217e64:       bl      ffff800008aea260 <mutex_unlock>
         : 4296             put_ctx(ctx);
    0.00 :   ffff800008217e68:       mov     x0, x24
    0.00 :   ffff800008217e6c:       bl      ffff8000082095d0 <put_ctx>
         : 4298             put_ctx(clone_ctx);
    0.00 :   ffff800008217e70:       mov     x0, x19
    0.00 :   ffff800008217e74:       bl      ffff8000082095d0 <put_ctx>
         : 4301             perf_event_exec():
         : 7694             perf_event_enable_on_exec(ctxn);
         : 7695             perf_event_remove_on_exec(ctxn);
         :
         : 7697             rcu_read_lock();
         : 7698             ctx = rcu_dereference(current->perf_event_ctxp[ctxn]);
    0.00 :   ffff800008217e78:       add     x20, x20, #0x110
         : 7700             rcu_read_lock():
         : 704              * read-side critical sections may be preempted and they may also block, but
         : 705              * only when acquiring spinlocks that are subject to priority inheritance.
         : 706              */
         : 707              static __always_inline void rcu_read_lock(void)
         : 708              {
         : 709              __rcu_read_lock();
    0.00 :   ffff800008217e7c:       bl      ffff8000080e3d70 <__rcu_read_lock>
         : 711              get_current():
    0.00 :   ffff800008217e80:       mrs     x0, sp_el0
         : 20               perf_event_exec():
    0.00 :   ffff800008217e84:       add     x20, x0, x20, lsl #3
    0.00 :   ffff800008217e88:       ldr     x21, [x20, #8]
         : 7695             if (ctx) {
    0.00 :   ffff800008217e8c:       cbz     x21, ffff800008217f4c <perf_event_exec+0x338>
         : 7697             perf_iterate_ctx():
         : 7579             list_for_each_entry_rcu(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217e90:       ldr     x19, [x21, #96]!
    0.00 :   ffff800008217e94:       cmp     x19, x21
    0.00 :   ffff800008217e98:       b.eq    ffff800008217f4c <perf_event_exec+0x338>  // b.none
    0.00 :   ffff800008217e9c:       nop
         : 7584             has_addr_filter():
         : 1431             return !!event->attr.write_backward;
         : 1432             }
         :
         : 1434             static inline bool has_addr_filter(struct perf_event *event)
         : 1435             {
         : 1436             return event->pmu->nr_addr_filters;
    0.00 :   ffff800008217ea0:       ldr     x1, [x19, #152]
         : 1438             perf_event_addr_filters():
         : 1440             * An inherited event uses parent's filters
         : 1441             */
         : 1442             static inline struct perf_addr_filters_head *
         : 1443             perf_event_addr_filters(struct perf_event *event)
         : 1444             {
         : 1445             struct perf_addr_filters_head *ifh = &event->addr_filters;
    0.00 :   ffff800008217ea4:       add     x0, x19, #0x348
         :
         : 1443             if (event->parent)
    0.00 :   ffff800008217ea8:       ldr     x20, [x19, #624]
         : 1445             perf_event_addr_filters_exec():
         : 7662             if (!has_addr_filter(event))
    0.00 :   ffff800008217eac:       ldr     w1, [x1, #92]
         : 7664             perf_event_addr_filters():
         : 1440             struct perf_addr_filters_head *ifh = &event->addr_filters;
    0.00 :   ffff800008217eb0:       cmp     x20, #0x0
    0.00 :   ffff800008217eb4:       add     x20, x20, #0x348
    0.00 :   ffff800008217eb8:       csel    x20, x20, x0, ne        // ne = any
         : 1444             perf_event_addr_filters_exec():
    0.00 :   ffff800008217ebc:       cbz     w1, ffff800008217f40 <perf_event_exec+0x32c>
         : 7665             raw_spin_lock_irqsave(&ifh->lock, flags);
    0.00 :   ffff800008217ec0:       add     x24, x20, #0x10
    0.00 :   ffff800008217ec4:       mov     x0, x24
    0.00 :   ffff800008217ec8:       bl      ffff800008af0820 <_raw_spin_lock_irqsave>
         : 7666             list_for_each_entry(filter, &ifh->list, entry) {
    0.00 :   ffff800008217ecc:       ldr     x2, [x20]
         : 7665             raw_spin_lock_irqsave(&ifh->lock, flags);
    0.00 :   ffff800008217ed0:       mov     x1, x0
         : 7666             list_for_each_entry(filter, &ifh->list, entry) {
    0.00 :   ffff800008217ed4:       cmp     x20, x2
    0.00 :   ffff800008217ed8:       b.eq    ffff800008217f64 <perf_event_exec+0x350>  // b.none
         : 7659             unsigned int restart = 0, count = 0;
    0.00 :   ffff800008217edc:       mov     w6, #0x0                        // #0
    0.00 :   ffff800008217ee0:       mov     w4, #0x0                        // #0
    0.00 :   ffff800008217ee4:       nop
         : 7667             if (filter->path.dentry) {
    0.00 :   ffff800008217ee8:       ldr     x3, [x2, #24]
    0.00 :   ffff800008217eec:       cbz     x3, ffff800008217f0c <perf_event_exec+0x2f8>
         : 7668             event->addr_filter_ranges[count].start = 0;
    0.00 :   ffff800008217ef0:       ldr     x5, [x19, #864]
    0.00 :   ffff800008217ef4:       ubfiz   x3, x4, #4, #32
         : 7670             restart++;
    0.00 :   ffff800008217ef8:       add     w6, w6, #0x1
         : 7668             event->addr_filter_ranges[count].start = 0;
    0.00 :   ffff800008217efc:       str     xzr, [x5, x3]
         : 7669             event->addr_filter_ranges[count].size = 0;
    0.00 :   ffff800008217f00:       ldr     x5, [x19, #864]
    0.00 :   ffff800008217f04:       add     x3, x5, x3
    0.00 :   ffff800008217f08:       str     xzr, [x3, #8]
         : 7666             list_for_each_entry(filter, &ifh->list, entry) {
    0.00 :   ffff800008217f0c:       ldr     x2, [x2]
         : 7673             count++;
    0.00 :   ffff800008217f10:       add     w4, w4, #0x1
         : 7666             list_for_each_entry(filter, &ifh->list, entry) {
    0.00 :   ffff800008217f14:       cmp     x20, x2
    0.00 :   ffff800008217f18:       b.ne    ffff800008217ee8 <perf_event_exec+0x2d4>  // b.any
         : 7676             if (restart)
    0.00 :   ffff800008217f1c:       cbz     w6, ffff800008217f64 <perf_event_exec+0x350>
         : 7677             event->addr_filters_gen++;
    0.00 :   ffff800008217f20:       ldr     x2, [x19, #872]
         : 7678             raw_spin_unlock_irqrestore(&ifh->lock, flags);
    0.00 :   ffff800008217f24:       mov     x0, x24
         : 7677             event->addr_filters_gen++;
    0.00 :   ffff800008217f28:       add     x2, x2, #0x1
    0.00 :   ffff800008217f2c:       str     x2, [x19, #872]
         : 7678             raw_spin_unlock_irqrestore(&ifh->lock, flags);
    0.00 :   ffff800008217f30:       bl      ffff800008af0df0 <_raw_spin_unlock_irqrestore>
         : 7681             perf_event_stop(event, 1);
    0.00 :   ffff800008217f34:       mov     x0, x19
    0.00 :   ffff800008217f38:       mov     w1, #0x1                        // #1
    0.00 :   ffff800008217f3c:       bl      ffff8000082085a0 <perf_event_stop.isra.0>
         : 7685             perf_iterate_ctx():
         : 7579             list_for_each_entry_rcu(event, &ctx->event_list, event_entry) {
    0.00 :   ffff800008217f40:       ldr     x19, [x19]
    0.00 :   ffff800008217f44:       cmp     x19, x21
    0.00 :   ffff800008217f48:       b.ne    ffff800008217ea0 <perf_event_exec+0x28c>  // b.any
         : 7583             rcu_read_unlock():
         : 738              static inline void rcu_read_unlock(void)
         : 739              {
         : 740              RCU_LOCKDEP_WARN(!rcu_is_watching(),
         : 741              "rcu_read_unlock() used illegally while idle");
         : 742              __release(RCU);
         : 743              __rcu_read_unlock();
    0.00 :   ffff800008217f4c:       bl      ffff8000080ea800 <__rcu_read_unlock>
         : 745              perf_event_exec():
         : 7689             for_each_task_context_nr(ctxn) {
    0.00 :   ffff800008217f50:       cbnz    w22, ffff800008217f70 <perf_event_exec+0x35c>
    0.00 :   ffff800008217f54:       mov     w22, #0x1                       // #1
    0.00 :   ffff800008217f58:       b       ffff800008217c5c <perf_event_exec+0x48>
         : 7693             arch_local_irq_restore():
         : 130              ARM64_HAS_IRQ_PRIO_MASKING)
         : 131              :
         : 132              : "r" (flags)
         : 133              : "memory");
         :
         : 135              pmr_sync();
    0.00 :   ffff800008217f5c:       dsb     sy
    0.00 :   ffff800008217f60:       b       ffff800008217d74 <perf_event_exec+0x160>
         : 138              perf_event_addr_filters_exec():
         : 7678             raw_spin_unlock_irqrestore(&ifh->lock, flags);
    0.00 :   ffff800008217f64:       mov     x0, x24
    0.00 :   ffff800008217f68:       bl      ffff800008af0df0 <_raw_spin_unlock_irqrestore>
         : 7680             if (restart)
    0.00 :   ffff800008217f6c:       b       ffff800008217f40 <perf_event_exec+0x32c>
         : 7682             perf_event_exec():
         : 7701             perf_iterate_ctx(ctx, perf_event_addr_filters_exec,
         : 7702             NULL, true);
         : 7703             }
         : 7704             rcu_read_unlock();
         : 7705             }
         : 7706             }
    0.00 :   ffff800008217f70:       mrs     x0, sp_el0
    0.00 :   ffff800008217f74:       ldr     x2, [sp, #120]
    0.00 :   ffff800008217f78:       ldr     x1, [x0, #1168]
    0.00 :   ffff800008217f7c:       subs    x2, x2, x1
    0.00 :   ffff800008217f80:       mov     x1, #0x0                        // #0
    0.00 :   ffff800008217f84:       b.ne    ffff800008218024 <perf_event_exec+0x410>  // b.any
    0.00 :   ffff800008217f88:       ldp     x19, x20, [sp, #16]
    0.00 :   ffff800008217f8c:       ldp     x21, x22, [sp, #32]
    0.00 :   ffff800008217f90:       ldp     x23, x24, [sp, #48]
    0.00 :   ffff800008217f94:       ldp     x25, x26, [sp, #64]
    0.00 :   ffff800008217f98:       ldp     x27, x28, [sp, #80]
    0.00 :   ffff800008217f9c:       ldp     x29, x30, [sp], #128
    0.00 :   ffff800008217fa0:       autiasp
    0.00 :   ffff800008217fa4:       ret
         : 7721             perf_event_enable_on_exec():
         : 4213             struct perf_event_context *ctx, *clone_ctx = NULL;
    0.00 :   ffff800008217fa8:       mov     x23, #0x0                       // #0
    0.00 :   ffff800008217fac:       b       ffff800008217d6c <perf_event_exec+0x158>
         : 4216             perf_event_remove_on_exec():
         : 4287             raw_spin_lock_irqsave(&ctx->lock, flags);
    0.00 :   ffff800008217fb0:       mov     x0, x27
    0.00 :   ffff800008217fb4:       bl      ffff800008af0820 <_raw_spin_lock_irqsave>
    0.00 :   ffff800008217fb8:       mov     x1, x0
         : 4290             --ctx->pin_count;
    0.00 :   ffff800008217fbc:       ldr     w2, [x24, #240]
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217fc0:       mov     x0, x27
         : 4290             --ctx->pin_count;
    0.00 :   ffff800008217fc4:       sub     w2, w2, #0x1
    0.00 :   ffff800008217fc8:       str     w2, [x24, #240]
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008217fcc:       bl      ffff800008af0df0 <_raw_spin_unlock_irqrestore>
         : 4294             mutex_unlock(&ctx->mutex);
    0.00 :   ffff800008217fd0:       mov     x0, x28
    0.00 :   ffff800008217fd4:       bl      ffff800008aea260 <mutex_unlock>
         : 4296             put_ctx(ctx);
    0.00 :   ffff800008217fd8:       mov     x0, x24
    0.00 :   ffff800008217fdc:       bl      ffff8000082095d0 <put_ctx>
         : 4297             if (clone_ctx)
    0.00 :   ffff800008217fe0:       b       ffff800008217e78 <perf_event_exec+0x264>
         : 4299             perf_event_enable_on_exec():
         : 4240             ctx_sched_in(ctx, cpuctx, EVENT_TIME);
    0.00 :   ffff800008217fe4:       mov     x0, x23
    0.00 :   ffff800008217fe8:       mov     x1, x21
         : 4213             struct perf_event_context *ctx, *clone_ctx = NULL;
    0.00 :   ffff800008217fec:       mov     x23, #0x0                       // #0
         : 4240             ctx_sched_in(ctx, cpuctx, EVENT_TIME);
    0.00 :   ffff800008217ff0:       mov     w2, #0x4                        // #4
    0.00 :   ffff800008217ff4:       bl      ffff800008212280 <ctx_sched_in>
    0.00 :   ffff800008217ff8:       b       ffff800008217d5c <perf_event_exec+0x148>
         : 4244             perf_event_remove_on_exec():
         : 4272             if (WARN_ON_ONCE(ctx->task != current))
    0.00 :   ffff800008217ffc:       brk     #0x800
         : 4294             mutex_unlock(&ctx->mutex);
    0.00 :   ffff800008218000:       mov     x0, x28
    0.00 :   ffff800008218004:       bl      ffff800008aea260 <mutex_unlock>
         : 4296             put_ctx(ctx);
    0.00 :   ffff800008218008:       mov     x0, x24
    0.00 :   ffff80000821800c:       bl      ffff8000082095d0 <put_ctx>
         : 4297             if (clone_ctx)
    0.00 :   ffff800008218010:       b       ffff800008217e78 <perf_event_exec+0x264>
         : 4299             unclone_ctx():
         : 1333             ctx->generation++;
    0.00 :   ffff800008218014:       str     x0, [x24, #232]
         : 1335             perf_event_remove_on_exec():
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008218018:       mov     x0, x27
         : 4290             --ctx->pin_count;
    0.00 :   ffff80000821801c:       str     w2, [x24, #240]
         : 4291             raw_spin_unlock_irqrestore(&ctx->lock, flags);
    0.00 :   ffff800008218020:       b       ffff800008217fcc <perf_event_exec+0x3b8>
         : 4293             perf_event_exec():
         : 7701             }
    0.00 :   ffff800008218024:       bl      ffff800008ae5de0 <__stack_chk_fail>
